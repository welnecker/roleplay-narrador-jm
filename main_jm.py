# main.py

# ============================================================
# Narrador JM ‚Äî Roleplay adulto (sem pornografia expl√≠cita)
# Compat√≠vel com o m√©todo antigo: GOOGLE_CREDS_JSON + oauth2client
# ============================================================

import os
import re
import json
import time
import random
from datetime import datetime
from typing import List, Tuple, Dict, Any

import streamlit as st
import requests
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import numpy as np

# (Opcional) Embeddings OpenAI para verifica√ß√£o sem√¢ntica/mem√≥ria longa
try:
    from openai import OpenAI
    OPENAI_CLIENT = OpenAI(api_key=st.secrets.get("OPENAI_API_KEY", ""))
    OPENAI_OK = bool(st.secrets.get("OPENAI_API_KEY"))
except Exception:
    OPENAI_CLIENT = None
    OPENAI_OK = False

# =========================
# CONFIG B√ÅSICA DO APP
# =========================

st.set_page_config(page_title="Narrador JM", page_icon="üé¨", layout="wide")

# Gate 18+
if "age_ok" not in st.session_state:
    st.session_state.age_ok = False
if not st.session_state.age_ok:
    st.title("üîû Conte√∫do adulto")
    st.caption("Narrativa adulta, sensual, **sem pornografia expl√≠cita**. Confirme para prosseguir.")
    if st.checkbox("Confirmo que tenho 18 anos ou mais e desejo prosseguir."):
        st.session_state.age_ok = True
        st.stop()

# =========================
# GOOGLE SHEETS ‚Äî MODO ANTIGO
# =========================

PLANILHA_ID_PADRAO = st.secrets.get("SPREADSHEET_ID", "").strip() or "1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM"

def conectar_planilha():
    """
    Conecta via GOOGLE_CREDS_JSON (modo antigo/est√°vel).
    Espera:
    - st.secrets["GOOGLE_CREDS_JSON"]: string JSON do service account
    - (opcional) st.secrets["SPREADSHEET_ID"]: ID da planilha
    """
    try:
        creds_dict = json.loads(st.secrets["GOOGLE_CREDS_JSON"])
        if "private_key" in creds_dict:
            creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        spreadsheet_id = st.secrets.get("SPREADSHEET_ID", "").strip() or PLANILHA_ID_PADRAO
        return client.open_by_key(spreadsheet_id)
    except Exception as e:
        st.error(f"Erro ao conectar √† planilha: {e}")
        return None

planilha = conectar_planilha()

# Abas esperadas
TAB_INTERACOES = "interacoes_jm" # timestamp | role | content
TAB_PERFIL = "perfil_jm" # timestamp | resumo
TAB_MEMORIAS = "memorias_jm" # tipo | conteudo
TAB_ML = "memoria_longa_jm" # texto | embedding | tags | timestamp | score

def _ws(name: str, create_if_missing: bool = True):
    if not planilha:
        return None
    try:
        return planilha.worksheet(name)
    except Exception:
        if not create_if_missing:
            return None
        try:
            ws = planilha.add_worksheet(title=name, rows=5000, cols=10)
            # cria cabe√ßalhos padr√£o
            if name == TAB_INTERACOES:
                ws.append_row(["timestamp", "role", "content"])
            elif name == TAB_PERFIL:
                ws.append_row(["timestamp", "resumo"])
            elif name == TAB_MEMORIAS:
                ws.append_row(["tipo", "conteudo"])
            elif name == TAB_ML:
                ws.append_row(["texto", "embedding", "tags", "timestamp", "score"])
            return ws
        except Exception:
            return None

# =========================
# UTILIDADES: MEM√ìRIAS / HIST√ìRICO
# =========================

def carregar_memorias_brutas() -> Dict[str, List[str]]:
    """L√™ 'memorias_jm' e devolve um dict {tag_lower: [linhas]}."""
    try:
        aba = _ws(TAB_MEMORIAS, create_if_missing=False)
        if not aba:
            return {}
        regs = aba.get_all_records()
        buckets: Dict[str, List[str]] = {}
        for r in regs:
            tag = (r.get("tipo","") or "").strip().lower()
            txt = (r.get("conteudo","") or "").strip()
            if tag and txt:
                buckets.setdefault(tag, []).append(txt)
        return buckets
    except Exception as e:
        st.warning(f"Erro ao carregar mem√≥rias: {e}")
        return {}

def persona_block(nome: str, buckets: dict, max_linhas: int = 8) -> str:
    """Monta bloco compacto da persona (ordena por prefixos √∫teis)."""
    tag = f"[{nome}]"
    linhas = buckets.get(tag, [])
    ordem = ["OBJ:", "TAT:", "LV:", "VOZ:", "BIO:", "ROTINA:", "LACOS:", "APS:", "CONFLITOS:"]
    def peso(l):
        up = l.upper()
        for i, p in enumerate(ordem):
            if up.startswith(p):
                return i
        return len(ordem)
    linhas_ordenadas = sorted(linhas, key=peso)[:max_linhas]
    titulo = "J√¢nio" if nome in ("janio","j√¢nio") else "Mary" if nome=="mary" else nome.capitalize()
    return (f"{titulo}:\n- " + "\n- ".join(linhas_ordenadas)) if linhas_ordenadas else ""

def carregar_resumo_salvo() -> str:
    """Busca o √∫ltimo resumo da aba 'perfil_jm' (cabe√ßalho: timestamp | resumo)."""
    try:
        aba = _ws(TAB_PERFIL, create_if_missing=False)
        if not aba:
            return ""
        registros = aba.get_all_records()
        for r in reversed(registros):
            txt = (r.get("resumo") or "").strip()
            if txt:
                return txt
        return ""
    except Exception as e:
        st.warning(f"Erro ao carregar resumo salvo: {e}")
        return ""

def salvar_resumo(resumo: str):
    """Salva uma nova linha em 'perfil_jm' (timestamp | resumo)."""
    try:
        aba = _ws(TAB_PERFIL)
        if not aba:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        aba.append_row([timestamp, resumo], value_input_option="RAW")
    except Exception as e:
        st.error(f"Erro ao salvar resumo: {e}")

def salvar_interacao(role: str, content: str):
    """Anexa uma intera√ß√£o na aba 'interacoes_jm'."""
    if not planilha:
        return
    try:
        aba = _ws(TAB_INTERACOES)
        if not aba:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        aba.append_row([timestamp, role.strip(), content.strip()], value_input_option="RAW")
    except Exception as e:
        st.error(f"Erro ao salvar intera√ß√£o: {e}")

def carregar_interacoes(n: int = 20):
    """Carrega as √∫ltimas n intera√ß√µes (role, content) da aba interacoes_jm."""
    try:
        aba = _ws(TAB_INTERACOES, create_if_missing=False)
        if not aba:
            return []
        registros = aba.get_all_records()
        return registros[-n:] if len(registros) > n else registros
    except Exception as e:
        st.warning(f"Erro ao carregar intera√ß√µes: {e}")
        return []

# =========================
# EMBEDDINGS / SIMILARIDADE
# =========================

def gerar_embedding_openai(texto: str):
    if not OPENAI_OK:
        return None
    try:
        resp = OPENAI_CLIENT.embeddings.create(
            input=texto,
            model="text-embedding-3-small"
        )
        return np.array(resp.data[0].embedding, dtype=float)
    except Exception as e:
        st.error(f"Erro ao gerar embedding: {e}")
        return None

def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

def verificar_quebra_semantica_openai(texto1: str, texto2: str, limite=0.6) -> str:
    if not OPENAI_OK:
        return ""
    e1 = gerar_embedding_openai(texto1)
    e2 = gerar_embedding_openai(texto2)
    if e1 is None or e2 is None:
        return ""
    sim = cosine_similarity(e1, e2)
    if sim < limite:
        return f"‚ö†Ô∏è Baixa continuidade narrativa (similaridade: {sim:.2f})."
    return ""

# =========================
# MEM√ìRIA LONGA (Sheets + Embeddings/OpenAI opcional)
# =========================

def _sheet_ensure_memoria_longa():
    """Retorna a aba memoria_longa_jm se existir (n√£o cria automaticamente)."""
    return _ws(TAB_ML, create_if_missing=False)

def _serialize_vec(vec: np.ndarray) -> str:
    return json.dumps(vec.tolist(), separators=(",", ":"))

def _deserialize_vec(s: str) -> np.ndarray:
    try:
        return np.array(json.loads(s), dtype=float)
    except Exception:
        return np.zeros(1, dtype=float)

def memoria_longa_salvar(texto: str, tags: str = "") -> bool:
    """Salva uma mem√≥ria com embedding (se poss√≠vel) e score inicial."""
    aba = _sheet_ensure_memoria_longa()
    if not aba:
        st.warning("Aba 'memoria_longa_jm' n√£o encontrada ‚Äî crie com cabe√ßalhos: texto | embedding | tags | timestamp | score")
        return False
    emb = gerar_embedding_openai(texto)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    linha = [texto.strip(), _serialize_vec(emb) if emb is not None else "", (tags or "").strip(), ts, 1.0]
    try:
        aba.append_row(linha, value_input_option="RAW")
        return True
    except Exception as e:
        st.error(f"Erro ao salvar mem√≥ria longa: {e}")
        return False

def memoria_longa_listar_registros():
    """Retorna todos os registros da aba memoria_longa_jm (ou [])."""
    aba = _sheet_ensure_memoria_longa()
    if not aba:
        return []
    try:
        return aba.get_all_records()
    except Exception:
        return []

def _tokenize(s: str) -> set:
    return set(re.findall(r"[a-z√†-√∫0-9]+", (s or "").lower()))

def memoria_longa_buscar_topk(query_text: str, k: int = 3, limiar: float = 0.78):
    """Top-K mem√≥rias. Usa embeddings se existir; sen√£o, Jaccard simples."""
    aba = _sheet_ensure_memoria_longa()
    if not aba:
        return []
    try:
        dados = aba.get_all_records()
    except Exception as e:
        st.warning(f"Erro ao carregar memoria_longa_jm: {e}")
        return []
    q = gerar_embedding_openai(query_text) if OPENAI_OK else None
    candidatos = []
    for row in dados:
        texto = (row.get("texto") or "").strip()
        emb_s = (row.get("embedding") or "").strip()
        try:
            score = float(row.get("score", 1.0) or 1.0)
        except Exception:
            score = 1.0
        if not texto:
            continue
        if q is not None and emb_s:
            vec = _deserialize_vec(emb_s)
            if vec.ndim == 1 and vec.size >= 10:
                sim = float(np.dot(q, vec) / (np.linalg.norm(q) * np.linalg.norm(vec)))
            else:
                sim = 0.0
        else:
            # fallback lexical
            s1 = _tokenize(texto)
            s2 = _tokenize(query_text)
            sim = len(s1 & s2) / max(1, len(s1 | s2))
        if sim >= limiar:
            rr = 0.7 * sim + 0.3 * score
            candidatos.append((texto, score, sim, rr))
    candidatos.sort(key=lambda x: x[3], reverse=True)
    return candidatos[:k]

def memoria_longa_reforcar(textos_usados: list):
    """Aumenta o score das mem√≥rias usadas (pequeno refor√ßo)."""
    aba = _sheet_ensure_memoria_longa()
    if not aba or not textos_usados:
        return
    try:
        dados = aba.get_all_values()
        if not dados or len(dados) < 2:
            return
        headers = dados
        idx_texto = headers.index("texto")
        idx_score = headers.index("score")
        for i, linha in enumerate(dados[1:], start=2):
            if len(linha) <= max(idx_texto, idx_score):
                continue
            t = (linha[idx_texto] or "").strip()
            if t in textos_usados:
                try:
                    sc = float(linha[idx_score] or 1.0)
                except Exception:
                    sc = 1.0
                sc = min(sc + 0.2, 2.0)
                aba.update_cell(i, idx_score + 1, sc)
    except Exception:
        pass

# =========================
# ROMANCE (FASES) + MOMENTO
# =========================

FASES_ROMANCE: Dict[int, Dict[str, str]] = {
    0: {"nome": "Estranhos",
        "permitidos": "olhares; near-miss (mesmo caf√©/rua/√¥nibus); detalhe do ambiente",
        "proibidos": "troca de nomes; toques; conversa pessoal"},
    1: {"nome": "Percep√ß√£o",
        "permitidos": "cumprimento neutro; pergunta impessoal curta",
        "proibidos": "contato f√≠sico; confid√™ncias"},
    2: {"nome": "Conhecidos",
        "permitidos": "troca de nomes; pequena ajuda; 1 pergunta pessoal leve",
        "proibidos": "toque prolongado; encontro a s√≥s planejado"},
    3: {"nome": "Amizade",
        "permitidos": "conversa 10‚Äì20 min; caminhar juntos; troca de contatos; 1 gesto de afeto leve (com consentimento)",
        "proibidos": "beijos; car√≠cias intimistas"},
    4: {"nome": "Confian√ßa / Quase",
        "permitidos": "confid√™ncias; abra√ßo com consentimento expresso; marcar encontro futuro claro",
        "proibidos": "sexo; sexo oral/manual; pressa ou ‚Äúprovas de amor‚Äù f√≠sicas"},
    5: {"nome": "Compromisso / Encontro definitivo",
        "permitidos": "beijo prolongado; dormir juntos; consuma√ß√£o **impl√≠cita** (fade-to-black); manh√£ seguinte sugerida",
        # Proibi√ß√µes originais retiradas para permitir expl√≠cito:
        "proibidos": ""},
}

FLAG_FASE_TXT_PREFIX = "FLAG: mj_fase="

def _fase_label(n: int) -> str:
    d = FASES_ROMANCE.get(int(n), FASES_ROMANCE[0])
    return f"{int(n)} ‚Äî {d['nome']}"

def mj_set_fase(n: int, persist: bool=True):
    n = max(0, min(max(FASES_ROMANCE.keys()), int(n)))
    st.session_state.mj_fase = n
    if persist:
        try:
            memoria_longa_salvar(f"{FLAG_FASE_TXT_PREFIX}{n}", tags="[flag]")
        except Exception:
            pass

def mj_carregar_fase_inicial() -> int:
    if "mj_fase" in st.session_state:
        return int(st.session_state.mj_fase)
    try:
        recs = memoria_longa_listar_registros()
        for r in reversed(recs):
            t = (r.get("texto") or "").strip()
            if t.startswith(FLAG_FASE_TXT_PREFIX):
                n = int(t.split("=")[1])
                st.session_state.mj_fase = n
                return n
    except Exception:
        pass
    st.session_state.mj_fase = 0
    return 0

# --------- Motor de Momento ----------
MOMENTOS = {
    0: {"nome": "Aproxima√ß√£o log√≠stica",
        "objetivo": "um acompanha o outro (ex.: at√© o p√≠er), clima cordial",
        "permitidos": "gentilezas; proximidade leve; di√°logo casual",
        "proibidos": "declara√ß√£o; revela√ß√µes √≠ntimas; toques prolongados",
        "gatilhos": [r"\b(p[i√≠]er|acompanhar|vamos embora|te levo)\b"],
        "proximo": 1},
    1: {"nome": "Declara√ß√£o",
        "objetivo": "um deles declara amor/ import√¢ncia",
        "permitidos": "confiss√£o afetiva; sil√™ncio tenso; abra√ßo curto",
        "proibidos": "negocia√ß√£o sexual; tirar roupas; explora√ß√£o do corpo",
        "gatilhos": [r"\b(amo voc[e√™]|te amo|n[a√£]o paro de pensar)\b"],
        "proximo": 2},
    2: {"nome": "Revela√ß√£o sens√≠vel",
        "objetivo": "Mary revela que √© virgem / vulnerabilidade equivalente",
        "permitidos": "dizer 'sou virgem'; estipular limites; conforto m√∫tuo",
        "proibidos": "car√≠cias √≠ntimas; tirar roupas",
        "gatilhos": [r"\b(sou virgem|nunca fiz|meu limite)\b"],
        "proximo": 3},
    3: {"nome": "Consentimento expl√≠cito",
        "objetivo": "alinhamento de limites e um 'sim' claro",
        "permitidos": "nomear fronteiras; pedir/receber consentimento; decidir 'agora sim'",
        # Proibi√ß√£o removida para permitir descri√ß√£o expl√≠cita:
        "proibidos": "",
        "gatilhos": [r"\b(consento|quero|vamos juntos|tudo bem pra voc[e√™])\b", r"\b(at[e√©] onde)\b"],
        "proximo": 4},
    4: {"nome": "Intimidade (el√≠ptica)",
        "objetivo": "intimidade sugerida (fade-to-black) / p√≥s-ato impl√≠cito",
        "permitidos": "beijos longos; proximidade forte; fade-to-black; manh√£ seguinte impl√≠cita",
        # Proibi√ß√£o removida para permitir descri√ß√£o gr√°fica:
        "proibidos": "",
        "gatilhos": [r"\b(quarto|cama|luz baixa|porta fechada|manh[a√£] seguinte)\b"],
        "proximo": 4},
}

def _momento_label(n: int) -> str:
    m = MOMENTOS.get(int(n), MOMENTOS[0])
    return f"{int(n)} ‚Äî {m['nome']}"

def detectar_momento_sugerido(texto: str, fallback: int = 0) -> int:
    t = (texto or "").lower()
    for i in range(4, -1, -1):
        for gx in MOMENTOS[i]["gatilhos"]:
            if re.search(gx, t, flags=re.IGNORECASE):
                return i
    # Removido o freio por termos expl√≠citos: n√£o for√ßa parada.
    return fallback

def clamp_momento(atual: int, proposto: int, max_steps: int) -> int:
    if proposto > atual + max_steps:
        return atual + max_steps
    if proposto < atual:
        return max(proposto, atual - 1)
    return proposto

def momento_set(n: int, persist: bool = True):
    n = max(0, min(max(MOMENTOS.keys()), int(n)))
    st.session_state.momento = n
    if persist:
        try:
            memoria_longa_salvar(f"FLAG: mj_momento={n}", tags="[flag]")
        except Exception:
            pass

def momento_carregar() -> int:
    if "momento" in st.session_state:
        return int(st.session_state.momento)
    try:
        recs = memoria_longa_listar_registros()
        for r in reversed(recs):
            t = (r.get("texto") or "").strip()
            if t.startswith("FLAG: mj_momento="):
                n = int(t.split("=")[1])
                st.session_state.momento = n
                return n
    except Exception:
        pass
    st.session_state.momento = 0
    return 0

def viola_momento(texto: str, momento: int) -> str:
    # N√£o bloquear/censurar conte√∫do expl√≠cito por momento.
    return ""

# =========================
# PROVEDORES E MODELOS
# =========================

MODELOS_OPENROUTER = {
    "üí¨ DeepSeek V3 ‚òÖ‚òÖ‚òÖ‚òÖ ($)": "deepseek/deepseek-chat-v3-0324",
    "üß† DeepSeek R1 0528 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$)": "deepseek/deepseek-r1-0528",
    "üß† DeepSeek R1T2 Chimera ‚òÖ‚òÖ‚òÖ‚òÖ (free)": "tngtech/deepseek-r1t2-chimera:free",
    "üß† GPT-4.1 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (1M ctx)": "openai/gpt-4.1",
    "üëë WizardLM 8x22B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$$)": "microsoft/wizardlm-2-8x22b",
    "üëë Qwen 235B 2507 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (PAID)": "qwen/qwen3-235b-a22b-07-25",
    "üëë EVA Qwen2.5 72B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-qwen-2.5-72b",
    "üëë EVA Llama 3.33 70B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-llama-3.33-70b",
    "üé≠ Nous Hermes 2 Yi 34B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ": "nousresearch/nous-hermes-2-yi-34b",
    "üî• MythoMax 13B ‚òÖ‚òÖ‚òÖ‚òÜ ($)": "gryphe/mythomax-l2-13b",
    "üíã LLaMA3 Lumimaid 8B ‚òÖ‚òÖ‚òÜ ($)": "neversleep/llama-3-lumimaid-8b",
    "üåπ Midnight Rose 70B ‚òÖ‚òÖ‚òÖ‚òÜ": "sophosympatheia/midnight-rose-70b",
    "üå∂Ô∏è Noromaid 20B ‚òÖ‚òÖ‚òÜ": "neversleep/noromaid-20b",
    "üíÄ Mythalion 13B ‚òÖ‚òÖ‚òÜ": "pygmalionai/mythalion-13b",
    "üêâ Anubis 70B ‚òÖ‚òÖ‚òÜ": "thedrummer/anubis-70b-v1.1",
    "üßö Rocinante 12B ‚òÖ‚òÖ‚òÜ": "thedrummer/rocinante-12b",
    "üç∑ Magnum v2 72B ‚òÖ‚òÖ‚òÜ": "anthracite-org/magnum-v2-72b",
}

MODELOS_TOGETHER_UI = {
    "üß† Qwen3 Coder 480B (Together)": "togethercomputer/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "üëë Mixtral 8x7B v0.1 (Together)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "üëë Perplexity R1-1776 (Together)": "perplexity-ai/r1-1776",
}

def model_id_for_together(api_ui_model_id: str) -> str:
    key = api_ui_model_id.strip()
    if "Qwen3-Coder-480B-A35B-Instruct-FP8" in key:
        return "Qwen/Qwen3-Coder-480B-Instruct-FP8"
    if key.lower().startswith("mistralai/mixtral-8x7b-instruct-v0.1"):
        return "mistralai/Mixtral-8x7B-Instruct-v0.1"
    return key

def api_config_for_provider(provider: str):
    if provider == "OpenRouter":
        return (
            "https://openrouter.ai/api/v1/chat/completions",
            st.secrets.get("OPENROUTER_API_KEY", ""),
            MODELOS_OPENROUTER,
        )
    else:
        return (
            "https://api.together.xyz/v1/chat/completions",
            st.secrets.get("TOGETHER_API_KEY", ""),
            MODELOS_TOGETHER_UI,
        )

# =========================
# PROMPT BUILDER
# =========================

def inserir_regras_mary_e_janio(prompt_base: str) -> str:
    # padr√£o elevado para 3 (expl√≠cito)
    calor = int(st.session_state.get("steam_level", st.session_state.get("nsfw_max_level", 3)))
    regras = f"""
‚öñÔ∏è Regras de coer√™ncia:
- Narre em terceira pessoa; n√£o se dirija ao leitor como "voc√™".
- Consentimento claro antes de qualquer gesto significativo.
- J√¢nio n√£o pressiona; respeita o ritmo de Mary.
- Linguagem sensual proporcional ao n√≠vel de calor ({calor}).
""".strip()
    fase = int(st.session_state.get("mj_fase", mj_carregar_fase_inicial()))
    if fase >= 5:
        # liberar expl√≠cito em fase 5
        regras += """
- Intimidade pode ser descrita com detalhes expl√≠citos (sem necessidade de corte)."""
    else:
        regras += """
- Sem consuma√ß√£o em cena; foque em progress√£o coerente."""
    return prompt_base + "\n" + regras

def construir_prompt_com_narrador() -> str:
    memos = carregar_memorias_brutas()
    perfil = carregar_resumo_salvo()
    fase = int(st.session_state.get("mj_fase", mj_carregar_fase_inicial()))
    fdata = FASES_ROMANCE.get(fase, FASES_ROMANCE[0])
    momento_atual = int(st.session_state.get("momento", momento_carregar()))
    mdata = MOMENTOS.get(momento_atual, MOMENTOS)
    proximo_nome = MOMENTOS[mdata["proximo"]]["nome"]
    estilo = st.session_state.get("estilo_escrita", "A√á√ÉO")

    # Hist√≥rico do Sheets
    n_hist = int(st.session_state.get("n_sheet_prompt", 15))
    hist = carregar_interacoes(n=n_hist)
    hist_txt = "\n".join(f"{r['role']}: {r['content']}" for r in hist) if hist else "(sem hist√≥rico)"

    # Mem√≥ria longa Top-K (texto apenas; se n√£o houver, "(nenhuma)")
    ml_topk_txt = "(nenhuma)"
    if st.session_state.get("use_memoria_longa", True) and hist:
        try:
            topk = memoria_longa_buscar_topk(
                query_text=hist[-1]["content"],
                k=int(st.session_state.get("k_memoria_longa", 3)),
                limiar=float(st.session_state.get("limiar_memoria_longa", 0.78)),
            )
            if topk:
                ml_topk_txt = "\n".join([f"- {t}" for (t, _sc, _sim, _rr) in topk])
                st.session_state["_ml_topk_texts"] = [t for (t, *_rest) in topk]
            else:
                st.session_state["_ml_topk_texts"] = []
        except Exception:
            st.session_state["_ml_topk_texts"] = []
    else:
        st.session_state["_ml_topk_texts"] = []

    recorrentes = [c for (t, lst) in memos.items() if t == "[all]" for c in lst]
    st.session_state["_ml_recorrentes"] = recorrentes

    dossie = []
    mary = persona_block("mary", memos, 8)
    janio = persona_block("janio", memos, 8)
    if mary: dossie.append(mary)
    if janio: dossie.append(janio)
    dossie_txt = "\n\n".join(dossie) if dossie else "(sem personas definidas)"

    prompt = f"""
Voc√™ √© o Narrador de um roleplay dram√°tico brasileiro, foque em Mary e J√¢nio. N√£o repita instru√ß√µes nem t√≠tulos.

### Dossi√™ (personas)
{dossie_txt}

### Diretrizes gerais (ALL)
{chr(10).join(['- '+c for c in recorrentes]) if recorrentes else '(vazio)'}

### Perfil (resumo mais recente)
{perfil or "(vazio)"}

### Hist√≥rico recente (planilha)
{hist_txt}

### Estilo
- Use o estilo **{estilo}**:
{("- Frases curtas, cortes r√°pidos, foco em gesto/ritmo.") if estilo=="A√á√ÉO" else
("- Atmosfera sombria, subtexto, sil√™ncio que pesa.") if estilo=="NOIR" else
("- Ritmo lento, tens√£o emocional, detalhes sensoriais (sem grafismo).")}

### Mem√≥ria longa ‚Äî Top-K relevantes
{ml_topk_txt}

### ‚è±Ô∏è Estado do romance (manual)
- Fase atual: {_fase_label(fase)}
- Permitidos: {fdata['permitidos']}
- Proibidos: {fdata['proibidos']}

### üéØ Momento dram√°tico (agora)
- Momento: {_momento_label(momento_atual)}
- Objetivo da cena: {mdata['objetivo']}
- Nesta cena, **permita**: {mdata['permitidos']}
- Evite/adiar: {mdata['proibidos']}
- **Micropassos:** avance no m√°ximo **{int(st.session_state.get("max_avancos_por_cena",1))}** subpasso(s) rumo a: {proximo_nome}.
- Se o roteirista pedir salto maior, **negocie**: nomeie limites, pe√ßa consentimento, e **prepare** a transi√ß√£o (n√£o pule etapas).

### Regra de sa√≠da
- Narre em **terceira pessoa**; n√£o fale com "voc√™".
- N√£o exiba r√≥tulos/meta (ex.: "Microconquista:", "Gancho:").
- Entregue uma cena coesa e finalizada; feche com um gancho impl√≠cito.
""".strip()

    prompt = inserir_regras_mary_e_janio(prompt)
    return prompt

# =========================
# FILTROS DE SA√çDA
# =========================

def render_tail(t: str) -> str:
    if not t: return ""
    # remove r√≥tulos meta e
    t = re.sub(r'^\s*\**\s*(microconquista|gancho)\s*:\s*.*$', '', t, flags=re.IGNORECASE | re.MULTILINE)
    t = re.sub(r'&lt;\s*think\s*&gt;.*?&lt;\s*/\s*think\s*&gt;', '', t, flags=re.IGNORECASE | re.DOTALL)
    t = re.sub(r'\n{3,}', '\n\n', t).strip()
    return t

EXPL_PAT = re.compile(
    r"\b(seio[s]?|mamilos?|bunda|fio[- ]?dental|genit[a√°]lia|ere[c√ß][a√£]o|penetra[c√ß][a√£]o|"
    r"boquete|gozada|gozo|sexo oral|chupar|enfiar)\b",
    flags=re.IGNORECASE
)

def classify_nsfw_level(t: str) -> int:
    if EXPL_PAT.search(t or ""):
        return 3 # expl√≠cito
    if re.search(r"\b(cintura|pesco[c√ß]o|costas|beijo prolongado|respira[c√ß][a√£]o curta)\b", (t or ""), re.IGNORECASE):
        return 2
    if re.search(r"\b(olhar|aproximar|toque|m[a√£]os dadas|beijo)\b", (t or ""), re.IGNORECASE):
        return 1
    return 0

def sanitize_explicit(t: str, max_level: int, action: str) -> str:
    # Libera√ß√£o: se o conte√∫do for de n√≠vel <= max_level, retorna tal como est√°.
    lvl = classify_nsfw_level(t)
    if lvl <= max_level:
        return t
    # Se extrapolar o m√°ximo definido, n√£o cortar por padr√£o (liberar NSFW). Apenas retorna sem censura.
    return t

def redact_for_logs(t: str) -> str:
    if not t: return ""
    # Mant√©m ofusca√ß√£o no log se desejar (opcional). Pode deixar como estava:
    t = re.sub(EXPL_PAT, "[‚Ä¶]", t, flags=re.IGNORECASE)
    return re.sub(r'\n{3,}', '\n\n', t).strip()

def resposta_valida(t: str) -> bool:
    if not t or t.strip() == "[Sem conte√∫do]":
        return False
    if len(t.strip()) < 5:
        return False
    return True

# =========================
# UI ‚Äî CABE√áALHO E CONTROLES
# =========================

st.title("üé¨ Narrador JM")
st.subheader("Voc√™ √© o roteirista. Digite uma dire√ß√£o de cena. A IA narrar√° Mary e J√¢nio.")
st.markdown("---")

# Estado inicial
if "resumo_capitulo" not in st.session_state:
    st.session_state.resumo_capitulo = carregar_resumo_salvo()
if "session_msgs" not in st.session_state:
    st.session_state.session_msgs = []
if "use_memoria_longa" not in st.session_state:
    st.session_state.use_memoria_longa = True
if "k_memoria_longa" not in st.session_state:
    st.session_state.k_memoria_longa = 3
if "limiar_memoria_longa" not in st.session_state:
    st.session_state.limiar_memoria_longa = 0.78
if "app_bloqueio_intimo" not in st.session_state:
    st.session_state.app_bloqueio_intimo = False
if "app_emocao_oculta" not in st.session_state:
    st.session_state.app_emocao_oculta = "nenhuma"
if "mj_fase" not in st.session_state:
    st.session_state.mj_fase = mj_carregar_fase_inicial()
if "momento" not in st.session_state:
    st.session_state.momento = momento_carregar()
if "max_avancos_por_cena" not in st.session_state:
    st.session_state.max_avancos_por_cena = 1
if "nsfw_max_level" not in st.session_state:
    # padr√£o elevado para 3 (expl√≠cito)
    st.session_state.nsfw_max_level = 3
if "estilo_escrita" not in st.session_state:
    st.session_state.estilo_escrita = "A√á√ÉO"

# Linha de op√ß√µes r√°pidas
col1, col2 = st.columns([3, 2])
with col1:
    st.markdown("#### üìñ √öltimo resumo salvo:")
    st.info(st.session_state.resumo_capitulo or "Nenhum resumo dispon√≠vel.")
with col2:
    st.markdown("#### ‚öôÔ∏è Op√ß√µes")
    st.checkbox(
        "Bloquear avan√ßos √≠ntimos sem ordem",
        value=st.session_state.app_bloqueio_intimo,
        key="ui_bloqueio_intimo",
    )
    st.selectbox(
        "üé≠ Emo√ß√£o oculta",
        ["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"],
        index=["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"].index(st.session_state.app_emocao_oculta),
        key="ui_app_emocao_oculta",
    )
    st.session_state.app_bloqueio_intimo = st.session_state.get("ui_bloqueio_intimo", False)
    st.session_state.app_emocao_oculta = st.session_state.get("ui_app_emocao_oculta", "nenhuma")

# =========================
# SIDEBAR ‚Äî Provedor, modelo, resumo, mem√≥ria, romance manual
# =========================

with st.sidebar:
    st.title("üß≠ Painel do Roteirista")
    # Provedor/modelos
    provedor = st.radio("üåê Provedor", ["OpenRouter", "Together"], index=0, key="provedor_ia")
    api_url, api_key, modelos_map = api_config_for_provider(provedor)
    if not api_key:
        st.warning("‚ö†Ô∏è API key ausente para o provedor selecionado. Defina em st.secrets.")
    modelo_nome = st.selectbox("ü§ñ Modelo de IA", list(modelos_map.keys()), index=0, key="modelo_nome_ui")
    modelo_escolhido_id_ui = modelos_map[modelo_nome]
    st.session_state.modelo_escolhido_id = modelo_escolhido_id_ui

    st.markdown("---")
    st.markdown("### ‚úçÔ∏è Estilo & NSFW")
    st.selectbox(
        "Estilo de escrita",
        ["A√á√ÉO", "ROMANCE LENTO", "NOIR"],
        index=["A√á√ÉO","ROMANCE LENTO","NOIR"].index(st.session_state.get("estilo_escrita","A√á√ÉO")),
        key="estilo_escrita",
    )
    # Elevar slider para aceitar 3
    st.slider("N√≠vel de calor (0=leve, 3=expl√≠cito)", 0, 3, value=int(st.session_state.get("nsfw_max_level",3)), key="nsfw_max_level")

    st.markdown("---")
    st.markdown("### ‚è±Ô∏è Comprimento/timeout")
    st.slider("Max tokens da resposta", 256, 2500, value=int(st.session_state.get("max_tokens_rsp", 1200)), step=32, key="max_tokens_rsp")
    st.slider("Timeout (segundos)", 60, 600, value=int(st.session_state.get("timeout_s", 300)), step=10, key="timeout_s")

    # Resumo r√°pido
    st.markdown("---")
    if st.button("üìù Gerar resumo do cap√≠tulo"):
        try:
            inter = carregar_interacoes(n=6)
            texto = "\n".join(f"{r['role']}: {r['content']}" for r in inter) if inter else ""
            prompt_resumo = (
                "Resuma o seguinte trecho como um cap√≠tulo de novela brasileiro, mantendo tom e emo√ß√µes.\n\n"
                + texto + "\n\nResumo:"
            )
            model_id_call = model_id_for_together(modelo_escolhido_id_ui) if provedor == "Together" else modelo_escolhido_id_ui
            r = requests.post(
                api_url,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json={"model": model_id_call, "messages": [{"role": "user", "content": prompt_resumo}], "max_tokens": 800, "temperature": 0.85},
                timeout=int(st.session_state.get("timeout_s", 300)),
            )
            if r.status_code == 200:
                resumo = r.json()["choices"][0]["message"]["content"].strip()
                st.session_state.resumo_capitulo = resumo
                salvar_resumo(resumo)
                st.success("Resumo gerado e salvo com sucesso!")
            else:
                st.error(f"Erro ao resumir: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro ao gerar resumo: {e}")

    # Mem√≥ria longa
    st.markdown("---")
    st.markdown("### üóÉÔ∏è Mem√≥ria Longa")
    st.checkbox("Usar mem√≥ria longa no prompt", value=st.session_state.get("use_memoria_longa", True), key="use_memoria_longa")
    st.slider("Top-K mem√≥rias", 1, 5, int(st.session_state.get("k_memoria_longa", 3)), 1, key="k_memoria_longa")
    st.slider("Limiar de similaridade", 0.50, 0.95, float(st.session_state.get("limiar_memoria_longa", 0.78)), 0.01, key="limiar_memoria_longa")
    if st.button("üíæ Salvar √∫ltima resposta como mem√≥ria"):
        ultimo_assist = ""
        for m in reversed(st.session_state.get("session_msgs", [])):
            if m.get("role") == "assistant":
                ultimo_assist = m.get("content", "").strip()
                break
        if ultimo_assist:
            ok = memoria_longa_salvar(ultimo_assist, tags="auto")
            st.success("Mem√≥ria de longo prazo salva!" if ok else "Falha ao salvar mem√≥ria.")
        else:
            st.info("Ainda n√£o h√° resposta do assistente nesta sess√£o.")

    # Hist√≥rico no prompt
    st.markdown("---")
    st.markdown("### üß© Hist√≥rico no prompt")
    st.slider("Intera√ß√µes do Sheets (N)", 10, 30, value=int(st.session_state.get("n_sheet_prompt", 15)), step=1, key="n_sheet_prompt")

    # ROMANCE MANUAL
    st.markdown("---")
    st.markdown("### üíû Romance Mary & J√¢nio (manual)")

    fase_default = mj_carregar_fase_inicial()
    options_fase = sorted(FASES_ROMANCE.keys())
    max_phase = max(options_fase)
    fase_ui_val = int(st.session_state.get("mj_fase", fase_default))
    fase_ui_val = max(min(fase_ui_val, max_phase), min(options_fase))
    fase_escolhida = st.select_slider("Fase do romance", options=options_fase, value=fase_ui_val, format_func=_fase_label, key="ui_mj_fase")
    if fase_escolhida != st.session_state.get("mj_fase", fase_default):
        mj_set_fase(fase_escolhida, persist=True)

    col_a, col_b = st.columns(2)
    with col_a:
        if st.button("‚ûï Avan√ßar 1 passo"):
            mj_set_fase(min(st.session_state.get("mj_fase", 0) + 1, max_phase), persist=True)
    with col_b:
        if st.button("‚Ü∫ Reiniciar (0)"):
            mj_set_fase(0, persist=True)

    st.slider("Micropassos por cena", 1, 3, value=int(st.session_state.get("max_avancos_por_cena", 1)), key="max_avancos_por_cena")

    st.markdown("### üéöÔ∏è Momento (manual)")
    options_momento = sorted(MOMENTOS.keys())
    mom_default = momento_carregar()
    mom_ui_val = int(st.session_state.get("momento", mom_default))
    mom_ui_val = max(min(mom_ui_val, max(options_momento)), min(options_momento))
    mom_ui = st.select_slider("Momento atual", options=options_momento, value=mom_ui_val, format_func=_momento_label, key="ui_momento")
    if mom_ui != st.session_state.get("momento", mom_default):
        momento_set(mom_ui, persist=True)

    st.caption("Regra: 1 microavan√ßo por cena. A fase s√≥ muda quando voc√™ decidir.")
    st.caption("Role a tela principal para ver intera√ß√µes anteriores.")

# =========================
# EXIBIR HIST√ìRICO (depois resumo)
# =========================

with st.container():
    interacoes = carregar_interacoes(n=20)
    for r in interacoes:
        role = r.get("role", "user")
        content = r.get("content", "")
        if role == "user":
            with st.chat_message("user"):
                st.markdown(content)
        else:
            with st.chat_message("assistant"):
                st.markdown(content)
    if st.session_state.get("resumo_capitulo"):
        with st.expander("üß† Resumo do cap√≠tulo (mais recente)"):
            st.markdown(st.session_state.resumo_capitulo)

# =========================
# ENVIO DO USU√ÅRIO + STREAMING (OpenRouter/Together) + FALLBACKS
# =========================

entrada = st.chat_input("Digite sua dire√ß√£o de cena...")

if entrada:
    # Atualiza Momento sugerido (opcional e seguro)
    try:
        mom_atual = int(st.session_state.get("momento", momento_carregar()))
        mom_sug = detectar_momento_sugerido(entrada, fallback=mom_atual)
        mom_novo = clamp_momento(mom_atual, mom_sug, int(st.session_state.get("max_avancos_por_cena",1)))
        if st.session_state.get("app_bloqueio_intimo", False):
            # n√£o retrocede automaticamente; apenas sobe at√© no m√°x. 1 passo
            mom_novo = clamp_momento(mom_atual, mom_sug, 1)
        momento_set(mom_novo, persist=True)
    except Exception:
        pass

    # salva a entrada e mant√©m hist√≥rico de sess√£o
salvar_interacao("user", entrada)
st.session_state.session_msgs.append({"role": "user", "content": entrada})

# constr√≥i prompt principal
prompt = construir_prompt_com_narrador()

# hist√≥rico curto (somente sess√£o atual; o prompt j√° inclui √∫ltimas do sheet)
historico = [{"role": m.get("role", "user"), "content": m.get("content", "")}
             for m in st.session_state.session_msgs]

# provedor + modelo
prov = st.session_state.get("provedor_ia", "OpenRouter")
if prov == "Together":
    endpoint = "https://api.together.xyz/v1/chat/completions"
    auth = st.secrets.get("TOGETHER_API_KEY", "")
    model_to_call = model_id_for_together(st.session_state.modelo_escolhido_id)
else:
    endpoint = "https://openrouter.ai/api/v1/chat/completions"
    auth = st.secrets.get("OPENROUTER_API_KEY", "")
    model_to_call = st.session_state.modelo_escolhido_id

if not auth:
    st.error("A chave de API do provedor selecionado n√£o foi definida em st.secrets.")
    st.stop()

# mensagens
system_pt = {
    "role": "system",
    "content": ("Responda em portugu√™s do Brasil. Evite conte√∫do meta. Mostre apenas a narrativa final ao leitor."),
}
messages = [system_pt, {"role": "system", "content": prompt}] + historico

payload = {
    "model": model_to_call,
    "messages": messages,
    "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
    "temperature": 0.9,
    "stream": True,
}

headers = {"Authorization": f"Bearer {auth}", "Content-Type": "application/json"}

left_sp, main_col, right_sp = st.columns[1,]
with main_col:
    with st.chat_message("assistant"):
        placeholder = st.empty()
        resposta_txt = ""  # texto bruto vindo do stream
        last_update = time.time()

        # Refor√ßo antecipado: mem√≥rias que ENTRARAM no prompt (topk + recorrentes)
        try:
            usados_prompt = []
            usados_prompt.extend(st.session_state.get("_ml_topk_texts", []))
            usados_prompt.extend(st.session_state.get("_ml_recorrentes", []))
            usados_prompt = [t for t in usados_prompt if t]
            if usados_prompt:
                memoria_longa_reforcar(usados_prompt)
        except Exception:
            pass

        # 1) STREAM
        try:
            with requests.post(
                endpoint, headers=headers, json=payload, stream=True,
                timeout=int(st.session_state.get("timeout_s", 300))
            ) as r:
                if r.status_code == 200:
                    for raw in r.iter_lines(decode_unicode=False):
                        if not raw:
                            continue
                        line = raw.decode("utf-8", errors="ignore").strip()
                        if not line.startswith("data:"):
                            continue
                        data = line[5:].strip()
                        if data == "[DONE]":
                            break
                        try:
                            j = json.loads(data)
                            delta = j["choices"]["delta"].get("content", "")
                            if not delta:
                                continue
                            resposta_txt += delta
                            if time.time() - last_update > 0.10:
                                out = render_tail(resposta_txt)
                                # Sanitiza√ß√£o NSFW liberada: usa n√≠vel configurado (at√© 3)
                                out = sanitize_explicit(out, max_level=int(st.session_state.get("nsfw_max_level", 3)), action="livre")
                                placeholder.markdown(out + "‚ñå")
                                last_update = time.time()
                        except Exception:
                            continue
                else:
                    st.error(f"Erro {('Together' if prov=='Together' else 'OpenRouter')}: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro no streaming: {e}")

        # 2) FALLBACKS se veio vazio
        visible_txt = sanitize_explicit(render_tail(resposta_txt).strip(), max_level=int(st.session_state.get("nsfw_max_level", 3)), action="livre")

        if not visible_txt:
            # 2a) retry sem stream
            try:
                r2 = requests.post(
                    endpoint, headers=headers,
                    json={**payload, "stream": False},
                    timeout=int(st.session_state.get("timeout_s", 300))
                )
                if r2.status_code == 200:
                    try:
                        resposta_txt = r2.json()["choices"]["message"]["content"].strip()
                    except Exception:
                        resposta_txt = ""
                    visible_txt = sanitize_explicit(render_tail(resposta_txt).strip(), max_level=int(st.session_state.get("nsfw_max_level", 3)), action="livre")
                else:
                    st.error(f"Fallback (sem stream) falhou: {r2.status_code} - {r2.text}")
            except Exception as e:
                st.error(f"Fallback (sem stream) erro: {e}")

        if not visible_txt:
            # 2b) retry sem o system extra (alguns modelos travam com system duplo)
            try:
                r3 = requests.post(
                    endpoint, headers=headers,
                    json={
                        "model": model_to_call,
                        "messages": [{"role": "system", "content": prompt}] + historico,
                        "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
                        "temperature": 0.9,
                        "stream": False,
                    },
                    timeout=int(st.session_state.get("timeout_s", 300))
                )
                if r3.status_code == 200:
                    try:
                        resposta_txt = r3.json()["choices"]["message"]["content"].strip()
                    except Exception:
                        resposta_txt = ""
                    visible_txt = sanitize_explicit(render_tail(resposta_txt).strip(), max_level=int(st.session_state.get("nsfw_max_level", 3)), action="livre")
                else:
                    st.error(f"Fallback (prompts limpos) falhou: {r3.status_code} - {r3.text}")
            except Exception as e:
                st.error(f"Fallback (prompts limpos) erro: {e}")

        # 3) Exibi√ß√£o final (texto filtrado de acordo com o n√≠vel)
        placeholder.markdown(visible_txt if visible_txt else "[Sem conte√∫do]")

        # 4) Valida√ß√£o de momento (aviso leve, n√£o bloqueia)
        try:
            viol = viola_momento(visible_txt, int(st.session_state.get("momento", 0)))
            if viol and st.session_state.get("app_bloqueio_intimo", False):
                st.info(f"‚ö†Ô∏è {viol}")
        except Exception:
            pass

        # 5) Valida√ß√£o sem√¢ntica (entrada do user vs resposta) usando texto vis√≠vel
        if len(st.session_state.session_msgs) >= 1 and visible_txt and visible_txt != "[Sem conte√∫do]":
            texto_anterior = st.session_state.session_msgs[-1]["content"]
            alerta = verificar_quebra_semantica_openai(texto_anterior, visible_txt)
            if alerta:
                st.info(alerta)

        # 6) Salvar resposta SEMPRE (usa o texto vis√≠vel)
        salvar_interacao("assistant", visible_txt or "[Sem conte√∫do]")
        st.session_state.session_msgs.append({"role": "assistant", "content": visible_txt or "[Sem conte√∫do]"})

        # 7) Refor√ßo de mem√≥rias usadas (p√≥s-resposta) com base no texto vis√≠vel
        try:
            usados = []
            topk_usadas = memoria_longa_buscar_topk(
                query_text=visible_txt,
                k=int(st.session_state.k_memoria_longa),
                limiar=float(st.session_state.limiar_memoria_longa),
            )
            for t, _sc, _sim, _rr in topk_usadas:
                usados.append(t)
            memoria_longa_reforcar(usados)
        except Exception:
            pass

