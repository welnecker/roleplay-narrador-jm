# main.py
# ============================================================
# Narrador JM ‚Äî Roleplay adulto (sem pornografia expl√≠cita)
# Compat√≠vel com o m√©todo antigo: GOOGLE_CREDS_JSON + oauth2client
# ============================================================

import os
import re
import json
import time
import math
import random
from datetime import datetime
from typing import List, Tuple, Dict, Any

import streamlit as st
import requests
import gspread
from oauth2client.service_account import ServiceAccountCredentials

import numpy as np

# (Opcional) Embeddings OpenAI para verifica√ß√£o sem√¢ntica/mem√≥ria longa
try:
    from openai import OpenAI
    OPENAI_CLIENT = OpenAI(api_key=st.secrets.get("OPENAI_API_KEY", ""))
    OPENAI_OK = bool(st.secrets.get("OPENAI_API_KEY"))
except Exception:
    OPENAI_CLIENT = None
    OPENAI_OK = False


# =========================
# CONFIG B√ÅSICA DO APP
# =========================
st.set_page_config(page_title="Narrador JM", page_icon="üé¨", layout="wide")

# Gate 18+
if "age_ok" not in st.session_state:
    st.session_state.age_ok = False

if not st.session_state.age_ok:
    st.title("üîû Conte√∫do adulto")
    st.caption("Narrativa adulta, sensual, **sem pornografia expl√≠cita**. Confirme para prosseguir.")
    ok = st.checkbox("Confirmo que tenho 18 anos ou mais e desejo prosseguir.")
    if ok:
        st.session_state.age_ok = True
    st.stop()


# =========================
# GOOGLE SHEETS ‚Äî MODO ANTIGO
# =========================
PLANILHA_ID_PADRAO = st.secrets.get("SPREADSHEET_ID", "").strip() or "1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM"

def conectar_planilha():
    """
    Conecta via GOOGLE_CREDS_JSON (modo antigo/est√°vel).
    Espera:
      - st.secrets["GOOGLE_CREDS_JSON"]: string JSON do service account
      - (opcional) st.secrets["SPREADSHEET_ID"]: ID da planilha
    """
    try:
        creds_dict = json.loads(st.secrets["GOOGLE_CREDS_JSON"])
        if "private_key" in creds_dict:
            creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        spreadsheet_id = st.secrets.get("SPREADSHEET_ID", "").strip() or PLANILHA_ID_PADRAO
        return client.open_by_key(spreadsheet_id)
    except Exception as e:
        st.error(f"Erro ao conectar √† planilha: {e}")
        return None

planilha = conectar_planilha()

# Abas esperadas
TAB_INTERACOES = "interacoes_jm"     # timestamp | role | content
TAB_PERFIL     = "perfil_jm"         # timestamp | resumo
TAB_MEMORIAS   = "memorias_jm"       # tipo | conteudo
TAB_ML         = "memoria_longa_jm"  # texto | embedding | tags | timestamp | score


def _ws(name: str, create_if_missing: bool = True):
    if not planilha:
        return None
    try:
        return planilha.worksheet(name)
    except Exception:
        if not create_if_missing:
            return None
        try:
            ws = planilha.add_worksheet(title=name, rows=5000, cols=10)
            # cria cabe√ßalhos padr√£o
            if name == TAB_INTERACOES:
                ws.append_row(["timestamp", "role", "content"])
            elif name == TAB_PERFIL:
                ws.append_row(["timestamp", "resumo"])
            elif name == TAB_MEMORIAS:
                ws.append_row(["tipo", "conteudo"])
            elif name == TAB_ML:
                ws.append_row(["texto", "embedding", "tags", "timestamp", "score"])
            return ws
        except Exception:
            return None


# =========================
# UTILIDADES: MEM√ìRIAS / HIST√ìRICO
# =========================
def carregar_memorias_brutas() -> Dict[str, List[str]]:
    """L√™ 'memorias_jm' e devolve um dict {tag_lower: [linhas]}."""
    try:
        aba = _ws(TAB_MEMORIAS, create_if_missing=False)
        if not aba: return {}
        regs = aba.get_all_records()
        buckets: Dict[str, List[str]] = {}
        for r in regs:
            tag = (r.get("tipo","") or "").strip().lower()
            txt = (r.get("conteudo","") or "").strip()
            if tag and txt:
                buckets.setdefault(tag, []).append(txt)
        return buckets
    except Exception as e:
        st.warning(f"Erro ao carregar mem√≥rias: {e}")
        return {}

def persona_block(nome: str, buckets: dict, max_linhas: int = 8) -> str:
    """Monta bloco compacto da persona (ordena por prefixos √∫teis)."""
    tag = f"[{nome}]"
    linhas = buckets.get(tag, [])
    ordem = ["OBJ:", "TAT:", "LV:", "VOZ:", "BIO:", "ROTINA:", "LACOS:", "APS:", "CONFLITOS:"]
    def peso(l):
        up = l.upper()
        for i, p in enumerate(ordem):
            if up.startswith(p):
                return i
        return len(ordem)
    linhas_ordenadas = sorted(linhas, key=peso)[:max_linhas]
    titulo = "J√¢nio" if nome in ("janio","j√¢nio") else "Mary" if nome=="mary" else nome.capitalize()
    return (f"{titulo}:\n- " + "\n- ".join(linhas_ordenadas)) if linhas_ordenadas else ""

def carregar_resumo_salvo() -> str:
    """Busca o √∫ltimo resumo da aba 'perfil_jm' (cabe√ßalho: timestamp | resumo)."""
    try:
        aba = _ws(TAB_PERFIL, create_if_missing=False)
        if not aba: return ""
        registros = aba.get_all_records()
        for r in reversed(registros):
            txt = (r.get("resumo") or "").strip()
            if txt:
                return txt
        return ""
    except Exception as e:
        st.warning(f"Erro ao carregar resumo salvo: {e}")
        return ""

def salvar_resumo(resumo: str):
    """Salva uma nova linha em 'perfil_jm' (timestamp | resumo)."""
    try:
        aba = _ws(TAB_PERFIL)
        if not aba: return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        aba.append_row([timestamp, resumo], value_input_option="RAW")
    except Exception as e:
        st.error(f"Erro ao salvar resumo: {e}")

def salvar_interacao(role: str, content: str):
    """Anexa uma intera√ß√£o na aba 'interacoes_jm'."""
    if not planilha:
        return
    try:
        aba = _ws(TAB_INTERACOES)
        if not aba: return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        aba.append_row([timestamp, role.strip(), content.strip()], value_input_option="RAW")
    except Exception as e:
        st.error(f"Erro ao salvar intera√ß√£o: {e}")

def carregar_interacoes(n: int = 20):
    """Carrega as √∫ltimas n intera√ß√µes (role, content) da aba interacoes_jm."""
    try:
        aba = _ws(TAB_INTERACOES, create_if_missing=False)
        if not aba: return []
        registros = aba.get_all_records()
        return registros[-n:] if len(registros) > n else registros
    except Exception as e:
        st.warning(f"Erro ao carregar intera√ß√µes: {e}")
        return []


# =========================
# EMBEDDINGS / SIMILARIDADE
# =========================
def gerar_embedding_openai(texto: str):
    if not OPENAI_OK:
        return None
    try:
        resp = OPENAI_CLIENT.embeddings.create(
            input=texto,
            model="text-embedding-3-small"
        )
        return np.array(resp.data[0].embedding, dtype=float)
    except Exception as e:
        st.error(f"Erro ao gerar embedding: {e}")
        return None

def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

def verificar_quebra_semantica_openai(texto1: str, texto2: str, limite=0.6) -> str:
    if not OPENAI_OK:
        return ""
    e1 = gerar_embedding_openai(texto1)
    e2 = gerar_embedding_openai(texto2)
    if e1 is None or e2 is None:
        return ""
    sim = cosine_similarity(e1, e2)
    if sim < limite:
        return f"‚ö†Ô∏è Baixa continuidade narrativa (similaridade: {sim:.2f})."
    return ""


# =========================
# MEM√ìRIA LONGA (Sheets + Embeddings/OpenAI opcional)
# =========================
def _sheet_ensure_memoria_longa():
    """Retorna a aba memoria_longa_jm se existir (n√£o cria automaticamente)."""
    return _ws(TAB_ML, create_if_missing=False)

def _serialize_vec(vec: np.ndarray) -> str:
    return json.dumps(vec.tolist(), separators=(",", ":"))

def _deserialize_vec(s: str) -> np.ndarray:
    try:
        return np.array(json.loads(s), dtype=float)
    except Exception:
        return np.zeros(1, dtype=float)

def memoria_longa_salvar(texto: str, tags: str = "") -> bool:
    """Salva uma mem√≥ria com embedding (se poss√≠vel) e score inicial."""
    aba = _sheet_ensure_memoria_longa()
    if not aba:
        st.warning("Aba 'memoria_longa_jm' n√£o encontrada ‚Äî crie com cabe√ßalhos: texto | embedding | tags | timestamp | score")
        return False
    emb = gerar_embedding_openai(texto)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    linha = [texto.strip(), _serialize_vec(emb) if emb is not None else "", (tags or "").strip(), ts, 1.0]
    try:
        aba.append_row(linha, value_input_option="RAW")
        return True
    except Exception as e:
        st.error(f"Erro ao salvar mem√≥ria longa: {e}")
        return False

def memoria_longa_listar_registros():
    """Retorna todos os registros da aba memoria_longa_jm (ou [])."""
    aba = _sheet_ensure_memoria_longa()
    if not aba:
        return []
    try:
        return aba.get_all_records()
    except Exception:
        return []

def _tokenize(s: str) -> set:
    return set(re.findall(r"[a-z√†-√∫0-9]+", (s or "").lower()))

def memoria_longa_buscar_topk(query_text: str, k: int = 3, limiar: float = 0.78):
    """Top-K mem√≥rias. Usa embeddings se existir; sen√£o, Jaccard simples."""
    aba = _sheet_ensure_memoria_longa()
    if not aba:
        return []
    try:
        dados = aba.get_all_records()
    except Exception as e:
        st.warning(f"Erro ao carregar memoria_longa_jm: {e}")
        return []

    if OPENAI_OK:
        q = gerar_embedding_openai(query_text)
    else:
        q = None

    candidatos = []
    for row in dados:
        texto = (row.get("texto") or "").strip()
        emb_s = (row.get("embedding") or "").strip()
        try:
            score = float(row.get("score", 1.0) or 1.0)
        except Exception:
            score = 1.0
        if not texto:
            continue

        if q is not None and emb_s:
            vec = _deserialize_vec(emb_s)
            if vec.ndim == 1 and vec.size >= 10:
                sim = float(np.dot(q, vec) / (np.linalg.norm(q) * np.linalg.norm(vec)))
            else:
                sim = 0.0
        else:
            # fallback lexical
            s1 = _tokenize(texto)
            s2 = _tokenize(query_text)
            sim = len(s1 & s2) / max(1, len(s1 | s2))

        if sim >= limiar:
            rr = 0.7 * sim + 0.3 * score
            candidatos.append((texto, score, sim, rr))
    candidatos.sort(key=lambda x: x[3], reverse=True)
    return candidatos[:k]

def memoria_longa_reforcar(textos_usados: list):
    """Aumenta o score das mem√≥rias usadas (pequeno refor√ßo)."""
    aba = _sheet_ensure_memoria_longa()
    if not aba or not textos_usados:
        return
    try:
        dados = aba.get_all_values()
        if not dados or len(dados) < 2:
            return
        headers = dados[0]
        idx_texto = headers.index("texto")
        idx_score = headers.index("score")
        for i, linha in enumerate(dados[1:], start=2):
            if len(linha) <= max(idx_texto, idx_score):
                continue
            t = (linha[idx_texto] or "").strip()
            if t in textos_usados:
                try:
                    sc = float(linha[idx_score] or 1.0)
                except Exception:
                    sc = 1.0
                sc = min(sc + 0.2, 2.0)
                aba.update_cell(i, idx_score + 1, sc)
    except Exception:
        pass


# =========================
# ROMANCE (FASES) + MOMENTO
# =========================
FASES_ROMANCE: Dict[int, Dict[str, str]] = {
    0: {"nome": "Estranhos",
        "permitidos": "olhares; near-miss (mesmo caf√©/rua/√¥nibus); detalhe do ambiente",
        "proibidos":  "troca de nomes; toques; conversa pessoal"},
    1: {"nome": "Percep√ß√£o",
        "permitidos": "cumprimento neutro; pergunta impessoal curta",
        "proibidos":  "contato f√≠sico; confid√™ncias"},
    2: {"nome": "Conhecidos",
        "permitidos": "troca de nomes; pequena ajuda; 1 pergunta pessoal leve",
        "proibidos":  "toque prolongado; encontro a s√≥s planejado"},
    3: {"nome": "Amizade",
        "permitidos": "conversa 10‚Äì20 min; caminhar juntos; troca de contatos; 1 gesto de afeto leve (com consentimento)",
        "proibidos":  "beijos; car√≠cias intimistas"},
    4: {"nome": "Confian√ßa / Quase",
        "permitidos": "confid√™ncias; abra√ßo com consentimento expresso; marcar encontro futuro claro",
        "proibidos":  "sexo; sexo oral/manual; pressa ou ‚Äúprovas de amor‚Äù f√≠sicas"},
    5: {"nome": "Compromisso / Encontro definitivo",
        "permitidos": "beijo prolongado; dormir juntos; consuma√ß√£o **impl√≠cita** (fade-to-black); manh√£ seguinte sugerida",
        "proibidos":  "descri√ß√£o expl√≠cita de atos sexuais; detalhes anat√¥micos; linguagem pornogr√°fica"},
}

FLAG_FASE_TXT_PREFIX = "FLAG: mj_fase="

def _fase_label(n: int) -> str:
    d = FASES_ROMANCE.get(int(n), FASES_ROMANCE[0])
    return f"{int(n)} ‚Äî {d['nome']}"

def mj_set_fase(n: int, persist: bool=True):
    n = max(0, min(5, int(n)))
    st.session_state.mj_fase = n
    if persist:
        try:
            memoria_longa_salvar(f"{FLAG_FASE_TXT_PREFIX}{n}", tags="[flag]")
        except Exception:
            pass

def mj_carregar_fase_inicial() -> int:
    if "mj_fase" in st.session_state:
        return int(st.session_state.mj_fase)
    try:
        recs = memoria_longa_listar_registros()
        for r in reversed(recs):
            t = (r.get("texto") or "").strip()
            if t.startswith(FLAG_FASE_TXT_PREFIX):
                n = int(t.split("=")[1])
                st.session_state.mj_fase = n
                return n
    except Exception:
        pass
    st.session_state.mj_fase = 0
    return 0


# --------- Motor de Momento ----------
MOMENTOS = {
    0: {"nome": "Aproxima√ß√£o log√≠stica",
        "objetivo": "um acompanha o outro (ex.: at√© o p√≠er), clima cordial",
        "permitidos": "gentilezas; proximidade leve; di√°logo casual",
        "proibidos": "declara√ß√£o; revela√ß√µes √≠ntimas; toques prolongados",
        "gatilhos": [r"\b(p[i√≠]er|acompanhar|vamos embora|te levo)\b"],
        "proximo": 1},
    1: {"nome": "Declara√ß√£o",
        "objetivo": "um deles declara amor/ import√¢ncia",
        "permitidos": "confiss√£o afetiva; sil√™ncio tenso; abra√ßo curto",
        "proibidos": "negocia√ß√£o sexual; tirar roupas; explora√ß√£o do corpo",
        "gatilhos": [r"\b(amo voc[e√™]|te amo|n[a√£]o paro de pensar)\b"],
        "proximo": 2},
    2: {"nome": "Revela√ß√£o sens√≠vel",
        "objetivo": "Mary revela que √© virgem / vulnerabilidade equivalente",
        "permitidos": "dizer 'sou virgem'; estipular limites; conforto m√∫tuo",
        "proibidos": "car√≠cias √≠ntimas; tirar roupas",
        "gatilhos": [r"\b(sou virgem|nunca fiz|meu limite)\b"],
        "proximo": 3},
    3: {"nome": "Consentimento expl√≠cito",
        "objetivo": "alinhamento de limites e um 'sim' claro",
        "permitidos": "nomear fronteiras; pedir/receber consentimento; decidir 'agora sim'",
        "proibidos": "descrever ato gr√°fico nesta cena",
        "gatilhos": [r"\b(consento|quero|vamos juntos|tudo bem pra voc[e√™])\b", r"\b(at[e√©] onde)\b"],
        "proximo": 4},
    4: {"nome": "Intimidade (el√≠ptica)",
        "objetivo": "intimidade sugerida (fade-to-black) / p√≥s-ato impl√≠cito",
        "permitidos": "beijos longos; proximidade forte; fade-to-black; manh√£ seguinte impl√≠cita",
        "proibidos": "descri√ß√£o gr√°fica de ato sexual ou anatomia",
        "gatilhos": [r"\b(quarto|cama|luz baixa|porta fechada|manh[a√£] seguinte)\b"],
        "proximo": 4},
}

def _momento_label(n: int) -> str:
    m = MOMENTOS.get(int(n), MOMENTOS[0])
    return f"{int(n)} ‚Äî {m['nome']}"

def detectar_momento_sugerido(texto: str, fallback: int = 0) -> int:
    t = (texto or "").lower()
    for i in range(4, -1, -1):
        for gx in MOMENTOS[i]["gatilhos"]:
            if re.search(gx, t, flags=re.IGNORECASE):
                return i
    if re.search(r"\b(tirar roupa|nu[a$o]?|penetra|sexo|boquete)\b", t, flags=re.IGNORECASE):
        return min(3, fallback)  # for√ßa parar em consentimento
    return fallback

def clamp_momento(atual: int, proposto: int, max_steps: int) -> int:
    if proposto > atual + max_steps:
        return atual + max_steps
    if proposto < atual:
        return max(proposto, atual - 1)
    return proposto

def momento_set(n: int, persist: bool = True):
    n = max(0, min(4, int(n)))
    st.session_state.momento = n
    if persist:
        try:
            memoria_longa_salvar(f"FLAG: mj_momento={n}", tags="[flag]")
        except Exception:
            pass

def momento_carregar() -> int:
    if "momento" in st.session_state:
        return int(st.session_state.momento)
    try:
        recs = memoria_longa_listar_registros()
        for r in reversed(recs):
            t = (r.get("texto") or "").strip()
            if t.startswith("FLAG: mj_momento="):
                n = int(t.split("=")[1])
                st.session_state.momento = n
                return n
    except Exception:
        pass
    st.session_state.momento = 0
    return 0

def viola_momento(texto: str, momento: int) -> str:
    t = (texto or "").lower()
    if momento <= 1 and re.search(r"\b(l[i√≠]ngua|tirar roupa|suti[a√£]|seio|penetra|boquete)\b", t):
        return "Ato √≠ntimo precoce antes de revela√ß√£o/consentimento."
    if momento == 2 and re.search(r"\b(tirar roupa|suti[a√£]|seio|penetra|boquete)\b", t):
        return "Ato √≠ntimo antes de consentimento expl√≠cito."
    if momento == 3 and re.search(r"\b(penetra|boquete|sexo explicitamente descrito)\b", t):
        return "Descri√ß√£o de ato em cena ‚Äî finalize com decis√£o/consentimento e corte el√≠ptico."
    return ""


# =========================
# PROVEDOR DE IA
# =========================
def api_config_for_provider(prov: str):
    if prov == "Together":
        url = "https://api.together.xyz/v1/chat/completions"
        key = st.secrets.get("TOGETHER_API_KEY", "")
        modelos = {
            "Llama-3.1 70B Instruct": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            "Qwen2.5 72B Instruct": "Qwen/Qwen2.5-72B-Instruct",
        }
    else:
        url = "https://openrouter.ai/api/v1/chat/completions"
        key = st.secrets.get("OPENROUTER_API_KEY", "")
        modelos = {
            "GPT-4.1 mini (OpenRouter)": "openai/gpt-4.1-mini",
            "Llama-3.1 70B (OpenRouter)": "meta-llama/llama-3.1-70b-instruct",
        }
    return url, key, modelos

def model_id_for_together(modelo_escolhido_id: str) -> str:
    return modelo_escolhido_id


# =========================
# PROMPT BUILDER
# =========================
def inserir_regras_mary_e_janio(prompt_base: str) -> str:
    calor = int(st.session_state.get("steam_level", st.session_state.get("nsfw_max_level", 1)))
    regras = f"""
‚öñÔ∏è Regras de coer√™ncia:
- Narre em terceira pessoa; n√£o se dirija ao leitor como "voc√™".
- Consentimento claro antes de qualquer gesto significativo.
- J√¢nio n√£o pressiona; respeita o ritmo de Mary.
- Linguagem sensual proporcional ao n√≠vel de calor ({calor}).
""".strip()
    fase = int(st.session_state.get("mj_fase", mj_carregar_fase_inicial()))
    if fase >= 5:
        regras += """
- Intimidade pode ser sugerida com corte el√≠ptico (fade-to-black); sem descri√ß√£o expl√≠cita de atos sexuais."""
    else:
        regras += """
- Sem consuma√ß√£o em cena; foque em progress√£o coerente."""
    return prompt_base + "\n" + regras

def construir_prompt_com_narrador() -> str:
    memos = carregar_memorias_brutas()
    perfil = carregar_resumo_salvo()

    fase = int(st.session_state.get("mj_fase", mj_carregar_fase_inicial()))
    fdata = FASES_ROMANCE.get(fase, FASES_ROMANCE[0])

    momento_atual = int(st.session_state.get("momento", momento_carregar()))
    mdata = MOMENTOS.get(momento_atual, MOMENTOS[0])
    proximo_nome = MOMENTOS[mdata["proximo"]]["nome"]

    estilo = st.session_state.get("estilo_escrita", "A√á√ÉO")

    # Hist√≥rico do Sheets
    n_hist = int(st.session_state.get("n_sheet_prompt", 15))
    hist = carregar_interacoes(n=n_hist)
    hist_txt = "\n".join(f"{r['role']}: {r['content']}" for r in hist) if hist else "(sem hist√≥rico)"

    # Mem√≥ria longa Top-K (texto apenas; se n√£o houver, "(nenhuma)")
    ml_topk_txt = "(nenhuma)"
    if st.session_state.get("use_memoria_longa", True) and hist:
        try:
            topk = memoria_longa_buscar_topk(
                query_text=hist[-1]["content"],
                k=int(st.session_state.get("k_memoria_longa", 3)),
                limiar=float(st.session_state.get("limiar_memoria_longa", 0.78)),
            )
            if topk:
                ml_topk_txt = "\n".join([f"- {t}" for (t, _sc, _sim, _rr) in topk])
                st.session_state["_ml_topk_texts"] = [t for (t, *_rest) in topk]
            else:
                st.session_state["_ml_topk_texts"] = []
        except Exception:
            st.session_state["_ml_topk_texts"] = []
    else:
        st.session_state["_ml_topk_texts"] = []

    recorrentes = [c for (t, lst) in memos.items() if t == "[all]" for c in lst]
    st.session_state["_ml_recorrentes"] = recorrentes

    dossie = []
    mary = persona_block("mary", memos, 8)
    janio = persona_block("janio", memos, 8)
    if mary: dossie.append(mary)
    if janio: dossie.append(janio)
    dossie_txt = "\n\n".join(dossie) if dossie else "(sem personas definidas)"

    prompt = f"""
Voc√™ √© o Narrador de um roleplay dram√°tico brasileiro, foque em Mary e J√¢nio. N√£o repita instru√ß√µes nem t√≠tulos.

### Dossi√™ (personas)
{dossie_txt}

### Diretrizes gerais (ALL)
{chr(10).join(['- '+c for c in recorrentes]) if recorrentes else '(vazio)'}

### Perfil (resumo mais recente)
{perfil or "(vazio)"}

### Hist√≥rico recente (planilha)
{hist_txt}

### Estilo
- Use o estilo **{estilo}**:
{("- Frases curtas, cortes r√°pidos, foco em gesto/ritmo.") if estilo=="A√á√ÉO" else
 ("- Atmosfera sombria, subtexto, sil√™ncio que pesa.") if estilo=="NOIR" else
 ("- Ritmo lento, tens√£o emocional, detalhes sensoriais (sem grafismo).")}

### Mem√≥ria longa ‚Äî Top-K relevantes
{ml_topk_txt}

### ‚è±Ô∏è Estado do romance (manual)
- Fase atual: {_fase_label(fase)}
- Permitidos: {fdata['permitidos']}
- Proibidos: {fdata['proibidos']}

### üéØ Momento dram√°tico (agora)
- Momento: {_momento_label(momento_atual)}
- Objetivo da cena: {mdata['objetivo']}
- Nesta cena, **permita**: {mdata['permitidos']}
- Evite/adiar: {mdata['proibidos']}
- **Micropassos:** avance no m√°ximo **{int(st.session_state.get("max_avancos_por_cena",1))}** subpasso(s) rumo a: {proximo_nome}.
- Se o roteirista pedir salto maior, **negocie**: nomeie limites, pe√ßa consentimento, e **prepare** a transi√ß√£o (n√£o pule etapas).

### Regra de sa√≠da
- Narre em **terceira pessoa**; n√£o fale com "voc√™".
- N√£o exiba r√≥tulos/meta (ex.: "Microconquista:", "Gancho:").
- Entregue uma cena coesa e finalizada; feche com um gancho impl√≠cito.
""".strip()

    prompt = inserir_regras_mary_e_janio(prompt)
    return prompt


# =========================
# FILTROS DE SA√çDA
# =========================
def render_tail(t: str) -> str:
    if not t: return ""
    # remove r√≥tulos meta e <think>
    t = re.sub(r'^\s*\**\s*(microconquista|gancho)\s*:\s*.*$', '', t, flags=re.IGNORECASE | re.MULTILINE)
    t = re.sub(r'<\s*think\s*>.*?<\s*/\s*think\s*>', '', t, flags=re.IGNORECASE | re.DOTALL)
    t = re.sub(r'\n{3,}', '\n\n', t).strip()
    return t

EXPL_PAT = re.compile(
    r"\b(seio[s]?|mamilos?|bunda|fio[- ]?dental|genit[a√°]lia|ere[c√ß][a√£]o|penetra[c√ß][a√£]o|"
    r"boquete|gozada|gozo|sexo oral|chupar|enfiar)\b",
    flags=re.IGNORECASE
)

def classify_nsfw_level(t: str) -> int:
    if EXPL_PAT.search(t or ""):
        return 3  # expl√≠cito
    if re.search(r"\b(cintura|pesco[c√ß]o|costas|beijo prolongado|respira[c√ß][a√£]o curta)\b", (t or ""), re.IGNORECASE):
        return 2
    if re.search(r"\b(olhar|aproximar|toque|m[a√£]os dadas|beijo)\b", (t or ""), re.IGNORECASE):
        return 1
    return 0

def sanitize_explicit(t: str, max_level: int, action: str) -> str:
    lvl = classify_nsfw_level(t)
    if lvl <= max_level:
        return t
    if action.lower().startswith("corte"):
        return re.sub(r"\s+$", "", t) + "\n\n[A luz baixa. O que vem depois fica fora de quadro.]"
    # Reescrita leve: remove linhas expl√≠citas
    t = re.sub(r"^.*" + EXPL_PAT.pattern + r".*$", "", t, flags=re.IGNORECASE | re.MULTILINE)
    t = re.sub(r'\n{3,}', '\n\n', t).strip()
    return t

def redact_for_logs(t: str) -> str:
    if not t: return ""
    t = re.sub(EXPL_PAT, "[‚Ä¶]", t, flags=re.IGNORECASE)
    return re.sub(r'\n{3,}', '\n\n', t).strip()

def resposta_valida(t: str) -> bool:
    if not t or t.strip() == "[Sem conte√∫do]":
        return False
    if len(t.strip()) < 5:
        return False
    return True


# =========================
# UI ‚Äî SIDEBAR
# =========================
with st.sidebar:
    st.title("üß≠ Painel do Roteirista")

   # -----------------------------------------------------------------------------
# PROVEDORES E MODELOS
# -----------------------------------------------------------------------------
MODELOS_OPENROUTER = {
    "üí¨ DeepSeek V3 ‚òÖ‚òÖ‚òÖ‚òÖ ($)": "deepseek/deepseek-chat-v3-0324",
    "üß† DeepSeek R1 0528 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$)": "deepseek/deepseek-r1-0528",
    "üß† DeepSeek R1T2 Chimera ‚òÖ‚òÖ‚òÖ‚òÖ (free)": "tngtech/deepseek-r1t2-chimera:free",
    "üß† GPT-4.1 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (1M ctx)": "openai/gpt-4.1",
    "üëë WizardLM 8x22B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$$)": "microsoft/wizardlm-2-8x22b",
    "üëë Qwen 235B 2507 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (PAID)": "qwen/qwen3-235b-a22b-07-25",
    "üëë EVA Qwen2.5 72B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-qwen-2.5-72b",
    "üëë EVA Llama 3.33 70B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-llama-3.33-70b",
    "üé≠ Nous Hermes 2 Yi 34B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ": "nousresearch/nous-hermes-2-yi-34b",
    "üî• MythoMax 13B ‚òÖ‚òÖ‚òÖ‚òÜ ($)": "gryphe/mythomax-l2-13b",
    "üíã LLaMA3 Lumimaid 8B ‚òÖ‚òÖ‚òÜ ($)": "neversleep/llama-3-lumimaid-8b",
    "üåπ Midnight Rose 70B ‚òÖ‚òÖ‚òÖ‚òÜ": "sophosympatheia/midnight-rose-70b",
    "üå∂Ô∏è Noromaid 20B ‚òÖ‚òÖ‚òÜ": "neversleep/noromaid-20b",
    "üíÄ Mythalion 13B ‚òÖ‚òÖ‚òÜ": "pygmalionai/mythalion-13b",
    "üêâ Anubis 70B ‚òÖ‚òÖ‚òÜ": "thedrummer/anubis-70b-v1.1",
    "üßö Rocinante 12B ‚òÖ‚òÖ‚òÜ": "thedrummer/rocinante-12b",
    "üç∑ Magnum v2 72B ‚òÖ‚òÖ‚òÜ": "anthracite-org/magnum-v2-72b",
}

MODELOS_TOGETHER_UI = {
    "üß† Qwen3 Coder 480B (Together)": "togethercomputer/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "üëë Mixtral 8x7B v0.1 (Together)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "üëë Perplexity R1-1776 (Together)": "perplexity-ai/r1-1776",
}

def model_id_for_together(api_ui_model_id: str) -> str:
    # normaliza algumas varia√ß√µes comuns de UI ‚Üí endpoint
    key = api_ui_model_id.strip()
    if "Qwen3-Coder-480B-A35B-Instruct-FP8" in key:
        return "Qwen/Qwen3-Coder-480B-Instruct-FP8"
    if key.lower().startswith("mistralai/mixtral-8x7b-instruct-v0.1"):
        return "mistralai/Mixtral-8x7B-Instruct-v0.1"
    return key

def api_config_for_provider(provider: str):
    if provider == "OpenRouter":
        return (
            "https://openrouter.ai/api/v1/chat/completions",
            st.secrets.get("OPENROUTER_API_KEY", ""),
            MODELOS_OPENROUTER,
        )
    else:
        return (
            "https://api.together.xyz/v1/chat/completions",
            st.secrets.get("TOGETHER_API_KEY", ""),
            MODELOS_TOGETHER_UI,
        )


# -----------------------------------------------------------------------------
# UI ‚Äì CABE√áALHO E CONTROLES
# -----------------------------------------------------------------------------
st.title("üé¨ Narrador JM")
st.subheader("Voc√™ √© o roteirista. Digite uma dire√ß√£o de cena. A IA narrar√° Mary e J√¢nio.")
st.markdown("---")

# Estado inicial
if "resumo_capitulo" not in st.session_state:
    st.session_state.resumo_capitulo = carregar_resumo_salvo()
if "session_msgs" not in st.session_state:
    st.session_state.session_msgs = []
if "use_memoria_longa" not in st.session_state:
    st.session_state.use_memoria_longa = True
if "k_memoria_longa" not in st.session_state:
    st.session_state.k_memoria_longa = 3
if "limiar_memoria_longa" not in st.session_state:
    st.session_state.limiar_memoria_longa = 0.78
if "app_bloqueio_intimo" not in st.session_state:
    st.session_state.app_bloqueio_intimo = False
if "app_emocao_oculta" not in st.session_state:
    st.session_state.app_emocao_oculta = "nenhuma"
if "mj_fase" not in st.session_state:
    st.session_state.mj_fase = mj_carregar_fase_inicial()

# Linha de op√ß√µes r√°pidas
col1, col2 = st.columns([3, 2])
with col1:
    st.markdown("#### üìñ √öltimo resumo salvo:")
    st.info(st.session_state.resumo_capitulo or "Nenhum resumo dispon√≠vel.")
with col2:
    st.markdown("#### ‚öôÔ∏è Op√ß√µes")
    st.checkbox(
        "Bloquear avan√ßos √≠ntimos sem ordem",
        value=st.session_state.app_bloqueio_intimo,
        key="ui_bloqueio_intimo",
    )
    st.selectbox(
        "üé≠ Emo√ß√£o oculta",
        ["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"],
        index=["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"].index(st.session_state.app_emocao_oculta),
        key="ui_app_emocao_oculta",
    )
    st.session_state.app_bloqueio_intimo = st.session_state.get("ui_bloqueio_intimo", False)
    st.session_state.app_emocao_oculta = st.session_state.get("ui_app_emocao_oculta", "nenhuma")


# -----------------------------------------------------------------------------
# Sidebar ‚Äì Provedor, modelos, resumo, mem√≥ria longa e ROMANCE MANUAL
# -----------------------------------------------------------------------------
with st.sidebar:
    st.title("üß≠ Painel do Roteirista")

    provedor = st.radio("üåê Provedor", ["OpenRouter", "Together"], index=0, key="provedor_ia")
    api_url, api_key, modelos_map = api_config_for_provider(provedor)

    if not api_key:
        st.warning("‚ö†Ô∏è API key ausente para o provedor selecionado. Defina em st.secrets.")

    modelo_nome = st.selectbox("ü§ñ Modelo de IA", list(modelos_map.keys()), index=0, key="modelo_nome_ui")
    modelo_escolhido_id_ui = modelos_map[modelo_nome]
    st.session_state.modelo_escolhido_id = modelo_escolhido_id_ui

    # ---- Comprimento / timeout ----
    st.markdown("---")
    st.markdown("### ‚è±Ô∏è Comprimento/timeout")
    st.slider(
        "Max tokens da resposta",
        256, 2500,
        value=int(st.session_state.get("max_tokens_rsp", 1200)),
        step=32,
        key="max_tokens_rsp",
    )
    st.slider(
        "Timeout (segundos)",
        60, 600,
        value=int(st.session_state.get("timeout_s", 300)),
        step=10,
        key="timeout_s",
    )

    # ---- Resumo r√°pido ----
    st.markdown("---")
    if st.button("üìù Gerar resumo do cap√≠tulo"):
        try:
            inter = carregar_interacoes(n=6)
            texto = "\n".join(f"{r['role']}: {r['content']}" for r in inter) if inter else ""
            prompt_resumo = (
                "Resuma o seguinte trecho como um cap√≠tulo de novela brasileiro, mantendo tom e emo√ß√µes.\n\n"
                + texto + "\n\nResumo:"
            )
            model_id_call = model_id_for_together(modelo_escolhido_id_ui) if provedor == "Together" else modelo_escolhido_id_ui
            r = requests.post(
                api_url,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json={
                    "model": model_id_call,
                    "messages": [{"role": "user", "content": prompt_resumo}],
                    "max_tokens": 800,
                    "temperature": 0.85,
                },
                timeout=int(st.session_state.get("timeout_s", 300)),
            )
            if r.status_code == 200:
                resumo = r.json()["choices"][0]["message"]["content"].strip()
                st.session_state.resumo_capitulo = resumo
                salvar_resumo(resumo)
                st.success("Resumo gerado e salvo com sucesso!")
            else:
                st.error(f"Erro ao resumir: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro ao gerar resumo: {e}")

    # ---- Mem√≥ria longa ----
    st.markdown("---")
    st.markdown("### üóÉÔ∏è Mem√≥ria Longa")
    st.checkbox(
        "Usar mem√≥ria longa no prompt",
        value=st.session_state.get("use_memoria_longa", True),
        key="use_memoria_longa",
    )
    st.slider(
        "Top-K mem√≥rias",
        1, 5,
        int(st.session_state.get("k_memoria_longa", 3)),
        1,
        key="k_memoria_longa",
    )
    st.slider(
        "Limiar de similaridade",
        0.50, 0.95,
        float(st.session_state.get("limiar_memoria_longa", 0.78)),
        0.01,
        key="limiar_memoria_longa",
    )
    if st.button("üíæ Salvar √∫ltima resposta como mem√≥ria"):
        ultimo_assist = ""
        for m in reversed(st.session_state.get("session_msgs", [])):
            if m.get("role") == "assistant":
                ultimo_assist = m.get("content", "").strip()
                break
        if ultimo_assist:
            ok = memoria_longa_salvar(ultimo_assist, tags="auto")
            st.success("Mem√≥ria de longo prazo salva!" if ok else "Falha ao salvar mem√≥ria.")
        else:
            st.info("Ainda n√£o h√° resposta do assistente nesta sess√£o.")

    # ---- Hist√≥rico no prompt ----
    st.markdown("---")
    st.markdown("### üß© Hist√≥rico no prompt")
    st.slider(
        "Intera√ß√µes do Sheets (N)",
        10, 30,
        value=int(st.session_state.get("n_sheet_prompt", 15)),
        step=1,
        key="n_sheet_prompt",
    )

    # ---- ROMANCE MANUAL ----
    st.markdown("---")
    st.markdown("### üíû Romance Mary & J√¢nio (manual)")
    fase_default = mj_carregar_fase_inicial()
    fase_escolhida = st.select_slider(
        "Fase do romance",
        options=[0,1,2,3,4],
        value=int(st.session_state.get("mj_fase", fase_default)),
        format_func=_fase_label,
        key="ui_mj_fase"
    )
    if fase_escolhida != st.session_state.get("mj_fase", fase_default):
        mj_set_fase(fase_escolhida, persist=True)

    col_a, col_b = st.columns(2)
    with col_a:
        if st.button("‚ûï Avan√ßar 1 passo"):
            mj_set_fase(min(4, int(st.session_state.get("mj_fase", 0)) + 1), persist=True)
    with col_b:
        if st.button("‚Ü∫ Reiniciar (0)"):
            mj_set_fase(0, persist=True)

    st.caption("Regra: 1 microavan√ßo por cena. A fase s√≥ muda quando voc√™ decidir.")
    st.caption("Role a tela principal para ver intera√ß√µes anteriores.")


# -----------------------------------------------------------------------------
# EXIBIR HIST√ìRICO RECENTE (primeiro intera√ß√µes, depois resumo)
# -----------------------------------------------------------------------------
with st.container():
    interacoes = carregar_interacoes(n=20)
    for r in interacoes:
        role = r.get("role", "user")
        content = r.get("content", "")
        if role == "user":
            with st.chat_message("user"):
                st.markdown(content)
        else:
            with st.chat_message("assistant"):
                st.markdown(content)

# Resumo no fim da tela
if st.session_state.get("resumo_capitulo"):
    with st.expander("üß† Resumo do cap√≠tulo (mais recente)"):
        st.markdown(st.session_state.resumo_capitulo)


# -----------------------------------------------------------------------------
# ENVIO DO USU√ÅRIO + STREAMING (OpenRouter/Together) + FALLBACKS
# -----------------------------------------------------------------------------
entrada = st.chat_input("Digite sua dire√ß√£o de cena...")
if entrada:
    # salva a entrada e mant√©m hist√≥rico de sess√£o
    salvar_interacao("user", entrada)
    st.session_state.session_msgs.append({"role": "user", "content": entrada})

    # constr√≥i prompt principal
    prompt = construir_prompt_com_narrador()

    # hist√≥rico curto (somente sess√£o atual; o prompt j√° inclui √∫ltimas do sheet)
    historico = [{"role": m.get("role", "user"), "content": m.get("content", "")}
                 for m in st.session_state.session_msgs]

    # provedor + modelo
    prov = st.session_state.get("provedor_ia", "OpenRouter")
    if prov == "Together":
        endpoint = "https://api.together.xyz/v1/chat/completions"
        auth = st.secrets.get("TOGETHER_API_KEY", "")
        model_to_call = model_id_for_together(st.session_state.modelo_escolhido_id)
    else:
        endpoint = "https://openrouter.ai/api/v1/chat/completions"
        auth = st.secrets.get("OPENROUTER_API_KEY", "")
        model_to_call = st.session_state.modelo_escolhido_id

    if not auth:
        st.error("A chave de API do provedor selecionado n√£o foi definida em st.secrets.")
        st.stop()

    # mensagens
    system_pt = {
        "role": "system",
        "content": (
            "Responda em portugu√™s do Brasil. Evite conte√∫do meta. "
            "Mostre apenas a narrativa final ao leitor."
        ),
    }
    messages = [system_pt, {"role": "system", "content": prompt}] + historico

    payload = {
        "model": model_to_call,
        "messages": messages,
        "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
        "temperature": 0.9,
        "stream": True,
    }
    headers = {"Authorization": f"Bearer {auth}", "Content-Type": "application/json"}

    # --- filtro anti-meta (aplica na exibi√ß√£o e no salvamento) ---
    def render_tail(t: str) -> str:
        import re
        if not t: return ""
        t = re.sub(r'^\s*\**\s*(microconquista|gancho)\s*:\s*.*$','', t, flags=re.IGNORECASE|re.MULTILINE)
        t = re.sub(r'<\s*think\s*>.*?<\s*/\s*think\s*>', '', t, flags=re.IGNORECASE|re.DOTALL)
        t = re.sub(r'\n{3,}', '\n\n', t)
        return t.strip()

    with st.chat_message("assistant"):
        placeholder = st.empty()
        resposta_txt = ""     # texto bruto vindo do stream
        last_update = time.time()

        # Refor√ßo antecipado: mem√≥rias que ENTRARAM no prompt (topk + recorrentes)
        try:
            usados_prompt = []
            usados_prompt.extend(st.session_state.get("_ml_topk_texts", []))
            usados_prompt.extend(st.session_state.get("_ml_recorrentes", []))
            usados_prompt = [t for t in usados_prompt if t]
            if usados_prompt:
                memoria_longa_reforcar(usados_prompt)
        except Exception:
            pass

        # 1) STREAM
        try:
            with requests.post(
                endpoint, headers=headers, json=payload, stream=True,
                timeout=int(st.session_state.get("timeout_s", 300))
            ) as r:
                if r.status_code == 200:
                    for raw in r.iter_lines(decode_unicode=False):
                        if not raw:
                            continue
                        line = raw.decode("utf-8", errors="ignore").strip()
                        if not line.startswith("data:"):
                            continue
                        data = line[5:].strip()
                        if data == "[DONE]":
                            break
                        try:
                            j = json.loads(data)
                            delta = j["choices"][0]["delta"].get("content", "")
                            if not delta:
                                continue
                            resposta_txt += delta
                            if time.time() - last_update > 0.10:
                                placeholder.markdown(render_tail(resposta_txt) + "‚ñå")
                                last_update = time.time()
                        except Exception:
                            continue
                else:
                    st.error(f"Erro {('Together' if prov=='Together' else 'OpenRouter')}: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro no streaming: {e}")

        # 2) FALLBACKS se veio vazio
        visible_txt = render_tail(resposta_txt).strip()
        if not visible_txt:
            # 2a) retry sem stream
            try:
                r2 = requests.post(
                    endpoint, headers=headers,
                    json={**payload, "stream": False},
                    timeout=int(st.session_state.get("timeout_s", 300))
                )
                if r2.status_code == 200:
                    try:
                        resposta_txt = r2.json()["choices"][0]["message"]["content"].strip()
                    except Exception:
                        resposta_txt = ""
                    visible_txt = render_tail(resposta_txt).strip()
                else:
                    st.error(f"Fallback (sem stream) falhou: {r2.status_code} - {r2.text}")
            except Exception as e:
                st.error(f"Fallback (sem stream) erro: {e}")

        if not visible_txt:
            # 2b) retry sem o system extra (alguns modelos travam com system duplo)
            try:
                r3 = requests.post(
                    endpoint, headers=headers,
                    json={
                        "model": model_to_call,
                        "messages": [{"role": "system", "content": prompt}] + historico,
                        "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
                        "temperature": 0.9,
                        "stream": False,
                    },
                    timeout=int(st.session_state.get("timeout_s", 300))
                )
                if r3.status_code == 200:
                    try:
                        resposta_txt = r3.json()["choices"][0]["message"]["content"].strip()
                    except Exception:
                        resposta_txt = ""
                    visible_txt = render_tail(resposta_txt).strip()
                else:
                    st.error(f"Fallback (prompts limpos) falhou: {r3.status_code} - {r3.text}")
            except Exception as e:
                st.error(f"Fallback (prompts limpos) erro: {e}")

        # 3) Exibi√ß√£o final (texto filtrado)
        placeholder.markdown(visible_txt if visible_txt else "[Sem conte√∫do]")

        # 4) Valida√ß√£o sem√¢ntica (entrada do user vs resposta) usando texto vis√≠vel
        if len(st.session_state.session_msgs) >= 1 and visible_txt and visible_txt != "[Sem conte√∫do]":
            texto_anterior = st.session_state.session_msgs[-1]["content"]
            alerta = verificar_quebra_semantica_openai(texto_anterior, visible_txt)
            if alerta:
                st.info(alerta)

        # 5) Salvar resposta SEMPRE (usa o texto limpo/vis√≠vel)
        salvar_interacao("assistant", visible_txt or "[Sem conte√∫do]")
        st.session_state.session_msgs.append({"role": "assistant", "content": visible_txt or "[Sem conte√∫do]"})

        # 6) Refor√ßo de mem√≥rias usadas (p√≥s-resposta) com base no texto vis√≠vel
        try:
            usados = []
            topk_usadas = memoria_longa_buscar_topk(
                query_text=visible_txt,
                k=int(st.session_state.k_memoria_longa),
                limiar=float(st.session_state.limiar_memoria_longa),
            )
            for t, _sc, _sim, _rr in topk_usadas:
                usados.append(t)
            memoria_longa_reforcar(usados)
        except Exception:
            pass
