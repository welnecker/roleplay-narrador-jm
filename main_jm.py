# ============================================================
# Narrador JM ‚Äî Roleplay adulto (sem pornografia expl√≠cita)
# Compat√≠vel com o m√©todo antigo: GOOGLE_CREDS_JSON + oauth2client
# ============================================================

import os
import re
import json
import time
import random

from datetime import datetime
from typing import List, Tuple, Dict, Any

import streamlit as st
import requests
import gspread

from oauth2client.service_account import ServiceAccountCredentials
import numpy as np
from gspread.exceptions import APIError

# ---- Backoff p/ 429 do Sheets ----
def _retry_429(callable_fn, *args, _retries=5, _base=0.6, **kwargs):
    for i in range(_retries):
        try:
            return callable_fn(*args, **kwargs)
        except APIError as e:
            msg = str(e)
            if "429" in msg or "quota" in msg.lower():
                time.sleep((_base * (2 ** i)) + random.uniform(0, 0.25))
                continue
            raise
    return callable_fn(*args, **kwargs)

@st.cache_data(ttl=45, show_spinner=False)
def _sheet_all_records_cached(sheet_name: str):
    ws = _ws(sheet_name, create_if_missing=False)
    if not ws:
        return []
    return _retry_429(ws.get_all_records)

@st.cache_data(ttl=45, show_spinner=False)
def _sheet_all_values_cached(sheet_name: str):
    ws = _ws(sheet_name, create_if_missing=False)
    if not ws:
        return []
    return _retry_429(ws.get_all_values)

def _invalidate_sheet_caches():
    try:
        _sheet_all_records_cached.clear()
        _sheet_all_values_cached.clear()
    except Exception:
        pass

# (Opcional) Embeddings OpenAI para verifica√ß√£o sem√¢ntica/mem√≥ria longa
try:
    from openai import OpenAI
    OPENAI_CLIENT = OpenAI(api_key=st.secrets.get("OPENAI_API_KEY", ""))
    OPENAI_OK = bool(st.secrets.get("OPENAI_API_KEY"))
except Exception:
    OPENAI_CLIENT = None
    OPENAI_OK = False

# =========================
# CONFIG B√ÅSICA DO APP
# =========================

PLANILHA_ID_PADRAO = st.secrets.get("SPREADSHEET_ID", "").strip() or "1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM"
TAB_INTERACOES = "interacoes_jm"
TAB_PERFIL = "perfil_jm"
TAB_MEMORIAS = "memoria_jm"
TAB_ML = "memoria_longa_jm"
TAB_TEMPLATES = "templates_jm"

def conectar_planilha():
    try:
        creds_dict = json.loads(st.secrets["GOOGLE_CREDS_JSON"])
        if "private_key" in creds_dict:
            creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        spreadsheet_id = st.secrets.get("SPREADSHEET_ID", "").strip() or PLANILHA_ID_PADRAO
        return client.open_by_key(spreadsheet_id)
    except Exception as e:
        st.error(f"Erro ao conectar √† planilha: {e}")
        return None

planilha = conectar_planilha()

def _ws(name: str, create_if_missing: bool = True):
    if not planilha:
        return None
    try:
        return planilha.worksheet(name)
    except Exception:
        if not create_if_missing:
            return None
        try:
            ws = planilha.add_worksheet(title=name, rows=5000, cols=10)
            if name == TAB_INTERACOES:
                _retry_429(ws.append_row, ["timestamp", "role", "content"])
            elif name == TAB_PERFIL:
                _retry_429(ws.append_row, ["timestamp", "resumo"])
            elif name == TAB_MEMORIAS:
                _retry_429(ws.append_row, ["tipo", "conteudo", "timestamp"])
            elif name == TAB_ML:
                _retry_429(ws.append_row, ["texto", "embedding", "tags", "timestamp", "score"])
            elif name == TAB_TEMPLATES:
                _retry_429(ws.append_row, ["template", "etapa", "texto"])
            return ws
        except Exception:
            return None

    def carregar_templates_planilha():
        """Carrega todos os templates sequenciais da aba templates_jm em {template:[etapa1, etapa2,...]}"""
        try:
            ws = _ws(TAB_TEMPLATES, create_if_missing=False)
            if not ws:
                return {}
            rows = _sheet_all_records_cached(TAB_TEMPLATES)
            templates = {}
            for row in rows:
                r = { (k or "").strip().lower(): (v or "") for k, v in row.items() }
                nome = r.get("template", "").strip()
                etapa_str = r.get("etapa", "1")
                try:
                    etapa = int(etapa_str)
                except Exception:
                    etapa = 1
                texto = r.get("texto", "").strip()
                if nome and texto:
                    templates.setdefault(nome, []).append((etapa, texto))
            for nome in templates:
                templates[nome].sort(key=lambda x: x[0])
                templates[nome] = [t[1] for t in templates[nome]]
            return templates
        except Exception as e:
            st.warning(f"Erro ao carregar templates do Sheets: {e}")
            return {}

# --- INICIALIZE AS VARI√ÅVEIS DE TEMPLATE ---
if "templates_jm" not in st.session_state:
    st.session_state.templates_jm = carregar_templates_planilha()
if "template_ativo" not in st.session_state:
    st.session_state.template_ativo = None
if "etapa_template" not in st.session_state:
    st.session_state.etapa_template = 0
# =====
# UTILIDADES: MEM√ìRIAS / HIST√ìRICO
# =====

# Observa√ß√£o 1: o c√≥digo espera TAB_MEMORIAS = "memoria_jm"
# Ex.: TAB_MEMORIAS = "memoria_jm"

def _normalize_tag(raw: str) -> str:
    """Normaliza 'tipo' para o formato com colchetes: [all], [mary], [janio], etc."""
    t = (raw or "").strip().lower()
    if not t:
        return ""
    return t if t.startswith("[") else f"[{t}]"

def _parse_ts(s: str) -> str:
    """
    Normaliza timestamp para 'YYYY-MM-DD HH:MM:SS'; se vier vazio/ruim, usa 'agora'.
    Importante: strings nesse formato s√£o compar√°veis lexicograficamente.
    """
    s = (s or "").strip()
    try:
        datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
        return s
    except Exception:
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def carregar_memorias_brutas() -> Dict[str, List[dict]]:
    """
    L√™ 'memoria_jm' (cabe√ßalho: tipo | conteudo | timestamp) e devolve
    {tag: [ {"conteudo":..., "timestamp":...} ]} com tags normalizadas ([all],[mary],[janio]) e
    timestamp padronizado.
    """
    try:
        regs = _sheet_all_records_cached(TAB_MEMORIAS)  # espera "memoria_jm"
        buckets: Dict[str, List[dict]] = {}
        for r in regs:
            tag = _normalize_tag(r.get("tipo"))
            txt = (r.get("conteudo") or "").strip()
            ts = (r.get("timestamp") or "").strip()
            if tag and txt:
                buckets.setdefault(tag, []).append({"conteudo": txt, "timestamp": ts})
        return buckets
    except Exception as e:
        st.warning(f"Erro ao carregar mem√≥rias: {e}")
        return {}

def persona_block(nome: str, buckets: dict, max_linhas: int = 8) -> str:
    """
    Monta bloco compacto da persona (ordena por prefixos √∫teis).
    Observa√ß√£o 2: usa tags no formato [mary], [janio].
    """
    tag = f"[{nome}]"
    linhas = buckets.get(tag, [])
    ordem = ["OBJ:", "TAT:", "LV:", "VOZ:", "BIO:", "ROTINA:", "LACOS:", "APS:", "CONFLITOS:"]

    def peso(linha_dict):
        l = linha_dict["conteudo"]
        up = l.upper()
        for i, p in enumerate(ordem):
            if up.startswith(p):
                return i
        return len(ordem)

    linhas_ordenadas = sorted(linhas, key=peso)[:max_linhas]
    titulo = "J√¢nio" if nome in ("janio", "j√¢nio") else "Mary" if nome == "mary" else nome.capitalize()
    return (
        f"{titulo}:\n- " + "\n- ".join(linha['conteudo'] for linha in linhas_ordenadas)
    ) if linhas_ordenadas else ""


def persona_block_temporal(nome: str, buckets: dict, ate_ts: str, max_linhas: int = 8) -> str:
    """
    Vers√£o temporal do bloco de persona.
    Usa apenas mem√≥rias com timestamp <= ate_ts (se houver timestamp).
    Mant√©m compatibilidade com registros sem timestamp (s√£o inclu√≠dos).
    """
    tag = f"[{nome}]"
    linhas = []
    for d in buckets.get(tag, []) or []:
        if not isinstance(d, dict):
            continue
        c = (d.get("conteudo") or "").strip()
        ts = (d.get("timestamp") or "").strip()  # pode estar vazio
        if not c:
            continue
        # Se houver timestamp e corte temporal, exclui mem√≥rias "do futuro"
        if ts and ate_ts and ts > ate_ts:
            continue
        linhas.append((ts, c))

    # Ordena por timestamp crescente (strings ISO ordenam lexicograficamente)
    # Registros sem timestamp ("") ficam no in√≠cio.
    linhas.sort(key=lambda x: x[0])

    # Pega as √∫ltimas N (mais recentes at√© o corte)
    ult = [c for _, c in linhas][-max_linhas:]
    if not ult:
        return ""

    titulo = "J√¢nio" if nome in ("janio", "j√¢nio") else "Mary" if nome == "mary" else nome.capitalize()
    return f"{titulo}:\n- " + "\n- ".join(ult)


def carregar_resumo_salvo() -> str:
    """
    Busca o √∫ltimo resumo da aba 'perfil_jm' (cabe√ßalho: timestamp | resumo) com cache TTL.
    """
    try:
        registros = _sheet_all_records_cached(TAB_PERFIL)
        for r in reversed(registros):
            txt = (r.get("resumo") or "").strip()
            if txt:
                return txt
        return ""
    except Exception as e:
        st.warning(f"Erro ao carregar resumo salvo: {e}")
        return ""


def salvar_resumo(resumo: str):
    """
    Salva uma nova linha em 'perfil_jm' (timestamp | resumo) e invalida caches.
    """
    try:
        aba = _ws(TAB_PERFIL)
        if not aba:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        _retry_429(aba.append_row, [timestamp, resumo], value_input_option="RAW")
        _invalidate_sheet_caches()
    except Exception as e:
        st.error(f"Erro ao salvar resumo: {e}")


def carregar_interacoes(n: int = 20):
    """
    Carrega √∫ltimas n intera√ß√µes (role, content) usando cache de sess√£o
    para evitar leituras repetidas.
    """
    cache = st.session_state.get("_cache_interacoes", None)
    if cache is None:
        regs = _sheet_all_records_cached(TAB_INTERACOES)
        st.session_state["_cache_interacoes"] = regs
        cache = regs
    return cache[-n:] if len(cache) > n else cache


def salvar_interacao(role: str, content: str):
    """
    Append no Sheets + atualiza cache local (sem reler) com backoff 429.
    (Observa√ß√£o 3: garante timestamp no padr√£o e corrige o else do cache.)
    """
    if not planilha:
        return
    try:
        aba = _ws(TAB_INTERACOES)
        if not aba:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # Observa√ß√£o 3 (formato)
        row_role = f"{role or ''}".strip()
        row_content = f"{content or ''}".strip()
        row = [timestamp, row_role, row_content]
        _retry_429(aba.append_row, row, value_input_option="RAW")

        # atualiza cache local
        lst = st.session_state.get("_cache_interacoes")
        if isinstance(lst, list):
            lst.append({"timestamp": timestamp, "role": row_role, "content": row_content})
        else:
            # (fix) cria corretamente a primeira entrada do cache
            st.session_state["_cache_interacoes"] = [{
                "timestamp": timestamp,
                "role": row_role,
                "content": row_content,
            }]

        _invalidate_sheet_caches()
    except Exception as e:
        st.error(f"Erro ao salvar intera√ß√£o: {e}")

# = BUSCA TEMPORAL DE MEM√ìRIAS =
def buscar_status_persona_ate(persona_tag: str, momento_ts: str, buckets: dict) -> List[str]:
    """
    Busca os tra√ßos mais recentes da persona at√© o timestamp informado.
    persona_tag: ex: '[mary]' (ser√° normalizado)
    momento_ts: timestamp limite (ex: '2025-08-21 16:04:00'; ser√° normalizado)
    buckets: dict retornado por carregar_memorias_brutas()
    """
    tag = _normalize_tag(persona_tag)
    limite = _parse_ts(momento_ts)
    linhas = buckets.get(tag, [])

    # filtra at√© o limite e ordena por timestamp (strings j√° normalizadas)
    linhas_filtradas = [l for l in linhas if (l.get("timestamp") or "") <= limite]
    linhas_ord = sorted(linhas_filtradas, key=lambda x: x.get("timestamp", ""))

    return [l.get("conteudo", "") for l in linhas_ord if l.get("conteudo")]



# =========================
# EMBEDDINGS / SIMILARIDADE
# =========================

def gerar_embedding_openai(texto: str):
    if not OPENAI_OK:
        return None
    try:
        resp = OPENAI_CLIENT.embeddings.create(
            input=texto,
            model="text-embedding-3-small"
        )
        return np.array(resp.data[0].embedding, dtype=float)
    except Exception as e:
        st.error(f"Erro ao gerar embedding: {e}")
        return None

def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

def verificar_quebra_semantica_openai(texto1: str, texto2: str, limite=0.6) -> str:
    if not OPENAI_OK:
        return ""
    e1 = gerar_embedding_openai(texto1)
    e2 = gerar_embedding_openai(texto2)
    if e1 is None or e2 is None:
        return ""
    sim = cosine_similarity(e1, e2)
    if sim < limite:
        return f"‚ö†Ô∏è Baixa continuidade narrativa (similaridade: {sim:.2f})."
    return ""

# =========================
# MEM√ìRIA LONGA (Sheets + Embeddings/OpenAI opcional) ‚Äî TIME-AWARE
# =========================

def _sheet_ensure_memoria_longa():
    """Retorna a aba memoria_longa_jm se existir (n√£o cria automaticamente)."""
    return _ws(TAB_ML, create_if_missing=False)

def _serialize_vec(vec: np.ndarray) -> str:
    return json.dumps(vec.tolist(), separators=(",", ":"))

def _deserialize_vec(s: str) -> np.ndarray:
    try:
        return np.array(json.loads(s), dtype=float)
    except Exception:
        return np.zeros(1, dtype=float)

def memoria_longa_salvar(texto: str, tags: str = "") -> bool:
    """Salva uma mem√≥ria com embedding (se poss√≠vel) e score inicial. Invalida cache."""
    aba = _sheet_ensure_memoria_longa()
    if not aba:
        st.warning("Aba 'memoria_longa_jm' n√£o encontrada ‚Äî crie com cabe√ßalhos: texto | embedding | tags | timestamp | score")
        return False
    emb = gerar_embedding_openai(texto)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    linha = [texto.strip(), _serialize_vec(emb) if emb is not None else "", (tags or "").strip(), ts, 1.0]
    try:
        _retry_429(aba.append_row, linha, value_input_option="RAW")
        _invalidate_sheet_caches()
        return True
    except Exception as e:
        st.error(f"Erro ao salvar mem√≥ria longa: {e}")
        return False

def memoria_longa_listar_registros():
    """Retorna todos os registros da aba memoria_longa_jm (cache TTL)."""
    try:
        return _sheet_all_records_cached(TAB_ML)
    except Exception:
        return []

def _tokenize(s: str) -> set:
    return set(re.findall(r"[a-z√†-√∫0-9]+", (s or "").lower()))

def memoria_longa_buscar_topk(query_text: str, k: int = 3, limiar: float = 0.78, ate_ts=None):
    """
    Top-K mem√≥rias (time-aware). Usa embeddings se existir; sen√£o, Jaccard simples.
    Se 'ate_ts' for informado (formato 'YYYY-MM-DD HH:MM:SS'), ignora registros com
    timestamp > ate_ts (ou seja, mem√≥rias 'do futuro' em rela√ß√£o ao hist√≥rico atual).
    Retorna lista de tuplas (texto, score, sim, rr).
    """
    try:
        dados = _sheet_all_records_cached(TAB_ML)  # [{'texto','embedding','tags','timestamp','score'}, ...]
    except Exception as e:
        st.warning(f"Erro ao carregar memoria_longa_jm: {e}")
        return []

    q = gerar_embedding_openai(query_text) if OPENAI_OK else None
    candidatos = []

    for row in dados:
        texto = (row.get("texto") or "").strip()
        emb_s = (row.get("embedding") or "").strip()
        row_ts = (row.get("timestamp") or "").strip()
        try:
            score = float(row.get("score", 1.0) or 1.0)
        except Exception:
            score = 1.0
        if not texto:
            continue

        # --- Corte temporal: ignora mem√≥rias mais novas que o corte (se fornecido)
        if ate_ts and row_ts and row_ts > ate_ts:
            continue  # strings no formato YYYY-MM-DD HH:MM:SS s√£o compar√°veis lexicograficamente

        # Similaridade por embedding (quando dispon√≠vel), sen√£o fallback lexical
        if q is not None and emb_s:
            vec = _deserialize_vec(emb_s)
            if vec.ndim == 1 and vec.size >= 10 and np.linalg.norm(vec) > 0 and np.linalg.norm(q) > 0:
                sim = float(np.dot(q, vec) / (np.linalg.norm(q) * np.linalg.norm(vec)))
            else:
                sim = 0.0
        else:
            s1 = _tokenize(texto)
            s2 = _tokenize(query_text)
            sim = len(s1 & s2) / max(1, len(s1 | s2))

        if sim >= limiar:
            rr = 0.7 * sim + 0.3 * score
            candidatos.append((texto, score, sim, rr))

    candidatos.sort(key=lambda x: x[3], reverse=True)
    return candidatos[:k]

def memoria_longa_reforcar(textos_usados: list):
    """Aumenta o score das mem√≥rias usadas (pequeno refor√ßo) com backoff + corre√ß√£o de √≠ndices."""
    aba = _sheet_ensure_memoria_longa()
    if not aba or not textos_usados:
        return
    try:
        dados = _sheet_all_values_cached(TAB_ML)
        if not dados or len(dados) < 2:
            return
        headers = dados[0]  # cabe√ßalho √© a primeira linha
        idx_texto = headers.index("texto")
        idx_score = headers.index("score")
        for i, linha in enumerate(dados[1:], start=2):
            if len(linha) <= max(idx_texto, idx_score):
                continue
            t = (linha[idx_texto] or "").strip()
            if t in textos_usados:
                try:
                    sc = float(linha[idx_score] or 1.0)
                except Exception:
                    sc = 1.0
                sc = min(sc + 0.2, 2.0)
                _retry_429(aba.update_cell, i, idx_score + 1, sc)
        _invalidate_sheet_caches()
    except Exception:
        pass

# =========================
# ROMANCE (FASES) + MOMENTO
# =========================

FASES_ROMANCE: Dict[int, Dict[str, str]] = {
    0: {"nome": "Estranhos",
        "permitidos": "olhares; near-miss (mesmo caf√©/rua/√¥nibus); detalhe do ambiente",
        "proibidos": "troca de nomes; toques; conversa pessoal"},
    1: {"nome": "Percep√ß√£o",
        "permitidos": "cumprimento neutro; pergunta impessoal curta",
        "proibidos": "contato f√≠sico; confid√™ncias"},
    2: {"nome": "Conhecidos",
        "permitidos": "troca de nomes; pequena ajuda; 1 pergunta pessoal leve",
        "proibidos": "toque prolongado; encontro a s√≥s planejado"},
    3: {"nome": "Amizade",
        "permitidos": "conversa 10‚Äì20 min; caminhar juntos; troca de contatos; 1 gesto de afeto leve (com consentimento)",
        "proibidos": "beijos; car√≠cias intimistas"},
    4: {"nome": "Confian√ßa / Quase",
        "permitidos": "confid√™ncias; abra√ßo com consentimento expresso; marcar encontro futuro claro",
        "proibidos": "sexo; sexo oral/manual; pressa ou ‚Äúprovas de amor‚Äù f√≠sicas"},
    5: {"nome": "Compromisso / Encontro definitivo",
        "permitidos": "beijo prolongado; dormir juntos; consuma√ß√£o impl√≠cita (fade-to-black); manh√£ seguinte sugerida",
        "proibidos": ""},
}

FLAG_FASE_TXT_PREFIX = "FLAG: mj_fase="

def _fase_label(n: int) -> str:
    d = FASES_ROMANCE.get(int(n), FASES_ROMANCE[0])
    return f"{int(n)} ‚Äî {d['nome']}"

def mj_set_fase(n: int, persist: bool=False):
    n = max(0, min(max(FASES_ROMANCE.keys()), int(n)))
    st.session_state.mj_fase = n
    if persist:
        try:
            memoria_longa_salvar(f"{FLAG_FASE_TXT_PREFIX}{n}", tags="[flag]")
        except Exception:
            pass

def mj_carregar_fase_inicial() -> int:
    if "mj_fase" in st.session_state:
        return int(st.session_state.mj_fase)
    try:
        recs = memoria_longa_listar_registros()
        for r in reversed(recs):
            t = (r.get("texto") or "").strip()
            if t.startswith(FLAG_FASE_TXT_PREFIX):
                n = int(t.split("=")[1])
                st.session_state.mj_fase = n
                return n
    except Exception:
        pass
    st.session_state.mj_fase = 0
    return 0

# --------- Motor de Momento ----------
MOMENTOS = {
    0: {"nome": "Aproxima√ß√£o log√≠stica",
        "objetivo": "um acompanha o outro (ex.: at√© o p√≠er), clima cordial",
        "permitidos": "gentilezas; proximidade leve; di√°logo casual",
        "proibidos": "declara√ß√£o; revela√ß√µes √≠ntimas; toques prolongados",
        "gatilhos": [r"\b(p[i√≠]er|acompanhar|vamos embora|te levo)\b"],
        "proximo": 1},
    1: {"nome": "Declara√ß√£o",
        "objetivo": "um deles declara amor/ import√¢ncia",
        "permitidos": "confiss√£o afetiva; sil√™ncio tenso; abra√ßo curto",
        "proibidos": "negocia√ß√£o sexual; tirar roupas; explora√ß√£o do corpo",
        "gatilhos": [r"\b(amo voc[e√™]|te amo|n[a√£]o paro de pensar)\b"],
        "proximo": 2},
    2: {"nome": "Revela√ß√£o sens√≠vel",
        "objetivo": "Mary revela que √© virgem / vulnerabilidade equivalente",
        "permitidos": "dizer 'sou virgem'; estipular limites; conforto m√∫tuo",
        "proibidos": "car√≠cias √≠ntimas; tirar roupas",
        "gatilhos": [r"\b(sou virgem|nunca fiz|meu limite)\b"],
        "proximo": 3},
    3: {"nome": "Consentimento expl√≠cito",
        "objetivo": "alinhamento de limites e um 'sim' claro",
        "permitidos": "nomear fronteiras; pedir/receber consentimento; decidir 'agora sim'",
        "proibidos": "",
        "gatilhos": [r"\b(consento|quero|vamos juntos|tudo bem pra voc[e√™])\b", r"\b(at[e√©] onde)\b"],
        "proximo": 4},
    4: {"nome": "Intimidade (el√≠ptica)",
        "objetivo": "intimidade sugerida (fade-to-black) / p√≥s-ato impl√≠cito",
        "permitidos": "beijos longos; proximidade forte; fade-to-black; manh√£ seguinte impl√≠cita",
        "proibidos": "",
        "gatilhos": [r"\b(quarto|cama|luz baixa|porta fechada|manh[a√£] seguinte)\b"],
        "proximo": 4},
}

def _momento_label(n: int) -> str:
    m = MOMENTOS.get(int(n), MOMENTOS[0])
    return f"{int(n)} ‚Äî {m['nome']}"

def detectar_momento_sugerido(texto: str, fallback: int = 0) -> int:
    t = (texto or "").lower()
    for i in range(4, -1, -1):
        for gx in MOMENTOS[i]["gatilhos"]:
            if re.search(gx, t, flags=re.IGNORECASE):
                return i
    return fallback

def clamp_momento(atual: int, proposto: int, max_steps: int) -> int:
    if proposto > atual + max_steps:
        return atual + max_steps
    if proposto < atual:
        return max(proposto, atual - 1)
    return proposto

def momento_set(n: int, persist: bool = True):
    n = max(0, min(max(MOMENTOS.keys()), int(n)))
    st.session_state.momento = n
    if persist:
        try:
            memoria_longa_salvar(f"FLAG: mj_momento={n}", tags="[flag]")
        except Exception:
            pass

def momento_carregar() -> int:
    if "momento" in st.session_state:
        return int(st.session_state.momento)
    try:
        recs = memoria_longa_listar_registros()
        for r in reversed(recs):
            t = (r.get("texto") or "").strip()
            if t.startswith("FLAG: mj_momento="):
                n = int(t.split("=")[1])
                st.session_state.momento = n
                return n
    except Exception:
        pass
    st.session_state.momento = 0
    return 0

def viola_momento(texto: str, momento: int) -> str:
    # N√£o bloquear/censurar conte√∫do expl√≠cito por momento.
    return ""

# =========================
# PROVEDORES E MODELOS
# =========================

MODELOS_OPENROUTER = {
    "üí¨ DeepSeek V3 ‚òÖ‚òÖ‚òÖ‚òÖ ($)": "deepseek/deepseek-chat-v3-0324",
    "üß† DeepSeek R1 0528 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$)": "deepseek/deepseek-r1-0528",
    "üß† DeepSeek R1T2 Chimera ‚òÖ‚òÖ‚òÖ‚òÖ (free)": "tngtech/deepseek-r1t2-chimera:free",
    "üß† GPT-4.1 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (1M ctx)": "openai/gpt-4.1",
    "üëë WizardLM 8x22B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$$)": "microsoft/wizardlm-2-8x22b",
    "üëë Qwen 235B 2507 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (PAID)": "qwen/qwen3-235b-a22b-07-25",
    "üëë EVA Qwen2.5 72B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-qwen-2.5-72b",
    "üëë EVA Llama 3.33 70B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-llama-3.33-70b",
    "üé≠ Nous Hermes 2 Yi 34B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ": "nousresearch/nous-hermes-2-yi-34b",
    "üî• MythoMax 13B ‚òÖ‚òÖ‚òÖ‚òÜ ($)": "gryphe/mythomax-l2-13b",
    "üíã LLaMA3 Lumimaid 8B ‚òÖ‚òÖ‚òÜ ($)": "neversleep/llama-3-lumimaid-8b",
    "üåπ Midnight Rose 70B ‚òÖ‚òÖ‚òÖ‚òÜ": "sophosympatheia/midnight-rose-70b",
    "üå∂Ô∏è Noromaid 20B ‚òÖ‚òÖ‚òÜ": "neversleep/noromaid-20b",
    "üíÄ Mythalion 13B ‚òÖ‚òÖ‚òÜ": "pygmalionai/mythalion-13b",
    "üêâ Anubis 70B ‚òÖ‚òÖ‚òÜ": "thedrummer/anubis-70b-v1.1",
    "üßö Rocinante 12B ‚òÖ‚òÖ‚òÜ": "thedrummer/rocinante-12b",
    "üç∑ Magnum v2 72B ‚òÖ‚òÖ‚òÜ": "anthracite-org/magnum-v2-72b",
}

MODELOS_TOGETHER_UI = {
    "üß† Qwen3 Coder 480B (Together)": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "üëë Mixtral 8x7B v0.1 (Together)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "üëë Perplexity R1-1776 (Together)": "perplexity-ai/r1-1776",
}

def model_id_for_together(api_ui_model_id: str) -> str:
    key = (api_ui_model_id or "").strip()
    if "Qwen3-Coder-480B-A35B-Instruct-FP8" in key:
        return "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
    low = key.lower()
    if low.startswith("mistralai/mixtral-8x7b-instruct-v0.1"):
        return "mistralai/Mixtral-8x7B-Instruct-v0.1"
    return key or "mistralai/Mixtral-8x7B-Instruct-v0.1"

def api_config_for_provider(provider: str):
    if provider == "OpenRouter":
        return (
            "https://openrouter.ai/api/v1/chat/completions",
            st.secrets.get("OPENROUTER_API_KEY", ""),
            MODELOS_OPENROUTER,
        )
    else:
        return (
            "https://api.together.xyz/v1/chat/completions",
            st.secrets.get("TOGETHER_API_KEY", ""),
            MODELOS_TOGETHER_UI,
        )

# =========================
# PROMPT BUILDER
# =========================

def inserir_regras_mary_e_janio(prompt_base: str) -> str:
    # padr√£o elevado para 3 (expl√≠cito)
    calor = int(st.session_state.get("steam_level", st.session_state.get("nsfw_max_level", 3)))
    regras = f"""
‚öñÔ∏è Regras de coer√™ncia:
- Narre em terceira pessoa; n√£o se dirija ao leitor como "voc√™".
- Consentimento claro antes de qualquer gesto significativo.
- J√¢nio n√£o pressiona; respeita o ritmo de Mary.
- Linguagem sensual proporcional ao n√≠vel de calor ({calor}).
""".strip()
    fase = int(st.session_state.get("mj_fase", mj_carregar_fase_inicial()))
    if fase >= 5:
        regras += """
- Intimidade pode ser descrita com detalhes expl√≠citos (sem necessidade de corte)."""
    else:
        regras += """
- Sem consuma√ß√£o em cena; foque em progress√£o coerente."""
    return prompt_base + "\n" + regras


def gerar_mary_sensorial(level: int = 2, n: int = 2, hair_on: bool = True) -> str:
    """
    Gera 1‚Äì3 frases sensoriais sobre Mary.
      level: 0=off, 1=leve, 2=marcado, 3=ousado
      n: quantidade de frases
      hair_on: garante ao menos 1 frase sobre os cabelos (negros, volumosos, levemente ondulados)
    """
    if level <= 0 or n <= 0:
        return ""

    # Base
    base_leve = [
        "Mary caminha com ritmo seguro; h√° algo hipn√≥tico no balan√ßo dos quadris.",
        "O olhar de Mary prende f√°cil: direto, firme, cativante.",
        "O perfume de Mary chega antes dela, discreto e morno.",
        "O sorriso aparece quando quer; breve, afiado, certeiro.",
    ]
    base_marcado = [
        "Os quadris de Mary balan√ßam num compasso que chama aten√ß√£o sem pedir licen√ßa.",
        "Enquanto caminha, os seios balan√ßam de leve sob o tecido.",
        "O tecido ro√ßa nas pernas e denuncia o passo: firme, √≠ntimo, decidido.",
        "O olhar de Mary √© um convite silencioso ‚Äî confiante e dif√≠cil de sustentar por muito tempo.",
    ]
    base_ousado = [
        "O balan√ßo dos quadris de Mary √© quase cruel: entra na cabe√ßa e n√£o sai.",
        "Os seios acompanham a passada num movimento suave que acende o ambiente.",
        "O olhar de Mary encosta na pele de quem cruza com ela: quente, demorado, insinuante.",
        "O perfume fica na mem√≥ria como um toque atr√°s da nuca.",
    ]

    # Frases espec√≠ficas de cabelo (negros, volumosos, levemente ondulados)
    hair_leve = [
        "Os cabelos de Mary ‚Äî negros, volumosos, levemente ondulados ‚Äî descansam nos ombros e acompanham o passo.",
        "Os cabelos negros, volumosos e levemente ondulados moldam o rosto quando ela vira de leve.",
    ]
    hair_marcado = [
        "Cabelos negros, volumosos, levemente ondulados, fazem um arco quando ela vira o rosto, refor√ßando o balan√ßo do corpo.",
        "Os cabelos, negros e volumosos, ondulam de leve a cada passada e criam uma moldura hipn√≥tica.",
    ]
    hair_ousado = [
        "Os cabelos negros, volumosos e levemente ondulados deslizam pela clav√≠cula como um toque que fica.",
        "O balan√ßo dos cabelos negros ‚Äî volumosos, levemente ondulados ‚Äî marca o compasso do corpo de Mary.",
    ]

    pool = []
    hair_pool = []
    if level == 1:
        pool = base_leve
        hair_pool = hair_leve
    elif level == 2:
        pool = base_leve + base_marcado
        hair_pool = hair_leve + hair_marcado
    else:  # level >= 3
        pool = base_leve + base_marcado + base_ousado
        hair_pool = hair_leve + hair_marcado + hair_ousado

    n_eff = max(1, min(n, len(pool)))
    frases = random.sample(pool, k=n_eff)

    if hair_on and hair_pool:
        hair_line = random.choice(hair_pool)
        if hair_line not in frases:
            frases.insert(0, hair_line)
            if len(frases) > n_eff:
                frases = frases[:n_eff]

    return " ".join(frases)

def encontrar_memorias_relevantes(pergunta, buckets):
    """
    Busca mem√≥rias relevantes conforme palavra-chave na pergunta do usu√°rio.
    """
    # Palavras comuns que indicam pergunta factual
    keywords = [
        "nome", "integrante", "banda", "integrantes", "profiss√£o", "rotina", "cargo", "ocupa√ß√£o",
        "onde", "quem", "quando", "idade", "universidade", "curso", "hist√≥ria", "grupo"
    ]
    relevantes = []
    pergunta_lc = (pergunta or "").lower()

    # Busca por tags de persona (ex: janio, mary) e keywords
    for tag, items in buckets.items():
        tag_limp = tag.strip("[]")
        # Se o nome/tag aparece na pergunta ou h√° palavras-chave, considera relevante
        if any(k in pergunta_lc for k in keywords) or tag_limp in pergunta_lc:
            relevantes.extend(items)
    return relevantes


def construir_prompt_com_narrador() -> str:
    memos = carregar_memorias_brutas()
    perfil = carregar_resumo_salvo()
    fase = int(st.session_state.get("mj_fase", mj_carregar_fase_inicial()))
    fdata = FASES_ROMANCE.get(fase, FASES_ROMANCE[0])
    momento_atual = int(st.session_state.get("momento", momento_carregar()))
    mdata = MOMENTOS.get(momento_atual, MOMENTOS[0])
    proximo_nome = MOMENTOS.get(mdata.get("proximo", 0), MOMENTOS[0])["nome"]
    estilo = st.session_state.get("estilo_escrita", "A√á√ÉO")

    # Camada sensorial de Mary (para o 1¬∫ par√°grafo da cena)
    _sens_on = bool(st.session_state.get("mary_sensorial_on", True))
    _sens_level = int(st.session_state.get("mary_sensorial_level", 2))
    _sens_n = int(st.session_state.get("mary_sensorial_n", 2))
    mary_sens_txt = gerar_mary_sensorial(_sens_level, n=_sens_n) if _sens_on else ""

    # Hist√≥rico
    n_hist = int(st.session_state.get("n_sheet_prompt", 15))
    hist = carregar_interacoes(n=n_hist)
    hist_txt = "\n".join(f"{r['role']}: {r['content']}" for r in hist) if hist else "(sem hist√≥rico)"
    pergunta_user = hist[-1]["content"] if hist and hist[-1].get("role") == "user" else ""
    
    # Se quiser incluir bloco citacoes, precisa da fun√ß√£o encontrar_memorias_relevantes
    bloco_citacoes = ""
    # Se implementar a busca de mem√≥rias factuais, descomente essa parte:
    # memorias_fatuais = encontrar_memorias_relevantes(pergunta_user, memos)
    # if memorias_fatuais:
    #     bloco_citacoes = "\n".join([
    #         f"- {m.get('conteudo', '')} (mem√≥ria registrada em {m.get('timestamp','')})"
    #         for m in memorias_fatuais if m.get("conteudo")
    #     ])
    instrucoes_citacao = ""
    # if bloco_citacoes:
    #     instrucoes_citacao = (
    #         "\n### FATOS OBRIGAT√ìRIOS PARA RESPONDER A PERGUNTA DO USU√ÅRIO\n"
    #         "Responda de forma factual e cite explicitamente os dados abaixo na sua resposta. N√£o invente nem omita informa√ß√µes factuais relacionadas aos personagens da pergunta.\n"
    #         f"{bloco_citacoes}\n"
    #     )

    # CORTE TEMPORAL (at√© o timestamp da √∫ltima intera√ß√£o)
    if hist:
        ate_ts = _parse_ts(hist[-1].get("timestamp", "")) if hist else datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    else:
        ate_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Mem√≥ria longa Top-K
    ml_topk_txt = "(nenhuma)"
    st.session_state["_ml_topk_texts"] = []
    if st.session_state.get("use_memoria_longa", True) and hist:
        try:
            topk = memoria_longa_buscar_topk(
                query_text=hist[-1]["content"],
                k=int(st.session_state.get("k_memoria_longa", 3)),
                limiar=float(st.session_state.get("limiar_memoria_longa", 0.78)),
                ate_ts=ate_ts,
            )
            if topk:
                ml_topk_txt = "\n".join([f"- {t}" for (t, _sc, _sim, _rr) in topk])
                st.session_state["_ml_topk_texts"] = [t for (t, *_rest) in topk]
            else:
                st.session_state["_ml_topk_texts"] = []
        except Exception:
            st.session_state["_ml_topk_texts"] = []
    else:
        st.session_state["_ml_topk_texts"] = []

    # Diretrizes [all] respeitando o tempo (<= ate_ts)
    recorrentes = [
        (d.get("conteudo") or "").strip()
        for d in memos.get("[all]", [])
        if isinstance(d, dict) and d.get("conteudo") and (not d.get("timestamp") or d.get("timestamp") <= ate_ts)
    ]
    st.session_state["_ml_recorrentes"] = recorrentes

    # Dossi√™ temporal
    dossie = []
    mary = persona_block_temporal("mary", memos, ate_ts, 8)
    janio = persona_block_temporal("janio", memos, ate_ts, 8)
    if mary:
        dossie.append(mary)
    if janio:
        dossie.append(janio)
    dossie_txt = "\n\n".join(dossie) if dossie else "(sem personas definidas)"

    flag_parallel = bool(st.session_state.get("no_coincidencias", True))

    # MONTAGEM DO PROMPT ‚Äî pe√ßa que as aspas triplas fechem antes do .strip()
    prompt = f"""{instrucoes_citacao}
Voc√™ √© o Narrador de um roleplay dram√°tico brasileiro, foque em Mary e J√¢nio. N√£o repita instru√ß√µes nem t√≠tulos.

### Dossi√™ (personas)
{dossie_txt}
### Diretrizes gerais (ALL)
{chr(10).join(f"- {c}" for c in recorrentes) if recorrentes else "(vazio)"}
### Perfil (resumo mais recente)
{perfil or "(vazio)"}
### Hist√≥rico recente (planilha)
{hist_txt}
### Estilo
- Use o estilo **{estilo}**:
{("- Frases curtas, cortes r√°pidos, foco em gesto/ritmo.") if estilo=="A√á√ÉO" else
("- Atmosfera sombria, subtexto, sil√™ncio que pesa.") if estilo=="NOIR" else
("- Ritmo lento, tens√£o emocional, detalhes sensoriais (sem grafismo).")}
- Todas as cenas devem ser altamente sensoriais e f√≠sicas. Detalhe toques, apertos, tremores, suor, respira√ß√£o, grada√ß√£o do prazer at√© o limite do orgasmo e depois.
- As falas de Mary devem ser sensuais, realistas e expl√≠citas, sem linguagem po√©tica ou frases floreadas.
- Mary verbaliza desejo sexual, pede e orienta sexo e prazer sem pudor: frases diretas e sensoriais como "Me fode agora!", "Quero gozar!", "Tira minha roupa!", "Mete fundo!", "Estou molhada!", "Faz eu gritar de prazer!", "Estou gozando!", "Que del√≠cia!", "Vai, n√£o para!", "Me pega desse jeito!", "P√µe a m√£o aqui!", "Goza dentro!", "Porra, t√¥ gozando!", "Mais forte!" etc.
- N√£o use met√°foras, floreios l√≠ricos ou reflex√µes profundas; priorize comandos, rea√ß√µes corporais e escancaradas do tes√£o e p√≥s-tes√£o.
- No cl√≠max, **obrigatoriamente inclua frases explosivas e altas de orgasmo**: Mary e J√¢nio devem gemer, gritar ou falar explicitamente seu prazer ("Estou gozando!", "Ahhh, que del√≠cia!", "Goza dentro de mim!", "Mais forte!", etc.) antes de qualquer relaxamento ou carinho.
- O desfecho da cena sexual sempre inclui rea√ß√£o explosiva: gemidos altos, frases de gozo ditas em voz alta, corpo pulsando, tremores intensos; s√≥ depois, podem relaxar exaustos, suados, abra√ßados, comentando o prazer ("Nunca gozei desse jeito!", "Estou toda mole", "N√£o sinto as pernas", etc).
- Jamais encerre sem que pelo menos um dos personagens verbalize de forma expl√≠cita o orgasmo e a intensidade do prazer.
- Mary reage fisicamente com agarr√µes, mordidas, pedidos ousados e sinceros, sem vergonha. Nunca escreva desfecho seco ou c√¥mico; sempre finalize mostrando o impacto corporal real, prazeroso e o orgasmo verbalizado.
### Camada sensorial ‚Äî Mary (OBRIGAT√ìRIA no 1¬∫ par√°grafo)
{mary_sens_txt or "- Comece com 1‚Äì2 frases curtas sobre o caminhar, olhar, perfume e cabelos (negros, volumosos, levemente ondulados) de Mary; pode mencionar o balan√ßo suave dos seios sob o tecido (sem vulgaridade)."}
- Aplique essa camada ANTES do primeiro di√°logo.
- Frases curtas, diretas, f√≠sicas; evite met√°foras rebuscadas.
### Mem√≥ria longa ‚Äî Top-K relevantes
{ml_topk_txt}
### ‚è±Ô∏è Estado do romance (manual)
- Fase atual: {_fase_label(fase)}
- Permitidos: {fdata['permitidos']}
- Proibidos: {fdata['proibidos']}
### üéØ Momento dram√°tico (agora)
- Momento: {_momento_label(momento_atual)}
- Objetivo da cena: {mdata['objetivo']}
- Nesta cena, **permita**: {mdata['permitidos']}
- Evite/adiar: {mdata['proibidos']}
- **Micropassos:** avance no m√°ximo **{int(st.session_state.get("max_avancos_por_cena",1))}** subpasso(s) rumo a: {proximo_nome}.
- Se o roteirista pedir salto maior, **negocie**: nomeie limites, pe√ßa consentimento, e **prepare** a transi√ß√£o (n√£o pule etapas).
### Geografia & Montagem
- **N√£o force coincid√™ncias**: se n√£o houver ponte clara (mensagem, convite, "ensaio 18h...", pedido do usu√°rio), mantenha **Mary e J√¢nio em locais distintos** e utilize **montagem paralela** (A/B).
- **Comece cada bloco** com uma frase que **ancore lugar e hora** (exemplo: "UFES - corredor de Pedagogia, 9h15 - ..." ou "Terminal Laranjeiras, 9h18 - ..."). N√£o use t√≠tulos; escreva essa informa√ß√£o na **primeira frase** do par√°grafo.
- **Se montagem paralela** (valor sugerido: {flag_parallel}):
  - Estruture em **2 blocos alternados**: primeiro Mary, depois J√¢nio (ou vice-versa), cada um em **seu lugar**.
  - Os blocos podem se "responder" por subtexto (mensagens, lembran√ßas, sons √† dist√¢ncia), mas **sem co-presen√ßa f√≠sica**.
- **Se houver ponte plaus√≠vel expl√≠cita**, pode convergir para co-presen√ßa ao final da cena (de forma plaus√≠vel), **sem teletransporte**.
- **Sem ponte dieg√©tica expl√≠cita, um personagem n√£o pode saber, afirmar ou reagir a fatos que s√≥ ocorreram no bloco do outro; se houver pressentimento ou ci√∫me, redija sem afirmar o fato. Exemplos de ponte: mensagem, foto/story, liga√ß√£o, testemunha, encontro marcado - se existir, mostre isso na cena (exemplo: celular vibra e mostra um story)**.
- **Objetos dieg√©ticos: caso a c√¢mera n√£o se encaixe na situa√ß√£o (encontro, banho, mar, revista), mostre a a√ß√£o de guardar antes e ignore o objeto at√© a retomada; n√£o descreva intera√ß√£o f√≠sica com a c√¢mera nesses contextos**.
### Formato OBRIGAT√ìRIO da cena
- **Inclua DI√ÅLOGOS diretos** com travess√£o (-), intercalados com a√ß√£o e rea√ß√£o f√≠sica/visual. Exemplo de travess√£o: - Ele disse ...
- Garanta **pelo menos 2 falas de Mary e 2 de J√¢nio** (quando ambos estiverem na cena).
- **N√£o inclua pensamentos internos em it√°lico, reflex√µes internas ou mon√≥logos subjetivos dos personagens.**
- N√£o escreva blocos finais de cr√©ditos, microconquistas, resumos ou ganchos. Apenas narra√ß√£o e intera√ß√£o direta.
- Mostre somente a√ß√µes, gestos, express√µes do ambiente, clima corporal e di√°logos.
- Sem t√≠tulos de se√ß√£o, microconquista ou gancho, nem qualquer nota meta ao final.
### Regra de sa√≠da
- Narre em **terceira pessoa**; nunca fale com "voc√™".
- Produza uma cena fechada e natural, sem inserir coment√°rios externos ou instru√ß√µes.
""".strip()
    prompt = inserir_regras_mary_e_janio(prompt)
    return prompt

# =========================
# FILTROS DE SA√çDA
# =========================

def render_tail(t: str) -> str:
    if not t:
        return ""
    # remove r√≥tulos meta e blocos <think>
    t = re.sub(r'^\s*\**\s*(microconquista|gancho)\s*:\s*.*$', '', t, flags=re.IGNORECASE | re.MULTILINE)
    t = re.sub(r'&lt;\s*think\s*&gt;.*?&lt;\s*/\s*think\s*&gt;', '', t, flags=re.IGNORECASE | re.DOTALL)
    t = re.sub(r'\n{3,}', '\n\n', t).strip()
    return t

EXPL_PAT = re.compile(
    r"\b(seio[s]?|mamilos?|bunda|fio[- ]?dental|genit[a√°]lia|ere[c√ß][a√£]o|penetra[c√ß][a√£]o|"
    r"boquete|gozada|gozo|sexo oral|chupar|enfiar)\b",
    flags=re.IGNORECASE
)

def classify_nsfw_level(t: str) -> int:
    if EXPL_PAT.search(t or ""):
        return 3  # expl√≠cito
    if re.search(r"\b(cintura|pesco[c√ß]o|costas|beijo prolongado|respira[c√ß][a√£]o curta)\b", (t or ""), re.IGNORECASE):
        return 2
    if re.search(r"\b(olhar|aproximar|toque|m[a√£]os dadas|beijo)\b", (t or ""), re.IGNORECASE):
        return 1
    return 0

def sanitize_explicit(t: str, max_level: int, action: str) -> str:
    # Libera√ß√£o: se o conte√∫do for de n√≠vel <= max_level, retorna tal como est√°.
    lvl = classify_nsfw_level(t)
    if lvl <= max_level:
        return t
    # Se extrapolar o m√°ximo definido, n√£o cortar por padr√£o (liberar NSFW).
    return t

def redact_for_logs(t: str) -> str:
    if not t:
        return ""
    t = re.sub(EXPL_PAT, "[‚Ä¶]", t, flags=re.IGNORECASE)
    return re.sub(r'\n{3,}', '\n\n', t).strip()

def resposta_valida(t: str) -> bool:
    if not t or t.strip() == "[Sem conte√∫do]":
        return False
    if len(t.strip()) < 5:
        return False
    return True

def precisa_reforcar_dialogo(texto: str) -> bool:
    if not texto:
        return True
    n_dialog = len(re.findall(r'(^|\n)\s*(‚Äî|")', texto))
    n_thoughts = len(re.findall(r'\*[^*\n]{4,}\*', texto))
    return (n_dialog < 4) or (n_thoughts < 2)

# =========================
# UI ‚Äî CABE√áALHO E CONTROLES
# =========================

st.title("üé¨ Narrador JM")
st.subheader("Voc√™ √© o roteirista. Digite uma dire√ß√£o de cena. A IA narrar√° Mary e J√¢nio.")
st.markdown("---")

# Inicializa√ß√£o dos estados de sess√£o (inclusive dos templates)
if "resumo_capitulo" not in st.session_state:
    st.session_state.resumo_capitulo = carregar_resumo_salvo()
if "session_msgs" not in st.session_state:
    st.session_state.session_msgs = []
if "use_memoria_longa" not in st.session_state:
    st.session_state.use_memoria_longa = True
if "k_memoria_longa" not in st.session_state:
    st.session_state.k_memoria_longa = 3
if "limiar_memoria_longa" not in st.session_state:
    st.session_state.limiar_memoria_longa = 0.78
if "app_bloqueio_intimo" not in st.session_state:
    st.session_state.app_bloqueio_intimo = False
if "app_emocao_oculta" not in st.session_state:
    st.session_state.app_emocao_oculta = "nenhuma"
if "mj_fase" not in st.session_state:
    st.session_state.mj_fase = mj_carregar_fase_inicial()
if "momento" not in st.session_state:
    st.session_state.momento = momento_carregar()
if "max_avancos_por_cena" not in st.session_state:
    st.session_state.max_avancos_por_cena = 1
#if "nsfw_max_level" not in st.session_state:
 #   st.session_state.nsfw_max_level = 3
if "estilo_escrita" not in st.session_state:
    st.session_state.estilo_escrita = "A√á√ÉO"
if "templates_jm" not in st.session_state:
    st.session_state.templates_jm = carregar_templates_planilha()
if "template_ativo" not in st.session_state:
    st.session_state.template_ativo = None
if "etapa_template" not in st.session_state:
    st.session_state.etapa_template = 0

col1, col2 = st.columns([3, 2])
with col1:
    st.markdown("#### üìñ √öltimo resumo salvo:")
    st.info(st.session_state.resumo_capitulo or "Nenhum resumo dispon√≠vel.")
with col2:
    st.markdown("#### ‚öôÔ∏è Op√ß√µes")
    st.write(
        f'- Bloqueio √≠ntimo: {"Sim" if st.session_state.get("app_bloqueio_intimo", False) else "N√£o"}\n'
        f'- Emo√ß√£o oculta: {st.session_state.get("app_emocao_oculta", "").capitalize()}'
    )


# =========================
# SIDEBAR ‚Äî Reorganizado
# =========================

with st.sidebar:
    st.title("üß≠ Painel do Roteirista")
    # Provedor/modelos
    provedor = st.radio("üåê Provedor", ["OpenRouter", "Together"], index=0, key="provedor_ia")
    api_url, api_key, modelos_map = api_config_for_provider(provedor)
    if not api_key:
        st.warning("‚ö†Ô∏è API key ausente para o provedor selecionado. Defina em st.secrets.")
    modelo_nome = st.selectbox("ü§ñ Modelo de IA", list(modelos_map.keys()), index=0, key="modelo_nome_ui")
    modelo_escolhido_id_ui = modelos_map[modelo_nome]
    st.session_state.modelo_escolhido_id = modelo_escolhido_id_ui

    st.markdown("---")
    st.markdown("### ‚úçÔ∏è Estilo & Progresso Dram√°tico")
    st.selectbox(
        "Estilo de escrita",
        ["A√á√ÉO", "ROMANCE LENTO", "NOIR"],
        index=["A√á√ÉO", "ROMANCE LENTO", "NOIR"].index(st.session_state.get("estilo_escrita", "A√á√ÉO")),
        key="estilo_escrita",
    )
    st.slider("N√≠vel de calor (0=leve, 3=expl√≠cito)", 0, 3, value=3, key="nsfw_max_level")

    st.markdown("---")
    st.markdown("### üíû Romance Mary & J√¢nio")
    fase_default = mj_carregar_fase_inicial()
    options_fase = sorted(FASES_ROMANCE.keys())
    fase_ui_val = int(st.session_state.get("mj_fase", fase_default))
    fase_ui_val = max(min(fase_ui_val, max(options_fase)), min(options_fase))
    fase_escolhida = st.select_slider("Fase do romance", options=options_fase, value=fase_ui_val, format_func=_fase_label, key="ui_mj_fase")
    if fase_escolhida != st.session_state.get("mj_fase", fase_default):
        mj_set_fase(fase_escolhida, persist=True)
    options_momento = sorted(MOMENTOS.keys())
    mom_default = momento_carregar()
    mom_ui_val = int(st.session_state.get("momento", mom_default))
    mom_ui_val = max(min(mom_ui_val, max(options_momento)), min(options_momento))
    mom_ui = st.select_slider("Momento atual", options=options_momento, value=mom_ui_val, format_func=_momento_label, key="ui_momento")
    if mom_ui != st.session_state.get("momento", mom_default):
        momento_set(mom_ui, persist=False)
    st.slider("Micropassos por cena", 1, 3, value=int(st.session_state.get("max_avancos_por_cena", 1)), key="max_avancos_por_cena")
    col_a, col_b = st.columns(2)
    with col_a:
        if st.button("‚ûï Avan√ßar 1 passo"):
            mj_set_fase(min(st.session_state.get("mj_fase", 0) + 1, max(options_fase)), persist=True)
    with col_b:
        if st.button("‚Ü∫ Reiniciar (0)"):
            mj_set_fase(0, persist=True)

        st.markdown("---")
    st.markdown("### üé¨ Roteiros Sequenciais (Templates)")
    nomes_templates = list(st.session_state.templates_jm.keys())
    
    if st.button("üîÑ Recarregar templates"):
        st.session_state.templates_jm = carregar_templates_planilha()
        st.success("Templates atualizados da planilha!")
    
    if nomes_templates:
        roteiro_escolhido = st.selectbox("Escolha o roteiro:", nomes_templates, key="sb_rota_sel")
        etapas = st.session_state.templates_jm.get(roteiro_escolhido, [])
    
        # Inicia o roteiro e j√° dispara a 1¬™ etapa (se existir)
        if st.button("Iniciar roteiro", key="btn_iniciar_roteiro"):
            st.session_state.template_ativo = roteiro_escolhido
            st.session_state.etapa_template = 0
            if etapas:
                comando = etapas[0]
                salvar_interacao("user", comando)
                st.session_state.session_msgs.append({"role": "user", "content": comando})
                st.session_state["_trigger_input"] = comando  # dispara gera√ß√£o
    
        # Progresso / pr√≥xima etapa
        if st.session_state.get("template_ativo"):
            etapas_ativas = st.session_state.templates_jm.get(st.session_state.template_ativo, [])
            etap = int(st.session_state.get("etapa_template", 0))
            if etap < len(etapas_ativas):
                st.markdown(f"Etapa atual: {etap + 1} de {len(etapas_ativas)}")
                if st.button("Pr√≥xima etapa (*)", key="btn_proxima_etapa"):
                    comando = etapas_ativas[etap]
                    salvar_interacao("user", comando)
                    st.session_state.session_msgs.append({"role": "user", "content": comando})
                    st.session_state.etapa_template = etap + 1
                    st.session_state["_trigger_input"] = comando  # dispara gera√ß√£o
            else:
                st.success("Roteiro conclu√≠do!")
                st.session_state.template_ativo = None
                st.session_state.etapa_template = 0
    else:
        st.info("Nenhum template encontrado na aba templates_jm.")

    st.markdown("---")
    st.checkbox(
        "Evitar coincid√™ncias for√ßadas (montagem paralela A/B)",
        value=st.session_state.get("no_coincidencias", True),
        key="no_coincidencias",
    )
    st.checkbox(
        "Bloquear avan√ßos √≠ntimos sem ordem",
        value=st.session_state.app_bloqueio_intimo,
        key="ui_bloqueio_intimo",
    )
    st.selectbox(
        "üé≠ Emo√ß√£o oculta",
        ["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"],
        index=["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"].index(st.session_state.app_emocao_oculta),
        key="ui_app_emocao_oculta",
    )
    st.session_state.app_bloqueio_intimo = st.session_state.get("ui_bloqueio_intimo", False)
    st.session_state.app_emocao_oculta = st.session_state.get("ui_app_emocao_oculta", "nenhuma")

    st.markdown("---")
    st.markdown("### ‚è±Ô∏è Comprimento/timeout")
    st.slider("Max tokens da resposta", 256, 2500, value=int(st.session_state.get("max_tokens_rsp", 1200)), step=32, key="max_tokens_rsp")
    st.slider("Timeout (segundos)", 60, 600, value=int(st.session_state.get("timeout_s", 300)), step=10, key="timeout_s")

    st.markdown("---")
    st.markdown("### üóÉÔ∏è Mem√≥ria Longa")
    st.checkbox("Usar mem√≥ria longa no prompt", value=st.session_state.get("use_memoria_longa", True), key="use_memoria_longa")
    st.slider("Top-K mem√≥rias", 1, 5, int(st.session_state.get("k_memoria_longa", 3)), 1, key="k_memoria_longa")
    st.slider("Limiar de similaridade", 0.50, 0.95, float(st.session_state.get("limiar_memoria_longa", 0.78)), 0.01, key="limiar_memoria_longa")

    st.markdown("---")
    if st.button("üìù Gerar resumo do cap√≠tulo"):
        try:
            inter = carregar_interacoes(n=6)
            texto = "\n".join(f"{r['role']}: {r['content']}" for r in inter) if inter else ""
            prompt_resumo = (
                "Resuma o seguinte trecho como um cap√≠tulo de novela brasileiro, mantendo tom e emo√ß√µes.\n\n"
                + texto + "\n\nResumo:"
            )
            model_id_call = model_id_for_together(modelo_escolhido_id_ui) if provedor == "Together" else modelo_escolhido_id_ui
            r = requests.post(
                api_url,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json={"model": model_id_call, "messages": [{"role": "user", "content": prompt_resumo}], "max_tokens": 800, "temperature": 0.85},
                timeout=int(st.session_state.get("timeout_s", 300)),
            )
            if r.status_code == 200:
                resumo = r.json()["choices"][0]["message"]["content"].strip()
                st.session_state.resumo_capitulo = resumo
                salvar_resumo(resumo)
                st.success("Resumo gerado e salvo com sucesso!")
            else:
                st.error(f"Erro ao resumir: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro ao gerar resumo: {e}")

    if st.button("üíæ Salvar √∫ltima resposta como mem√≥ria"):
        ultimo_assist = ""
        for m in reversed(st.session_state.get("session_msgs", [])):
            if m.get("role") == "assistant":
                ultimo_assist = m.get("content", "").strip()
                break
        if ultimo_assist:
            ok = memoria_longa_salvar(ultimo_assist, tags="auto")
            st.success("Mem√≥ria de longo prazo salva!" if ok else "Falha ao salvar mem√≥ria.")
        else:
            st.info("Ainda n√£o h√° resposta do assistente nesta sess√£o.")

    if st.button("üîÅ Refor√ßar mem√≥rias biogr√°ficas"):
        memos = carregar_memorias_brutas()
        count = 0
        for k in ["[mary]", "[janio]", "[all]"]:
            for entrada in memos.get(k, []):
                texto = entrada.get("conteudo", "").strip()
                if texto:
                    ok = memoria_longa_salvar(texto, tags=k)
                    if ok:
                        count += 1
        st.success(f"{count} mem√≥rias biogr√°ficas refor√ßadas na mem√≥ria longa!")

    st.markdown("### üß© Hist√≥rico no prompt")
    st.slider("Intera√ß√µes do Sheets (N)", 10, 30, value=int(st.session_state.get("n_sheet_prompt", 15)), step=1, key="n_sheet_prompt")



   
# =========================
# EXIBIR HIST√ìRICO (depois resumo)
# =========================

with st.container():
    interacoes = carregar_interacoes(n=20)
    for r in interacoes:
        role = r.get("role", "user")
        content = r.get("content", "")
        if role == "user":
            with st.chat_message("user"):
                st.markdown(content)
        else:
            with st.chat_message("assistant"):
                st.markdown(content)
    if st.session_state.get("resumo_capitulo"):
        with st.expander("üß† Resumo do cap√≠tulo (mais recente)"):
            st.markdown(st.session_state.resumo_capitulo)

# =========================
# ENVIO DO USU√ÅRIO + STREAMING (OpenRouter/Together) + FALLBACKS
# =========================

entrada = st.chat_input("Digite sua dire√ß√£o de cena...")
# Permite que bot√µes do roteiro disparem a gera√ß√£o
if not entrada:
    entrada = st.session_state.pop("_trigger_input", None)


if entrada:
    # 0) Atualiza Momento sugerido (opcional e seguro)
    try:
        mom_atual = int(st.session_state.get("momento", momento_carregar()))
        mom_sug = detectar_momento_sugerido(entrada, fallback=mom_atual)
        mom_novo = clamp_momento(mom_atual, mom_sug, int(st.session_state.get("max_avancos_por_cena", 1)))
        if st.session_state.get("app_bloqueio_intimo", False):
            mom_novo = clamp_momento(mom_atual, mom_sug, 1)
        momento_set(mom_novo, persist=False)
    except Exception:
        pass

    # 1) Salva a entrada e mant√©m hist√≥rico de sess√£o
    salvar_interacao("user", str(entrada))
    st.session_state.session_msgs.append({"role": "user", "content": str(entrada)})

    # 2) Constr√≥i prompt principal
    prompt = construir_prompt_com_narrador()

    # 3) Hist√≥rico curto (somente sess√£o atual; o prompt j√° inclui √∫ltimas do sheet)
    historico = [{"role": m.get("role", "user"), "content": m.get("content", "")}
                 for m in st.session_state.session_msgs]

    # 4) Provedor + modelo
    prov = st.session_state.get("provedor_ia", "OpenRouter")
    if prov == "Together":
        endpoint = "https://api.together.xyz/v1/chat/completions"
        auth = st.secrets.get("TOGETHER_API_KEY", "")
        # Garanta que o ID tenha o A35B:
        # ex.: "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
        model_to_call = model_id_for_together(st.session_state.modelo_escolhido_id)
    else:
        endpoint = "https://openrouter.ai/api/v1/chat/completions"
        auth = st.secrets.get("OPENROUTER_API_KEY", "")
        model_to_call = st.session_state.modelo_escolhido_id

    if not auth:
        st.error("A chave de API do provedor selecionado n√£o foi definida em st.secrets.")
        st.stop()

    # 5) Mensagens
    system_pt = {
        "role": "system",
        "content": (
            "Responda em portugu√™s do Brasil. Evite conte√∫do meta. "
            "Mostre apenas a narrativa final ao leitor."
        ),
    }
    messages = [system_pt, {"role": "system", "content": prompt}] + historico

    payload = {
        "model": model_to_call,
        "messages": messages,
        "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
        "temperature": 0.9,
        "stream": True,
    }
    headers = {"Authorization": f"Bearer {auth}", "Content-Type": "application/json"}

    # 6) Render / Filtro de sa√≠da
    def _render_visible(t: str) -> str:
        out = render_tail(t)
        out = sanitize_explicit(
            out,
            max_level=int(st.session_state.get("nsfw_max_level", 3)),
            action="livre"
        )
        return out

    with st.chat_message("assistant"):
        placeholder = st.empty()
        resposta_txt = ""   # texto bruto vindo do stream
        last_update = time.time()

        # 7) Refor√ßo antecipado: mem√≥rias que ENTRARAM no prompt (topk + recorrentes)
        try:
            usados_prompt = []
            usados_prompt.extend(st.session_state.get("_ml_topk_texts", []))
            usados_prompt.extend(st.session_state.get("_ml_recorrentes", []))
            usados_prompt = [t for t in usados_prompt if t]
            if usados_prompt:
                memoria_longa_reforcar(usados_prompt)
        except Exception:
            pass

        # 8) STREAM
        try:
            with requests.post(
                endpoint, headers=headers, json=payload, stream=True,
                timeout=int(st.session_state.get("timeout_s", 300))
            ) as r:
                if r.status_code == 200:
                    for raw in r.iter_lines(decode_unicode=False):
                        if not raw:
                            continue
                        line = raw.decode("utf-8", errors="ignore").strip()
                        if not line.startswith("data:"):
                            continue
                        data = line[5:].strip()
                        if data == "[DONE]":
                            break
                        try:
                            j = json.loads(data)
                            # CORRETO: choices[0]["delta"]["content"]
                            delta = j["choices"][0]["delta"].get("content", "")
                            if not delta:
                                continue
                            resposta_txt += delta
                            if time.time() - last_update > 0.10:
                                placeholder.markdown(_render_visible(resposta_txt) + "‚ñå")
                                last_update = time.time()
                        except Exception:
                            continue
                else:
                    st.error(f"Erro {('Together' if prov=='Together' else 'OpenRouter')}: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro no streaming: {e}")

        # 9) FALLBACKS se veio vazio
        visible_txt = _render_visible(resposta_txt).strip()

        if not visible_txt:
            # 9a) retry sem stream
            try:
                r2 = requests.post(
                    endpoint, headers=headers,
                    json={**payload, "stream": False},
                    timeout=int(st.session_state.get("timeout_s", 300))
                )
                if r2.status_code == 200:
                    try:
                        # CORRETO: choices[0]["message"]["content"]
                        resposta_txt = r2.json()["choices"][0]["message"]["content"].strip()
                    except Exception:
                        resposta_txt = ""
                    visible_txt = _render_visible(resposta_txt).strip()
                else:
                    st.error(f"Fallback (sem stream) falhou: {r2.status_code} - {r2.text}")
            except Exception as e:
                st.error(f"Fallback (sem stream) erro: {e}")

        if not visible_txt:
            # 9b) retry sem o system extra (alguns modelos travam com system duplo)
            try:
                r3 = requests.post(
                    endpoint, headers=headers,
                    json={
                        "model": model_to_call,
                        "messages": [{"role": "system", "content": prompt}] + historico,
                        "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
                        "temperature": 0.9,
                        "stream": False,
                    },
                    timeout=int(st.session_state.get("timeout_s", 300))
                )
                if r3.status_code == 200:
                    try:
                        resposta_txt = r3.json()["choices"][0]["message"]["content"].strip()
                    except Exception:
                        resposta_txt = ""
                    visible_txt = _render_visible(resposta_txt).strip()
                else:
                    st.error(f"Fallback (prompts limpos) falhou: {r3.status_code} - {r3.text}")
            except Exception as e:
                st.error(f"Fallback (prompts limpos) erro: {e}")

        # 10) Exibi√ß√£o final
        placeholder.markdown(visible_txt if visible_txt else "[Sem conte√∫do]")

        # 11) Aviso de momento (n√£o bloqueia)
        try:
            viol = viola_momento(visible_txt, int(st.session_state.get("momento", 0)))
            if viol and st.session_state.get("app_bloqueio_intimo", False):
                st.info(f"‚ö†Ô∏è {viol}")
        except Exception:
            pass

        # 12) Valida√ß√£o sem√¢ntica (entrada do user vs resposta) usando texto vis√≠vel
        if len(st.session_state.session_msgs) >= 1 and visible_txt and visible_txt != "[Sem conte√∫do]":
            texto_anterior = st.session_state.session_msgs[-1]["content"]
            alerta = verificar_quebra_semantica_openai(texto_anterior, visible_txt)
            if alerta:
                st.info(alerta)

        # 13) Salvar resposta SEMPRE (usa o texto vis√≠vel)
        salvar_interacao("assistant", visible_txt if visible_txt else "[Sem conte√∫do]")
        st.session_state.session_msgs.append({"role": "assistant", "content": visible_txt if visible_txt else "[Sem conte√∫do]"})

        # 14) Refor√ßo de mem√≥rias usadas (p√≥s-resposta)
        try:
            usados = []
            topk_usadas = memoria_longa_buscar_topk(
                query_text=visible_txt,
                k=int(st.session_state.k_memoria_longa),
                limiar=float(st.session_state.limiar_memoria_longa),
            )
            for t, _sc, _sim, _rr in topk_usadas:
                usados.append(t)
            memoria_longa_reforcar(usados)
        except Exception:
            pass

#





































