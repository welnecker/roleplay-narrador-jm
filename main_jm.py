# -*- coding: utf-8 -*-
# main_jm.py â€” build estÃ¡vel
# Requisitos: streamlit, requests, gspread, oauth2client, numpy, openai
# Segredos esperados em st.secrets:
# - OPENAI_API_KEY (para embeddings)
# - OPENROUTER_API_KEY (se usar OpenRouter)
# - TOGETHER_API_KEY (se usar Together)
# - GOOGLE_CREDS_JSON (service account JSON do Google)
# Google Sheet esperado (por key) em SHEET_KEY (ou substitua abaixo):
#   abas: interacoes_jm, memoria_longa_jm, perfil_jm (p/ resumos)

import streamlit as st
import requests
import gspread
import json
import time
import re
import numpy as np
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

# --- CorreÃ§Ã£o de texto corrompido (mojibake) ---
# Se aparecerem sequÃªncias como "Ãƒ", "Ã¢Â€Â”", etc., tentamos reparar
# strings que foram renderizadas com encoding errado.
def fix_mojibake(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s
    # HeurÃ­stica simples: se hÃ¡ padrÃµes tÃ­picos de UTF-8 mal decodificado,
    # tentamos re-interpretar como latin-1 -> utf-8.
    if ("Ãƒ" in s) or ("Ã¢Â€" in s) or ("Ã‚" in s):
        try:
            return s.encode("latin-1", errors="ignore").decode("utf-8", errors="ignore")
        except Exception:
            return s
    return s

# ===============
# CONFIG BÃSICA
# ===============
st.set_page_config(page_title="Mary / JÃ¢nio â€” Novela Interativa", page_icon="ğŸŒ¹", layout="wide")
SHEET_KEY = st.secrets.get("SHEET_KEY", "1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM")
OPENROUTER_API_KEY = st.secrets.get("OPENROUTER_API_KEY", "")
TOGETHER_API_KEY = st.secrets.get("TOGETHER_API_KEY", "")
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")

# ===============
# UTIL: PLANILHA
# ===============
@st.cache_resource(show_spinner=False)
def conectar_planilha():
    try:
        creds_dict = json.loads(st.secrets["GOOGLE_CREDS_JSON"]) if "GOOGLE_CREDS_JSON" in st.secrets else None
        if not creds_dict:
            st.stop()
        creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        sh = client.open_by_key(SHEET_KEY)
        return sh
    except Exception as e:
        st.error(f"Erro ao conectar Ã  planilha: {e}")
        return None

sh = conectar_planilha()

# ---------------
# Abas helpers
# ---------------
@st.cache_data(ttl=15, show_spinner=False)
def get_ws(nome):
    if not sh:
        return None
    try:
        return sh.worksheet(nome)
    except Exception:
        return None

# ===============
# MEMÃ“RIA LONGA (embeddings)
# ===============
from openai import OpenAI
_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

def _embedding(texto: str):
    if not _client:
        return None
    try:
        resp = _client.embeddings.create(model="text-embedding-3-small", input=texto)
        return np.array(resp.data[0].embedding, dtype=np.float32)
    except Exception as e:
        st.warning(f"Falha no embedding: {e}")
        return None


def salvar_memoria_longa(texto: str, tags: str = "auto"):
    ws = get_ws("memoria_longa_jm")
    if not ws:
        return
    emb = _embedding(texto) if texto else None
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    row = [ts, texto, tags, json.dumps(emb.tolist() if emb is not None else []), 1]
    try:
        ws.append_row(row, value_input_option="RAW")
    except Exception as e:
        st.warning(f"NÃ£o foi possÃ­vel salvar memÃ³ria longa: {e}")


def buscar_memoria_longa(query: str, k: int = 3, limiar: float = 0.75):
    ws = get_ws("memoria_longa_jm")
    if not ws:
        return []
    try:
        vals = ws.get_all_records()
    except Exception:
        return []
    if not vals:
        return []
    qemb = _embedding(query)
    if qemb is None:
        return []
    # cosine
    def cos(a, b):
        a = np.array(a, dtype=np.float32); b = np.array(b, dtype=np.float32)
        denom = (np.linalg.norm(a) * np.linalg.norm(b))
        return float(np.dot(a, b) / denom) if denom else 0.0
    scored = []
    for r in vals:
        try:
            emb = json.loads(r.get("embedding_json", "[]"))
            if emb:
                s = cos(qemb, emb)
                if s >= limiar:
                    scored.append((s, r.get("texto", "")))
        except Exception:
            continue
    scored.sort(key=lambda x: x[0], reverse=True)
    return [t for _, t in scored[:k]]

# ===============
# INTERAÃ‡Ã•ES (histÃ³rico curto)
# ===============

def salvar_interacao(role: str, content: str):
    ws = get_ws("interacoes_jm")
    if not ws:
        return
    try:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        ws.append_row([ts, role, content], value_input_option="RAW")
    except Exception as e:
        st.warning(f"Falha ao salvar interaÃ§Ã£o: {e}")

@st.cache_data(ttl=10, show_spinner=False)
def carregar_ultimas_interacoes(n=20):
    ws = get_ws("interacoes_jm")
    if not ws:
        return []
    try:
        data = ws.get_all_records()
        return data[-n:]
    except Exception:
        return []

# ===============
# RESUMO (aba perfil_jm) â€” aparece DEPOIS do histÃ³rico
# ===============

def salvar_resumo(resumo: str):
    ws = get_ws("perfil_jm")
    if not ws:
        return
    try:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        # Tenta detectar se a planilha usa o formato antigo (resumo na coluna 7)
        vals = ws.get_all_values()
        if vals and len(vals[0]) >= 7:
            # Formato antigo (7 colunas): timestamp na 6, resumo na 7
            row = ["", "", "", "", "", ts, resumo]
        else:
            # Formato simples (2 colunas): timestamp, resumo
            row = [ts, resumo]
        ws.append_row(row, value_input_option="RAW")
    except Exception as e:
        st.warning(f"Falha ao salvar resumo: {e}")

@st.cache_data(ttl=20, show_spinner=False)
def carregar_ultimo_resumo():
    ws = get_ws("perfil_jm")
    if not ws:
        return None
    try:
        vals = ws.get_all_values()
        if len(vals) < 2:
            return None
        # Pega a Ãºltima linha nÃ£o vazia
        last = vals[-1]
        if len(last) >= 7 and last[6].strip():
            return last[6].strip()
        if len(last) >= 2 and last[1].strip():
            return last[1].strip()
        # Se a Ãºltima linha estiver vazia, tenta procurar de trÃ¡s pra frente
        for row in reversed(vals[1:-1]):
            if len(row) >= 7 and row[6].strip():
                return row[6].strip()
            if len(row) >= 2 and row[1].strip():
                return row[1].strip()
    except Exception:
        pass
    return None

# ===============
# REGRAS DE NARRATIVA (Guardrails)
# ===============(ttl=20, show_spinner=False)


# ===============
# REGRAS DE NARRATIVA (Guardrails)
# ===============
VIRGINIDADE_MARY = True  # Mary permanece virgem por vontade prÃ³pria
JANIO_LIVRE = True       # JÃ¢nio pode ter relaÃ§Ãµes com qualquer personagem

REGRAS_MARY = (
    "Mary Ã© virgem e escolhe manter-se casta atÃ© o encontro definitivo com JÃ¢nio. "
    "Ela pode namorar, beijar, flertar e viver tensÃ£o romÃ¢ntica, mas recusa sexo. "
    "Nunca descreva Mary transando, nem algo que implique penetraÃ§Ã£o ou perda da virgindade. "
    "Se a cena ameaÃ§ar ultrapassar limites, Mary impÃµe limites com firmeza e elegÃ¢ncia."
)

REGRAS_JANIO = (
    "JÃ¢nio nÃ£o tem essa limitaÃ§Ã£o: ele pode ter relaÃ§Ãµes consensuais com outros personagens, "
    "desde que a cena mantenha o bom gosto e evite descriÃ§Ãµes grÃ¡ficas explÃ­citas."
)

# Prompt base
BASE_SYSTEM = f"""
VocÃª Ã© o narrador de uma novela em portuguÃªs do Brasil. Escreva em 3Âª pessoa a narraÃ§Ã£o e use 1Âª pessoa para falas e pensamentos dos personagens.
Mantenha continuidade com o histÃ³rico e memÃ³rias fornecidas. Seja sensorial sem apelar para pornografia explÃ­cita.

REGRAS FIXAS:
- {REGRAS_MARY if VIRGINIDADE_MARY else ''}
- {REGRAS_JANIO if JANIO_LIVRE else ''}
- NÃ£o invente falas do usuÃ¡rio. O usuÃ¡rio sÃ³ fala quando ele enviar mensagem.
- Evite comandos tÃ©cnicos ([SFX], (cut), etc.).
""".strip()

# ===============
# MODELOS E CHATS
# ===============

# Listas completas (como na sua lousa)
MODELOS_OPENROUTER = {
    "ğŸ’¬ DeepSeek V3 â˜…â˜…â˜…â˜… ($)": "deepseek/deepseek-chat-v3-0324",
    "ğŸ§  DeepSeek R1 0528 â˜…â˜…â˜…â˜…â˜† ($$)": "deepseek/deepseek-r1-0528",
    "ğŸ§  DeepSeek R1T2 Chimera â˜…â˜…â˜…â˜… (free)": "tngtech/deepseek-r1t2-chimera:free",
    "ğŸ§  GPT-4.1 â˜…â˜…â˜…â˜…â˜… (1M ctx)": "openai/gpt-4.1",
    "ğŸ‘‘ WizardLM 8x22B â˜…â˜…â˜…â˜…â˜† ($$$)": "microsoft/wizardlm-2-8x22b",
    "ğŸ‘‘ Qwen 235B 2507 â˜…â˜…â˜…â˜…â˜… (PAID)": "qwen/qwen3-235b-a22b-07-25",
    "ğŸ‘‘ EVA Qwen2.5 72B â˜…â˜…â˜…â˜…â˜… (RP Pro)": "eva-unit-01/eva-qwen-2.5-72b",
    "ğŸ‘‘ EVA Llama 3.33 70B â˜…â˜…â˜…â˜…â˜… (RP Pro)": "eva-unit-01/eva-llama-3.33-70b",
    "ğŸ­ Nous Hermes 2 Yi 34B â˜…â˜…â˜…â˜…â˜†": "nousresearch/nous-hermes-2-yi-34b",
    "ğŸ”¥ MythoMax 13B â˜…â˜…â˜…â˜† ($)": "gryphe/mythomax-l2-13b",
    "ğŸ’‹ LLaMA3 Lumimaid 8B â˜…â˜…â˜† ($)": "neversleep/llama-3-lumimaid-8b",
    "ğŸŒ¹ Midnight Rose 70B â˜…â˜…â˜…â˜†": "sophosympatheia/midnight-rose-70b",
    "ğŸŒ¶ï¸ Noromaid 20B â˜…â˜…â˜†": "neversleep/noromaid-20b",
    "ğŸ’€ Mythalion 13B â˜…â˜…â˜†": "pygmalionai/mythalion-13b",
    "ğŸ‰ Anubis 70B â˜…â˜…â˜†": "thedrummer/anubis-70b-v1.1",
    "ğŸ§š Rocinante 12B â˜…â˜…â˜†": "thedrummer/rocinante-12b",
    "ğŸ· Magnum v2 72B â˜…â˜…â˜†": "anthracite-org/magnum-v2-72b",
}

MODELOS_TOGETHER_UI = {
    "ğŸ§  Qwen3 Coder 480B (Together)": "togethercomputer/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "ğŸ‘‘ Mixtral 8x7B v0.1 (Together)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "ğŸ‘‘ Perplexity R1-1776 (Together)": "perplexity-ai/r1-1776",
}

# DicionÃ¡rio unificado esperado pelo resto do app
MODELOS = {}
for nome, mid in MODELOS_OPENROUTER.items():
    MODELOS[nome] = {"prov": "openrouter", "id": mid}
for nome, mid in MODELOS_TOGETHER_UI.items():
    MODELOS[nome] = {"prov": "together", "id": mid}

def _stream_openrouter(model_id: str, messages, temperature=0.85, max_tokens=700):
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": model_id,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": True,
    }
    resposta_txt = ""
    with requests.post(url, headers=headers, json=payload, stream=True, timeout=300) as r:
        r.raise_for_status()
        for raw in r.iter_lines(decode_unicode=True):
            if not raw:
                continue
            if not raw.startswith("data:"):
                continue
            data = raw[5:].strip()
            if data == "[DONE]":
                break
            try:
                j = json.loads(data)
                delta = j["choices"][0]["delta"].get("content", "")
                if delta:
                    resposta_txt += delta
                    yield resposta_txt
            except Exception:
                continue


def _stream_together(model_id: str, messages, temperature=0.85, max_tokens=700):
    url = "https://api.together.xyz/v1/chat/completions"
    headers = {"Authorization": f"Bearer {TOGETHER_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": model_id,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": True,
    }
    resposta_txt = ""
    with requests.post(url, headers=headers, json=payload, stream=True, timeout=300) as r:
        # Tratar 400 gracefully
        if r.status_code == 400:
            raise RuntimeError(f"Together 400: {r.text}")
        r.raise_for_status()
        for raw in r.iter_lines(decode_unicode=True):
            if not raw:
                continue
            if raw.strip() == "data: [DONE]":
                break
            if raw.startswith("data:"):
                data = raw[5:].strip()
                try:
                    j = json.loads(data)
                    delta = j["choices"][0]["delta"].get("content", "")
                    # Perplexity pode devolver <think> â€¦ </think>. NÃƒO remova â€” exiba completo.
                    if delta:
                        resposta_txt += delta
                        yield resposta_txt
                except Exception:
                    continue


def gerar_resposta(modelo_cfg, historico_msgs):
    prov = modelo_cfg["prov"]
    model_id = modelo_cfg["id"]

    # Recuperar memÃ³rias longas relevantes a partir da Ãºltima entrada do usuÃ¡rio
    ultima_usuario = next((m["content"] for m in reversed(historico_msgs) if m["role"] == "user"), "")
    mem_longas = buscar_memoria_longa(ultima_usuario, k=3, limiar=0.75)
    bloco_memoria = ("\n\nMemÃ³rias relevantes:\n" + "\n".join([f"- {t}" for t in mem_longas])) if mem_longas else ""

    system = {"role": "system", "content": BASE_SYSTEM + bloco_memoria}
    msgs = [system] + historico_msgs

    if prov == "openrouter":
        return _stream_openrouter(model_id, msgs)
    elif prov == "together":
        return _stream_together(model_id, msgs)
    else:
        raise ValueError("Provedor desconhecido")

# ===============
# STATE INICIAL
# ===============
if "hist" not in st.session_state:
    # hist guarda apenas user/assistant desta sessÃ£o (na tela)
    st.session_state.hist = []
if "modelo_nome" not in st.session_state:
    st.session_state.modelo_nome = list(MODELOS.keys())[0]

# ===============
# UI LATERAL
# ===============
with st.sidebar:
    st.subheader("Config")
    st.session_state.modelo_nome = st.selectbox("Modelo", list(MODELOS.keys()), index=list(MODELOS.keys()).index(st.session_state.modelo_nome))
    st.caption("Trocar de modelo NÃƒO apaga histÃ³rico.")

    st.markdown("---")
    st.subheader("MemÃ³ria Longa")
    if st.button("Salvar Ãºltima resposta como memÃ³ria", use_container_width=True):
        ult = next((m["content"] for m in reversed(st.session_state.hist) if m["role"] == "assistant"), "")
        if ult:
            salvar_memoria_longa(ult, tags="auto")
            st.success("MemÃ³ria salva.")
        else:
            st.info("Sem resposta para salvar ainda.")

    st.markdown("---")
    st.subheader("Resumo do capÃ­tulo")
    if st.button("Gerar e salvar resumo", use_container_width=True):
        # Usa o prÃ³prio modelo atual para resumir as Ãºltimas interaÃ§Ãµes da sessÃ£o
        trecho = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.hist[-10:]]) or "(vazio)"
        user_prompt = (
            "Resuma como capÃ­tulo de novela, coeso e elegante, sem pornografia, mantendo tom emocional.\n\n" + trecho
        )
        modelo_cfg = MODELOS[st.session_state.modelo_nome]
        try:
            placeholder = st.empty()
            out = ""
            spinner = st.spinner("Resumindo...")
            with spinner:
                for parcial in gerar_resposta(modelo_cfg, [
                    {"role": "user", "content": user_prompt}
                ]):
                    out = parcial
                    placeholder.markdown(out)
            if out.strip():
                salvar_resumo(out.strip())
                st.success("Resumo salvo na aba perfil_jm.")
        except Exception as e:
            st.error(f"Erro ao resumir: {e}")

# ===============
# HISTÃ“RICO DA PLANILHA + SESSÃƒO
# ===============
col_hist, = st.columns(1)
with col_hist:
    st.title("ğŸŒ¹ Mary / ğŸ¸ JÃ¢nio â€” Novela Interativa")

# Render histÃ³rico curto recente da planilha (somente exibiÃ§Ã£o)
historico_planilha = carregar_ultimas_interacoes(15)
for m in historico_planilha:
    with st.chat_message(m.get("role", "user")):
        st.markdown(fix_mojibake(m.get("content", "")))

# Render histÃ³rico da sessÃ£o atual
for m in st.session_state.hist:
    with st.chat_message(m["role"]):
        st.markdown(fix_mojibake(m["content"])) 

# Depois de TUDO, mostra o Ãºltimo resumo
ultimo_resumo = carregar_ultimo_resumo()
if ultimo_resumo:
    with st.chat_message("assistant"):
        st.markdown("### ğŸ§  Resumo do capÃ­tulo anterior\n\n" + fix_mojibake(ultimo_resumo))

# ===============
# ENTRADA DO USUÃRIO
# ===============
entrada = st.chat_input("Digite sua cena / direÃ§Ã£o narrativa...")
if entrada:
    # salva user na planilha e no estado
    st.chat_message("user").markdown(entrada)
    st.session_state.hist.append({"role": "user", "content": entrada})
    salvar_interacao("user", entrada)

    # montar histÃ³rico para envio ao modelo (usa apenas a sessÃ£o atual + 6 Ãºltimas da planilha p/ contexto)
    contexto_base = []
    for m in historico_planilha[-6:]:
        contexto_base.append({"role": m.get("role", "user"), "content": m.get("content", "")})
    contexto = contexto_base + st.session_state.hist[-8:]

    modelo_cfg = MODELOS[st.session_state.modelo_nome]

    # Gera resposta streaming com fallback
    with st.chat_message("assistant"):
        placeholder = st.empty()
        resposta_txt = ""
        try:
            for parcial in gerar_resposta(modelo_cfg, contexto):
                resposta_txt = parcial
                placeholder.markdown(resposta_txt if resposta_txt.strip() else "[Gerandoâ€¦]")
            # fallback se veio vazio
            if not resposta_txt.strip():
                # tentativa nÃ£o-stream
                prov = modelo_cfg["prov"]; model_id = modelo_cfg["id"]
                if prov == "openrouter":
                    url = "https://openrouter.ai/api/v1/chat/completions"
                    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
                else:
                    url = "https://api.together.xyz/v1/chat/completions"
                    headers = {"Authorization": f"Bearer {TOGETHER_API_KEY}", "Content-Type": "application/json"}
                payload = {
                    "model": model_id,
                    "messages": [{"role": "system", "content": BASE_SYSTEM}] + contexto,
                    "temperature": 0.85,
                    "max_tokens": 700,
                    "stream": False,
                }
                r = requests.post(url, headers=headers, json=payload, timeout=120)
                if r.status_code == 200:
                    j = r.json()
                    resposta_txt = j.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                else:
                    raise RuntimeError(f"Fallback falhou: {r.status_code} - {r.text}")
                placeholder.markdown(resposta_txt or "[Sem conteÃºdo]")
        except Exception as e:
            placeholder.markdown(f"[Erro: {e}]")
            resposta_txt = f"[Erro: {e}]"

    # Salvar resposta (sempre)
    st.session_state.hist.append({"role": "assistant", "content": resposta_txt})
    salvar_interacao("assistant", resposta_txt)

    # Opcional: salvar pedacinhos em memÃ³ria longa automaticamente
    if len(resposta_txt) > 300:
        try:
            salvar_memoria_longa(resposta_txt[:1200], tags="auto")
        except Exception:
            pass

# ===============
# FIM
# ===============
