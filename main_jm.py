import streamlit as st
import requests
import gspread
import json
import re
import time
import numpy as np
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from openai import OpenAI

# ============================================================
# Configura√ß√£o b√°sica
# ============================================================
st.set_page_config(page_title="üé¨ Narrador JM", page_icon="üé¨")

# Chaves esperadas em st.secrets:
# - GOOGLE_CREDS_JSON
# - OPENROUTER_API_KEY
# - TOGETHER_API_KEY
# - OPENAI_API_KEY

# Cliente OpenAI para embeddings SEMPRE via OPENAI_API_KEY
client_openai = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# ============================================================
# Conectar √† planilha
# ============================================================
def conectar_planilha():
    try:
        creds_dict = json.loads(st.secrets["GOOGLE_CREDS_JSON"])
        creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        return client.open_by_key("1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM")
    except Exception as e:
        st.error(f"Erro ao conectar √† planilha: {e}")
        return None

planilha = conectar_planilha()

# ============================================================
# Utilidades de planilha (mem√≥rias curtas, intera√ß√µes, resumos)
# ============================================================
def carregar_memorias():
    """L√™ a aba 'memorias_jm' e retorna (mem_mary, mem_janio, mem_all)."""
    try:
        aba = planilha.worksheet("memorias_jm")
        registros = aba.get_all_records()
        mem_mary = [r["conteudo"] for r in registros if r.get("tipo", "").strip().lower() == "[mary]"]
        mem_janio = [r["conteudo"] for r in registros if r.get("tipo", "").strip().lower() in ("[j√¢nio]", "[janio]")]
        mem_all = [r["conteudo"] for r in registros if r.get("tipo", "").strip().lower() == "[all]"]
        return mem_mary, mem_janio, mem_all
    except Exception as e:
        st.warning(f"Erro ao carregar mem√≥rias: {e}")
        return [], [], []

def carregar_resumo_salvo():
    """Busca o √∫ltimo resumo (coluna 7) da aba 'perfil_jm'."""
    try:
        aba = planilha.worksheet("perfil_jm")
        valores = aba.col_values(7)
        for val in reversed(valores[1:]):
            if val and val.strip():
                return val.strip()
        return ""
    except Exception as e:
        st.warning(f"Erro ao carregar resumo salvo: {e}")
        return ""

def salvar_resumo(resumo: str):
    """Salva um novo resumo na aba 'perfil_jm' (timestamp na coluna 6, resumo na 7)."""
    try:
        aba = planilha.worksheet("perfil_jm")
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        linha = ["", "", "", "", "", timestamp, resumo]
        aba.append_row(linha, value_input_option="RAW")
    except Exception as e:
        st.error(f"Erro ao salvar resumo: {e}")

def salvar_interacao(role: str, content: str):
    """Anexa uma intera√ß√£o na aba 'interacoes_jm'."""
    if not planilha:
        return
    try:
        aba = planilha.worksheet("interacoes_jm")
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        aba.append_row([timestamp, role.strip(), content.strip()], value_input_option="RAW")
    except Exception as e:
        st.error(f"Erro ao salvar intera√ß√£o: {e}")

def carregar_interacoes(n=20):
    """Carrega as √∫ltimas n intera√ß√µes (role, content)."""
    try:
        aba = planilha.worksheet("interacoes_jm")
        registros = aba.get_all_records()
        return registros[-n:] if len(registros) > n else registros
    except Exception as e:
        st.warning(f"Erro ao carregar intera√ß√µes: {e}")
        return []

# ============================================================
# Valida√ß√µes (sint√°tica + sem√¢ntica com OpenAI Embeddings)
# ============================================================
def resposta_valida(texto: str) -> bool:
    # Heur√≠stica simples para detectar ‚Äúresposta corrompida‚Äù
    padroes_invalidos = [
        r"check if.*string", r"#\s?1(\.\d+)+", r"\d{10,}", r"the cmd package",
        r"(111\s?)+", r"#+\s*\d+", r"\bimport\s", r"\bdef\s", r"```", r"class\s"
    ]
    for padrao in padroes_invalidos:
        if re.search(padrao, texto.lower()):
            return False
    return True

def gerar_embedding_openai(texto: str):
    try:
        resp = client_openai.embeddings.create(
            input=texto,
            model="text-embedding-3-small"
        )
        return np.array(resp.data[0].embedding, dtype=float)
    except Exception as e:
        st.error(f"Erro ao gerar embedding: {e}")
        return None

def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    if v1 is None or v2 is None:
        return 0.0
    denom = float(np.linalg.norm(v1) * np.linalg.norm(v2))
    if denom == 0.0:
        return 0.0
    return float(np.dot(v1, v2) / denom)

def verificar_quebra_semantica_openai(texto1: str, texto2: str, limite=0.6) -> str:
    e1 = gerar_embedding_openai(texto1)
    e2 = gerar_embedding_openai(texto2)
    if e1 is None or e2 is None:
        return ""
    sim = cosine_similarity(e1, e2)
    if sim < limite:
        return f"‚ö†Ô∏è Baixa continuidade narrativa (similaridade: {sim:.2f})."
    return ""

# ============================================================
# Mem√≥ria vetorial de longo prazo (Sheets + OpenAI embedding)
# ============================================================
def _mem_longa_sheet():
    return planilha.worksheet("memoria_longa_jm")

def salvar_memoria_longa(texto: str, tags: str = ""):
    """Cria embedding do texto, inicia score=1.0 e salva na 'memoria_longa_jm'."""
    emb = gerar_embedding_openai(texto)
    if emb is None:
        return False
    try:
        aba = _mem_longa_sheet()
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        aba.append_row(
            [timestamp, texto, tags or "", json.dumps(emb.tolist()), "1.0"],
            value_input_option="RAW"
        )
        return True
    except Exception as e:
        st.warning(f"Erro ao salvar mem√≥ria longa: {e}")
        return False

def _decay_score(score: float, rounds: int = 1, fator: float = 0.97):
    return float(score * (fator ** rounds))

def _boost_score(score: float, inc: float = 0.2, max_score: float = 2.0):
    return float(min(max_score, score + inc))

def buscar_memorias_relevantes(query_text: str, top_k: int = 3, min_sim: float = 0.78):
    """
    Retorna at√© top_k mem√≥rias re-ranqueadas por: 0.7*similaridade + 0.3*score.
    """
    q_emb = gerar_embedding_openai(query_text)
    if q_emb is None:
        return []

    try:
        aba = _mem_longa_sheet()
        dados = aba.get_all_records()
    except Exception as e:
        st.warning(f"Erro ao ler mem√≥ria longa: {e}")
        return []

    candidatos = []
    for r in dados:
        try:
            e = np.array(json.loads(r.get("embedding_json", "[]")), dtype=float)
            if e.size == 0:
                continue
            sim = cosine_similarity(q_emb, e)
            score = float(r.get("score", 1.0))
            rerank = 0.7 * sim + 0.3 * score
            if sim >= min_sim:
                candidatos.append({
                    "timestamp": r.get("timestamp", ""),
                    "texto": r.get("texto", ""),
                    "tags": r.get("tags", ""),
                    "score": score,
                    "sim": sim,
                    "rerank": rerank,
                })
        except Exception:
            continue

    candidatos.sort(key=lambda x: x["rerank"], reverse=True)
    return candidatos[:top_k]

def reforcar_memorias_utilizadas(textos_usados: list[str]):
    """Quando usar mem√≥rias no prompt, chama isso pra dar boost nelas e salvar de volta."""
    if not textos_usados:
        return
    try:
        aba = _mem_longa_sheet()
        valores = aba.get_all_values()
        if not valores:
            return
        header = valores[0]
        linhas = valores[1:]
        idx_texto = header.index("texto")
        idx_score = header.index("score")
        for i, linha in enumerate(linhas, start=2):  # cabe√ßalho na linha 1
            if len(linha) <= max(idx_texto, idx_score):
                continue
            if linha[idx_texto] in textos_usados:
                try:
                    sc = float(linha[idx_score] or "1.0")
                    sc = _boost_score(sc)
                    aba.update_cell(i, idx_score + 1, str(sc))
                except Exception:
                    continue
    except Exception as e:
        st.warning(f"Erro ao refor√ßar mem√≥rias: {e}")

# ============================================================
# Estado estruturado de cena
# ============================================================
if "scene" not in st.session_state:
    st.session_state.scene = {
        "local": "",
        "tempo": "",
        "objetivo_mary": "",
        "objetivo_janio": "",
        "tensao": 0.0,
        "ganchos": [],
        "itens_relevantes": [],
        "intent": {},
    }

def atualizar_scene_state_via_llm(texto_resposta: str):
    """
    Chamada r√°pida (sem stream) ao provedor atual pedindo um JSON compacto com o estado da cena.
    Usa o provedor/modelo j√° selecionado.
    """
    prov = st.session_state.get("provedor_ia", "OpenRouter")
    if prov == "Together":
        endpoint = "https://api.together.xyz/v1/chat/completions"
        auth = st.secrets["TOGETHER_API_KEY"]
        model_to_call = None
        try:
            model_to_call = st.session_state.modelo_escolhido_id
            model_to_call = model_id_for_together(model_to_call)
        except Exception:
            model_to_call = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    else:
        endpoint = "https://openrouter.ai/api/v1/chat/completions"
        auth = st.secrets["OPENROUTER_API_KEY"]
        model_to_call = st.session_state.get("modelo_escolhido_id", "deepseek/deepseek-chat-v3-0324")

    sys = (
        "Extraia um estado de cena em JSON com as chaves: "
        "local, tempo, objetivo_mary, objetivo_janio, tensao (0..1), ganchos (lista), itens_relevantes (lista). "
        "Se n√£o souber, deixe vazio. Responda APENAS o JSON v√°lido."
    )
    user = f"Texto da cena:\n{texto_resposta}\n\nJSON:"
    try:
        r = requests.post(
            endpoint,
            headers={"Authorization": f"Bearer {auth}", "Content-Type": "application/json"},
            json={
                "model": model_to_call,
                "messages": [
                    {"role": "system", "content": sys},
                    {"role": "user", "content": user},
                ],
                "max_tokens": 220,
                "temperature": 0.2,
                "stream": False,
            },
            timeout=60,
        )
        if r.status_code == 200:
            txt = r.json()["choices"][0]["message"]["content"].strip()
            try:
                parsed = json.loads(txt)
                st.session_state.scene.update({
                    "local": parsed.get("local", st.session_state.scene["local"]),
                    "tempo": parsed.get("tempo", st.session_state.scene["tempo"]),
                    "objetivo_mary": parsed.get("objetivo_mary", st.session_state.scene["objetivo_mary"]),
                    "objetivo_janio": parsed.get("objetivo_janio", st.session_state.scene["objetivo_janio"]),
                    "tensao": parsed.get("tensao", st.session_state.scene["tensao"]),
                    "ganchos": parsed.get("ganchos", st.session_state.scene["ganchos"]),
                    "itens_relevantes": parsed.get("itens_relevantes", st.session_state.scene["itens_relevantes"]),
                })
            except Exception:
                pass
    except Exception:
        pass

# ============================================================
# Provedores e modelos
# ============================================================
MODELOS_OPENROUTER = {
    # === OPENROUTER ===
    "üí¨ DeepSeek V3 ‚òÖ‚òÖ‚òÖ‚òÖ ($)": "deepseek/deepseek-chat-v3-0324",
    "üß† DeepSeek R1 0528 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$)": "deepseek/deepseek-r1-0528",
    "üß† DeepSeek R1T2 Chimera ‚òÖ‚òÖ‚òÖ‚òÖ (free)": "tngtech/deepseek-r1t2-chimera:free",
    "üß† GPT-4.1 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (1M ctx)": "openai/gpt-4.1",
    "üëë WizardLM 8x22B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$$)": "microsoft/wizardlm-2-8x22b",
    "üëë Qwen 235B 2507 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (PAID)": "qwen/qwen3-235b-a22b-07-25",
    "üëë EVA Qwen2.5 72B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-qwen-2.5-72b",
    "üëë EVA Llama 3.33 70B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-llama-3.33-70b",
    "üé≠ Nous Hermes 2 Yi 34B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ": "nousresearch/nous-hermes-2-yi-34b",
    "üî• MythoMax 13B ‚òÖ‚òÖ‚òÖ‚òÜ ($)": "gryphe/mythomax-l2-13b",
    "üíã LLaMA3 Lumimaid 8B ‚òÖ‚òÖ‚òÜ ($)": "neversleep/llama-3-lumimaid-8b",
    "üåπ Midnight Rose 70B ‚òÖ‚òÖ‚òÖ‚òÜ": "sophosympatheia/midnight-rose-70b",
    "üå∂Ô∏è Noromaid 20B ‚òÖ‚òÖ‚òÜ": "neversleep/noromaid-20b",
    "üíÄ Mythalion 13B ‚òÖ‚òÖ‚òÜ": "pygmalionai/mythalion-13b",
    "üêâ Anubis 70B ‚òÖ‚òÖ‚òÜ": "thedrummer/anubis-70b-v1.1",
    "üßö Rocinante 12B ‚òÖ‚òÖ‚òÜ": "thedrummer/rocinante-12b",
    "üç∑ Magnum v2 72B ‚òÖ‚òÖ‚òÜ": "anthracite-org/magnum-v2-72b",
}

# Together (UI -> mapeamento p/ ID aceito pela API)
MODELOS_TOGETHER_UI = {
    "üß† Qwen3 Coder 480B (Together)": "togethercomputer/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "üëë Mixtral 8x7B v0.1 (Together)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "üëë perplexity-ai (Together)": "perplexity-ai/r1-1776",
}
def model_id_for_together(api_ui_model_id: str) -> str:
    # Corrige para os IDs aceitos pela Together API
    if "Qwen3-Coder-480B-A35B-Instruct-FP8" in api_ui_model_id:
        return "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
    if api_ui_model_id.lower().startswith("mistralai/mixtral-8x7b-instruct-v0.1"):
        return "mistralai/Mixtral-8x7B-Instruct-v0.1"
    return api_ui_model_id

def api_config_for_provider(provider: str):
    if provider == "OpenRouter":
        return (
            "https://openrouter.ai/api/v1/chat/completions",
            st.secrets["OPENROUTER_API_KEY"],
            MODELOS_OPENROUTER,
        )
    else:
        return (
            "https://api.together.xyz/v1/chat/completions",
            st.secrets["TOGETHER_API_KEY"],
            MODELOS_TOGETHER_UI,
        )

# ============================================================
# Constru√ß√£o do prompt (com mem√≥rias vetoriais + scene state)
# ============================================================
def construir_prompt_com_narrador():
    mem_mary, mem_janio, mem_all = carregar_memorias()
    emocao = st.session_state.get("emocao_oculta", "nenhuma")
    resumo = st.session_state.get("resumo_capitulo", "")

    # √∫ltimas intera√ß√µes (compacto)
    try:
        aba = planilha.worksheet("interacoes_jm")
        registros = aba.get_all_records()
        ultimas = registros[-10:] if len(registros) > 10 else registros
        texto_ultimas = "\n".join(f"{r['role']}: {r['content']}" for r in ultimas)
    except Exception:
        registros = []
        texto_ultimas = ""

    # consulta p/ mem√≥ria vetorial = √∫ltima fala do usu√°rio
    consulta_mem = ""
    try:
        for r in reversed(registros):
            if r.get("role") == "user":
                consulta_mem = r.get("content", "")
                break
    except Exception:
        pass

    mem_relevantes = buscar_memorias_relevantes(consulta_mem) if consulta_mem else []
    st.session_state._mem_relevantes = mem_relevantes  # para refor√ßo ap√≥s resposta
    bloco_mem_relev = "\n".join([f"- {m['texto']}" for m in mem_relevantes]) if mem_relevantes else "Nenhuma encontrada para o momento."

    # scene state atual
    scene = st.session_state.get("scene", {})
    bloco_scene = (
        f"Local: {scene.get('local','')}\n"
        f"Tempo: {scene.get('tempo','')}\n"
        f"Objetivo de Mary: {scene.get('objetivo_mary','')}\n"
        f"Objetivo de J√¢nio: {scene.get('objetivo_janio','')}\n"
        f"Tens√£o: {scene.get('tensao',0.0)}\n"
        f"Ganchos: {', '.join(scene.get('ganchos',[]))}\n"
        f"Itens relevantes: {', '.join(scene.get('itens_relevantes',[]))}\n"
    )

    regra_intimo = (
        "\n‚õî Jamais antecipe encontros, conex√µes emocionais ou cenas √≠ntimas sem ordem expl√≠cita do roteirista."
        if st.session_state.get("bloqueio_intimo", False) else ""
    )

    prompt = f"""Voc√™ √© o narrador de uma hist√≥ria em constru√ß√£o. Os protagonistas s√£o Mary e J√¢nio.

Sua fun√ß√£o √© narrar cenas com naturalidade e profundidade. Use narra√ß√£o em 3¬™ pessoa e falas/pensamentos dos personagens em 1¬™ pessoa.{regra_intimo}

üé≠ Emo√ß√£o oculta: {emocao}

üìñ Cap√≠tulo anterior:
{(resumo or 'Nenhum resumo salvo.')}

### üß† Mem√≥rias (fixas):
Mary:
- {("\n- ".join(mem_mary)) if mem_mary else 'Nenhuma.'}

J√¢nio:
- {("\n- ".join(mem_janio)) if mem_janio else 'Nenhuma.'}

Compartilhadas:
- {("\n- ".join(mem_all)) if mem_all else 'Nenhuma.'}

### üß† Mem√≥rias relevantes (recupera√ß√£o vetorial):
{bloco_mem_relev}

### üéõÔ∏è Estado Atual da Cena
{bloco_scene}

### üìñ √öltimas intera√ß√µes (recorte):
{texto_ultimas}

### üéØ Estilo de prosa
- ‚ÄúMostre, n√£o conte‚Äù: prefira a√ß√µes e detalhes sensoriais.
- Em cada frase, foque **um** canal sensorial (som/cheiro/tato/vis√£o).
- Falas curtas, com subtexto (‚â§ 18 palavras).
- Avance o enredo suavemente, sem cortes bruscos.
"""
    return prompt.strip()

# ============================================================
# UI ‚Äì Cabe√ßalho e controles
# ============================================================
st.title("üé¨ Narrador JM")
st.subheader("Voc√™ √© o roteirista. Digite uma dire√ß√£o de cena. A IA narrar√° Mary e J√¢nio.")
st.markdown("---")

# Estado inicial
if "resumo_capitulo" not in st.session_state:
    st.session_state.resumo_capitulo = carregar_resumo_salvo()
if "session_msgs" not in st.session_state:
    st.session_state.session_msgs = []
if "provedor_ia" not in st.session_state:
    st.session_state.provedor_ia = "OpenRouter"

# Linha de op√ß√µes r√°pidas
col1, col2 = st.columns([3, 2])
with col1:
    st.markdown("#### üìñ √öltimo resumo salvo:")
    st.info(st.session_state.resumo_capitulo or "Nenhum resumo dispon√≠vel.")
with col2:
    st.markdown("#### ‚öôÔ∏è Op√ß√µes")
    st.session_state.bloqueio_intimo = st.checkbox("Bloquear avan√ßos √≠ntimos sem ordem", value=False)
    st.session_state.emocao_oculta = st.selectbox(
        "üé≠ Emo√ß√£o oculta", ["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"], index=0
    )

# ============================================================
# Sidebar ‚Äì Provedor, modelos e resumo
# ============================================================
with st.sidebar:
    st.title("üß≠ Painel do Roteirista")

    provedor = st.radio("üåê Provedor", ["OpenRouter", "Together"], index=0, key="provedor_ia")
    api_url, api_key, modelos_map = api_config_for_provider(provedor)

    modelo_nome = st.selectbox("ü§ñ Modelo de IA", list(modelos_map.keys()), index=0, key="modelo_nome_ui")
    modelo_escolhido_id_ui = modelos_map[modelo_nome]
    st.session_state.modelo_escolhido_id = modelo_escolhido_id_ui  # armazenar para uso no envio

    st.markdown("---")
    if st.button("üìù Gerar resumo do cap√≠tulo"):
        try:
            inter = carregar_interacoes(n=6)
            texto = "\n".join(f"{r['role']}: {r['content']}" for r in inter) if inter else ""
            prompt_resumo = (
                "Resuma o seguinte trecho como um cap√≠tulo de novela brasileiro, mantendo tom e emo√ß√µes.\n\n"
                + texto + "\n\nResumo:"
            )

            # escolher o modelo atual tamb√©m para o resumo
            if provedor == "Together":
                model_id_call = model_id_for_together(modelo_escolhido_id_ui)
            else:
                model_id_call = modelo_escolhido_id_ui

            r = requests.post(
                api_url,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json={
                    "model": model_id_call,
                    "messages": [{"role": "user", "content": prompt_resumo}],
                    "max_tokens": 800,
                    "temperature": 0.85,
                },
                timeout=120,
            )
            if r.status_code == 200:
                resumo = r.json()["choices"][0]["message"]["content"].strip()
                st.session_state.resumo_capitulo = resumo
                salvar_resumo(resumo)
                st.success("Resumo gerado e salvo com sucesso!")
            else:
                st.error(f"Erro ao resumir: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro ao gerar resumo: {e}")

    st.caption("Role a tela principal para ver intera√ß√µes anteriores.")

# ============================================================
# Exibir hist√≥rico recente (role para cima para ver mais)
# ============================================================
with st.container():
    interacoes = carregar_interacoes(n=20)
    for r in interacoes:
        role = r.get("role", "user")
        content = r.get("content", "")
        if role == "user":
            with st.chat_message("user"):
                st.markdown(content)
        else:
            with st.chat_message("assistant"):
                st.markdown(content)

# ============================================================
# Envio do usu√°rio + Streaming com ‚Äúdigita√ß√£o‚Äù
# ============================================================
entrada = st.chat_input("Digite sua dire√ß√£o de cena...")
if entrada:
    # Salva e mostra a fala do usu√°rio
    salvar_interacao("user", entrada)
    st.session_state.session_msgs.append({"role": "user", "content": entrada})

    # Construir prompt e hist√≥rico
    prompt = construir_prompt_com_narrador()
    historico = [{"role": m.get("role", "user"), "content": m.get("content", "")}
                 for m in st.session_state.session_msgs]

    # Penalidades din√¢micas se repetindo demais
    freq_penalty = 0.0
    presence_penalty = 0.0
    try:
        ult_assist = ""
        for m in reversed(st.session_state.session_msgs):
            if m.get("role") == "assistant":
                ult_assist = m.get("content", "")
                break
        if ult_assist:
            sim_rep = cosine_similarity(
                gerar_embedding_openai(ult_assist), gerar_embedding_openai(entrada)
            )
            if sim_rep and sim_rep > 0.92:
                freq_penalty = 0.2
                presence_penalty = 0.4
    except Exception:
        pass

    # Roteia por provedor e ajusta ID do Together quando necess√°rio
    prov = st.session_state.get("provedor_ia", "OpenRouter")
    if prov == "Together":
        endpoint = "https://api.together.xyz/v1/chat/completions"
        auth = st.secrets["TOGETHER_API_KEY"]
        model_to_call = model_id_for_together(st.session_state.modelo_escolhido_id)
    else:
        endpoint = "https://openrouter.ai/api/v1/chat/completions"
        auth = st.secrets["OPENROUTER_API_KEY"]
        model_to_call = st.session_state.modelo_escolhido_id

    payload = {
        "model": model_to_call,
        "messages": [{"role": "system", "content": prompt}] + historico,
        "max_tokens": 900,
        "temperature": 0.85,
        "frequency_penalty": freq_penalty,
        "presence_penalty": presence_penalty,
        "stream": True,
    }
    headers = {"Authorization": f"Bearer {auth}", "Content-Type": "application/json"}

    # Streaming com ‚Äúmarcapasso‚Äù para digita√ß√£o fluida
    with st.chat_message("assistant"):
        placeholder = st.empty()
        resposta_txt = ""
        buffer_txt = ""
        last_flush = time.time()

        try:
            with requests.post(endpoint, headers=headers, json=payload, stream=True, timeout=300) as r:
                if r.status_code != 200:
                    st.error(f"Erro {('Together' if prov=='Together' else 'OpenRouter')}: {r.status_code} - {r.text}")
                    resposta_txt = "[ERRO STREAM]"
                else:
                    for raw in r.iter_lines(decode_unicode=False):
                        if not raw:
                            continue
                        line = raw.decode("utf-8", errors="ignore").strip()
                        if not line.startswith("data:"):
                            continue
                        data = line[5:].strip()
                        if data == "[DONE]":
                            break
                        try:
                            j = json.loads(data)
                            delta = j["choices"][0]["delta"].get("content", "")
                            if delta:
                                buffer_txt += delta
                                now = time.time()
                                if (now - last_flush) > 0.10 or buffer_txt.count(" ") > 20:
                                    resposta_txt += buffer_txt
                                    buffer_txt = ""
                                    last_flush = now
                                    placeholder.markdown(resposta_txt + "‚ñå")
                        except Exception:
                            continue
        except Exception as e:
            st.error(f"Erro no streaming: {e}")
            resposta_txt = "[Erro ao gerar resposta]"

        # Flush final do buffer
        if buffer_txt:
            resposta_txt += buffer_txt
            buffer_txt = ""

        # Valida√ß√£o sint√°tica
        if not resposta_valida(resposta_txt):
            st.warning("‚ö†Ô∏è Resposta corrompida detectada. Tentando regenerar...")
            try:
                regen = requests.post(
                    endpoint,
                    headers=headers,
                    json={
                        "model": model_to_call,
                        "messages": [{"role": "system", "content": prompt}] + historico,
                        "max_tokens": 900,
                        "temperature": 0.85,
                        "stream": False,
                    },
                    timeout=180,
                )
                if regen.status_code == 200:
                    resposta_txt = regen.json()["choices"][0]["message"]["content"].strip()
                else:
                    st.error(f"Erro ao regenerar: {regen.status_code} - {regen.text}")
            except Exception as e:
                st.error(f"Erro ao regenerar: {e}")

        # Valida√ß√£o sem√¢ntica (com OpenAI embeddings) entre √∫ltima entrada do user e resposta
        if len(st.session_state.session_msgs) >= 1 and resposta_txt and resposta_txt != "[ERRO STREAM]":
            texto_anterior = st.session_state.session_msgs[-1]["content"]  # √∫ltima entrada do user
            alerta = verificar_quebra_semantica_openai(texto_anterior, resposta_txt)
            if alerta:
                st.info(alerta)

        # Finaliza na tela
        placeholder.markdown(resposta_txt or "[Sem conte√∫do]")
        salvar_interacao("assistant", resposta_txt)
        st.session_state.session_msgs.append({"role": "assistant", "content": resposta_txt})

        # Refor√ßa mem√≥rias usadas nesta rodada (se houver)
        if st.session_state.get("_mem_relevantes"):
            try:
                reforcar_memorias_utilizadas([m['texto'] for m in st.session_state._mem_relevantes])
            except Exception:
                pass

        # Atualiza estado da cena
        atualizar_scene_state_via_llm(resposta_txt)

