# roleplay_narrador_jm_clean_messages.py
# ---------------------------------------------------------------------------------
# Objetivo: manter toda a ESTRUTURA DE MODELOS do app original, mas
# ENVIAR APENAS MENSAGENS M√çNIMAS aos modelos (sem prompts auxiliares, fases, etc.).
# - System √∫nico (persona Mary)
# - Hist√≥rico user/assistant como est√°
# - Persist√™ncia s√≥ em "interacoes_jm"
# ---------------------------------------------------------------------------------

import json
import re
from datetime import datetime
from typing import Dict, List, Any

import gspread
import requests
import streamlit as st
from gspread.exceptions import APIError, GSpreadException
from oauth2client.service_account import ServiceAccountCredentials
from huggingface_hub import InferenceClient

st.set_page_config(page_title="Narrador JM ‚Äî Clean Messages", page_icon="üé¨")

# =================================================================================
# Config Planilha
# =================================================================================
PLANILHA_ID = (
    st.secrets.get("JM_SHEET_ID")
    or st.secrets.get("SPREADSHEET_ID")
    or "1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM"
).strip()
TAB_INTERACOES = "interacoes_jm"

# =================================================================================
# Modelos (mantidos como no app original do Janio)
# =================================================================================
MODELOS_OPENROUTER = {
    "üí¨ DeepSeek V3 ‚òÖ‚òÖ‚òÖ‚òÖ ($)": "deepseek/deepseek-chat-v3-0324",
    "üß† DeepSeek R1 0528 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$)": "deepseek/deepseek-r1-0528",
    "üß† DeepSeek R1T2 Chimera ‚òÖ‚òÖ‚òÖ‚òÖ (free)": "tngtech/deepseek-r1t2-chimera:free",
    "üß† GPT-4.1 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (1M ctx)": "openai/gpt-4.1",
    "‚ö° Google Gemini 2.5 Pro": "google/gemini-2.5-pro",
    "üëë WizardLM 8x22B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$$)": "microsoft/wizardlm-2-8x22b",
    "üëë Qwen 235B 2507 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (PAID)": "qwen/qwen3-235b-a22b-07-25",
    "üëë EVA Qwen2.5 72B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-qwen-2.5-72b",
    "üëë EVA Llama 3.33 70B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-llama-3.33-70b",
    "üé≠ Nous Hermes 2 Yi 34B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ": "nousresearch/nous-hermes-2-yi-34b",
    "üî• MythoMax 13B ‚òÖ‚òÖ‚òÖ‚òÜ ($)": "gryphe/mythomax-l2-13b",
    "üíã LLaMA3 Lumimaid 8B ‚òÖ‚òÖ‚òÜ ($)": "neversleep/llama-3-lumimaid-8b",
    "üåπ Midnight Rose 70B ‚òÖ‚òÖ‚òÖ‚òÜ": "sophosympatheia/midnight-rose-70b",
    "üå∂Ô∏è Noromaid 20B ‚òÖ‚òÖ‚òÜ": "neversleep/noromaid-20b",
    "üíÄ Mythalion 13B ‚òÖ‚òÖ‚òÜ": "pygmalionai/mythalion-13b",
    "üêâ Anubis 70B ‚òÖ‚òÖ‚òÜ": "thedrummer/anubis-70b-v1.1",
    "üßö Rocinante 12B ‚òÖ‚òÖ‚òÜ": "thedrummer/rocinante-12b",
    "üç∑ Magnum v2 72B ‚òÖ‚òÖ‚òÜ": "anthracite-org/magnum-v2-72b",
}

MODELOS_TOGETHER_UI = {
    "üß† Qwen3 Coder 480B (Together)": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "üß† Qwen2.5-VL (72B) Instruct (Together)": "Qwen/Qwen2.5-VL-72B-Instruct",
    "üëë Mixtral 8x7B v0.1 (Together)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "üëë Perplexity R1-1776 (Together)": "perplexity-ai/r1-1776",
    "üëë DeepSeek R1-0528 (Together)": "deepseek-ai/DeepSeek-R1",
}

MODELOS_HF = {
    "Llama 3.1 8B Instruct (HF)": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "Qwen2.5 7B Instruct (HF)": "Qwen/Qwen3-235B-A22B-Instruct-2507",
    "zai-org: GLM-4.5-Air (HF)": "zai-org/GLM-4.5-Air",
    "Mixtral 8x7B Instruct (HF)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "DeepSeek R1 (HF)": "deepseek-ai/DeepSeek-R1",
}

# LM Studio (padr√£o: base local; pode ser editado no sidebar)
DEFAULT_LMS_BASE_URL = (
    st.secrets.get("LMS_BASE_URL") or "http://127.0.0.1:1234/v1"
).rstrip("/")

@st.cache_data(ttl=15, show_spinner=False)
def _lms_models_dict(base_url: str) -> Dict[str, str]:
    try:
        r = requests.get(f"{base_url}/models", timeout=8)
        r.raise_for_status()
        data = r.json()
        return {f"{m['id']} (LM Studio)": m["id"] for m in data.get("data", []) if m.get("id")}
    except Exception:
        return {"<digite manualmente> (LM Studio)": "llama-3.1-8b-instruct"}

# =================================================================================
# Persona ‚Äî system m√≠nimo (como solicitado)
# =================================================================================
PERSONA_MARY = """nome: Mary Massariol
idade: 20 anos
Corpo: Cabelos negros e volumosos; olhos verdes; seios m√©dios e empinados; quadril largo; Barriga tonificada; Bumbum carnudo, redondo e firme; coxas grossas e torneadas;
Cursa engenharia civil na universidade federal do esp√≠rito santo-UFES; tem uma moto 500cc; mora com a m√£e, Joselina Massariol, mora no apartamento 202, na rua Beethoven, em laranjeiras;
√â inteligente; √© rom√¢ntica, √© virgem por op√ß√£o; gosta de praia; gosta de baladas com amigas; gosta de academia ao ar livre; gosta de flertar; n√£o tolera grosserias; n√£o tolera cantadas baratas;
adora dan√ßar; √© sensual; gosta de andar sem suti√£ sob a roupa; Silvia √© sua amiga de faculdade; Alexandra √© sua amiga de faculdade; Ricardo √© namorado ciumento e possessivo; quer encontrar o verdadeiro amor;
frequenta a Praia de Camburi; almo√ßo ou jantar no restaurante Partido Alto em Camburi; adora moqueca capixaba e camar√µes fritos; adora dan√ßar; frequenta baladas no Serra Bella Clube; Motel status √© onde os jovens transam;J√¢nio Donisete;
Orfeu √© o c√£o labrador de J√¢nio; J√¢nio mora em Camburi, no edif√≠cio Alianz, apartamento 2002, no 20¬∫ andar;


[REGRAS DE REALISMO]
‚Äî Sem onisci√™ncia: ningu√©m sabe fatos que n√£o foram ditos, vistos em cena ou lembrados do hist√≥rico (ex.: prato favorito, @ de redes, destino exato).
‚Äî Conhecimento plaus√≠vel s√≥ por: di√°logo expl√≠cito, pistas observ√°veis ou algo j√° estabelecido no chat.
‚Äî Sem atalhos milagrosos: nada de 'resolveu em 5 minutos', 'em 10 segundos', ou trocas instant√¢neas sob press√£o. Se houver pressa, use 'alguns instantes' e consequ√™ncia plaus√≠vel.
‚Äî Conflitos evoluem em degraus: tens√£o > rea√ß√£o > consequ√™ncia. N√£o salte para solu√ß√µes completas sem passos interm√©dios.
‚Äî Mary mant√©m limites e seguran√ßa: recusa grosseria, busca apoio das amigas/ambiente quando necess√°rio; evita risco f√≠sico.
‚Äî Consist√™ncia temporal: preserve o "rel√≥gio" da cena. Se for dia na praia e algu√©m chamar para uma balada noturna, trate como PROPOSTA para mais tarde; a mudan√ßa de tempo/lugar s√≥ ocorre ap√≥s aceita√ß√£o expl√≠cita e com marcador claro (ex.: "mais tarde", "ao anoitecer", "√† noite, no Serra Bella").
‚Äî Tempo/Lugar n√£o avan√ßam sozinhos: n√£o mude cen√°rio/tempo sem um gatilho (convite aceito, indica√ß√£o do usu√°rio ou marcador textual expl√≠cito).
‚Äî Convite ‚â† presen√ßa: convites (ex.: Partido Alto/Serra Bella) soam como sugest√£o; s√≥ vire encontro ap√≥s aceita√ß√£o e transi√ß√£o plaus√≠vel.
‚Äî Instagram/contato exigem gesto plaus√≠vel (troca combinada, QR, anota√ß√£o). Evite 'digitou @ em 1s' em p√∫blico com amea√ßa pr√≥xima.
‚Äî Evite adjetivos grandiloquentes repetidos; privilegie a√ß√µes simples e coerentes.
‚Äî Auto-checagem: antes de"""


# =================================================================================
# Conector Google Sheets (apenas interacoes_jm)
# =================================================================================

def _load_google_creds_dict() -> dict:
    raw = st.secrets.get("GOOGLE_CREDS_JSON")
    if raw is None:
        raise RuntimeError("Faltando GOOGLE_CREDS_JSON no secrets.")
    creds = json.loads(raw) if isinstance(raw, str) else dict(raw)
    if isinstance(creds.get("private_key"), str):
        creds["private_key"] = creds["private_key"].replace("\\n", "\n")
    return creds

@st.cache_resource(show_spinner=False)
def _open_ws_interacoes():
    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/drive",
    ]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(_load_google_creds_dict(), scope)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(PLANILHA_ID)
    return sh.worksheet(TAB_INTERACOES)

try:
    WS_INTERACOES = _open_ws_interacoes()
except Exception as e:
    WS_INTERACOES = None
    st.error(f"N√£o foi poss√≠vel abrir a aba '{TAB_INTERACOES}'. Verifique permiss√µes/ID. Detalhe: {e}")


def salvar_interacao(ts: str, session_id: str, provider: str, model: str, role: str, content: str):
    if not WS_INTERACOES:
        return
    try:
        WS_INTERACOES.append_row(
            [ts, session_id, provider, model, role, content], value_input_option="USER_ENTERED"
        )
    except (APIError, GSpreadException) as e:
        st.warning(f"Falha ao salvar intera√ß√£o: {e}")

# =================================================================================
# Carregamento de hist√≥rico persistente (√∫ltimas intera√ß√µes)
# =================================================================================

from typing import Tuple


def carregar_ultimas_interacoes(n_min: int = 5) -> list[dict]:
    """Carrega ao menos n_min intera√ß√µes da √∫ltima sess√£o registrada na aba interacoes_jm.
    Retorna uma lista de mensagens no formato [{"role": "user|assistant", "content": str}, ...].
    Limita implicitamente a 30 ao aplicar na tela."""
    if not WS_INTERACOES:
        return []
    try:
        valores = WS_INTERACOES.get_all_values()  # [[timestamp, session_id, provider, model, role, content], ...]
        if not valores or len(valores) < 2:
            return []
        linhas = [r for r in valores[1:] if len(r) >= 6]
        if not linhas:
            return []
        # Usa a √∫ltima session_id registrada na planilha
        last_sid = linhas[-1][1].strip()
        sess = [r for r in linhas if r[1].strip() == last_sid and r[4] in ("user", "assistant")]
        if not sess:
            return []
        recortes = sess[-max(n_min, 1):]
        chat = [{"role": r[4].strip(), "content": (r[5] or "").strip()} for r in recortes]
        return chat
    except Exception:
        return []


# =================================================================================
# Helpers ‚Äî mensagens m√≠nimas
# =================================================================================

def build_minimal_messages(history: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """Retorna apenas: system(persona) + hist√≥rico bruto user/assistant."""
    return [{"role": "system", "content": PERSONA_MARY}] + history


# =================================================================================
# Chamadas por provedor ‚Äî sem par√¢metros extras
# =================================================================================

def call_openrouter(model: str, messages: List[Dict[str, str]]) -> str:
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {st.secrets.get('OPENROUTER_API_KEY', '')}",
        "Content-Type": "application/json",
        "HTTP-Referer": st.secrets.get("APP_URL", ""),
        "X-Title": st.secrets.get("APP_TITLE", "Narrador JM"),
    }
    payload = {"model": model, "messages": messages}
    r = requests.post(url, headers=headers, json=payload, timeout=120)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"].strip()


def call_together(model: str, messages: List[Dict[str, str]]) -> str:
    url = "https://api.together.xyz/v1/chat/completions"
    headers = {"Authorization": f"Bearer {st.secrets.get('TOGETHER_API_KEY', '')}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": messages}
    r = requests.post(url, headers=headers, json=payload, timeout=120)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"].strip()


def call_lmstudio(base_url: str, model: str, messages: List[Dict[str, str]]) -> str:
    url = f"{base_url.rstrip('/')}/chat/completions"
    headers = {"Content-Type": "application/json"}
    payload = {"model": model, "messages": messages}
    r = requests.post(url, headers=headers, json=payload, timeout=120)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"].strip()


def call_huggingface(model: str, messages: List[Dict[str, str]]) -> str:
    # Converte chat ‚Üí prompt simples
    parts: List[str] = []
    for m in messages:
        role = m.get("role")
        content = (m.get("content") or "").strip()
        if not content:
            continue
        if role == "system":
            parts.append(f"[SYSTEM]\n{content}\n")
        elif role == "user":
            parts.append(f"[USER]\n{content}\n")
        else:
            parts.append(f"[ASSISTANT]\n{content}\n")
    prompt = "\n".join(parts) + "\n[ASSISTANT]\n"

    client = InferenceClient(api_key=st.secrets.get("HUGGINGFACE_API_KEY", ""))
    # Par√¢metros m√≠nimos (sem temperature/top_p etc.)
    return client.text_generation(model=model, prompt=prompt, max_new_tokens=512)

# =================================================================================
# Streaming helpers ‚Äî envia em peda√ßos para a UI
# =================================================================================

import time


def _sse_stream(url: str, headers: Dict[str, str], payload: Dict[str, Any]):
    """Itera eventos SSE de /chat/completions com stream=True e retorna trechos de texto."""
    payload_stream = dict(payload)
    payload_stream["stream"] = True
    with requests.post(url, headers=headers, json=payload_stream, stream=True, timeout=300) as r:
        r.raise_for_status()
        for raw in r.iter_lines(decode_unicode=False):
            if not raw:
                continue
            line = raw.decode("utf-8", errors="ignore").strip()
            if not line.startswith("data:"):
                continue
            data = line[5:].strip()
            if data == "[DONE]":
                break
            try:
                j = json.loads(data)
                ch = j.get("choices", [{}])[0]
                delta = (ch.get("delta") or {}).get("content") or (ch.get("message") or {}).get("content")
                if delta:
                    yield delta
            except Exception:
                continue


def stream_openrouter(model: str, messages: List[Dict[str, str]]):
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {st.secrets.get('OPENROUTER_API_KEY', '')}",
        "Content-Type": "application/json",
        "HTTP-Referer": st.secrets.get("APP_URL", ""),
        "X-Title": st.secrets.get("APP_TITLE", "Narrador JM"),
    }
    payload = {"model": model, "messages": messages}
    yield from _sse_stream(url, headers, payload)


def stream_together(model: str, messages: List[Dict[str, str]]):
    url = "https://api.together.xyz/v1/chat/completions"
    headers = {"Authorization": f"Bearer {st.secrets.get('TOGETHER_API_KEY', '')}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": messages}
    yield from _sse_stream(url, headers, payload)


def stream_lmstudio(base_url: str, model: str, messages: List[Dict[str, str]]):
    url = f"{base_url.rstrip('/')}/chat/completions"
    headers = {"Content-Type": "application/json"}
    payload = {"model": model, "messages": messages}
    yield from _sse_stream(url, headers, payload)


def _chunker(txt: str, n: int = 48):
    """Quebra texto em peda√ßos de ~n caracteres, respeitando espa√ßos quando poss√≠vel."""
    buf = []
    cur = 0
    while cur < len(txt):
        end = min(len(txt), cur + n)
        slice_ = txt[cur:end]
        if end < len(txt) and " " in slice_:
            last_sp = slice_.rfind(" ")
            if last_sp > 0:
                end = cur + last_sp + 1
                slice_ = txt[cur:end]
        buf.append(slice_)
        cur = end
    return buf


def stream_huggingface(model: str, messages: List[Dict[str, str]]):
    """HF Inference API nem sempre fornece SSE; simulamos streaming dividindo o texto."""
    full = call_huggingface(model, messages)
    for piece in _chunker(full, 48):
        yield piece
        time.sleep(0.02)  # leve suaviza√ß√£o visual


# =================================================================================
# UI
# =================================================================================
if "session_id" not in st.session_state:
    st.session_state.session_id = datetime.now().strftime("%Y%m%d-%H%M%S")
if "chat" not in st.session_state:
    st.session_state.chat: List[Dict[str, str]] = []
    # üîÅ Carrega as 5 √∫ltimas intera√ß√µes salvas da sess√£o anterior (se houver)
    try:
        _loaded = carregar_ultimas_interacoes(5)
        if _loaded:
            st.session_state.chat = _loaded[-30:]
    except Exception:
        pass

st.title("Narrador JM ‚Äî Clean Messages üé¨")

with st.sidebar:
    prov = st.radio("üåê Provedor", ["OpenRouter", "Together", "LM Studio", "Hugging Face"], index=0)

    if prov == "OpenRouter":
        modelo = st.selectbox("Modelo (OpenRouter)", list(MODELOS_OPENROUTER.keys()), index=0)
        model_id = MODELOS_OPENROUTER[modelo]
    elif prov == "Together":
        modelo = st.selectbox("Modelo (Together)", list(MODELOS_TOGETHER_UI.keys()), index=0)
        model_id = MODELOS_TOGETHER_UI[modelo]
    elif prov == "Hugging Face":
        modelo = st.selectbox("Modelo (HF)", list(MODELOS_HF.keys()), index=0)
        model_id = MODELOS_HF[modelo]
    else:
        base = st.text_input("LM Studio base URL", value=DEFAULT_LMS_BASE_URL)
        st.session_state.setdefault("lms_base_url", base)
        st.session_state.lms_base_url = base or DEFAULT_LMS_BASE_URL
        lms_models = _lms_models_dict(st.session_state.lms_base_url)
        modelo = st.selectbox("Modelo (LM Studio)", list(lms_models.keys()), index=0)
        model_id = lms_models[modelo]

    if st.button("üóëÔ∏è Resetar chat"):
        st.session_state.chat.clear()
        st.rerun()

# Render hist√≥rico
for m in st.session_state.chat:
    with st.chat_message(m["role"]).container():
        st.markdown(m["content"])  # sem filtros extras

# Entrada
if user_msg := st.chat_input("Fale com a Mary..."):
    ts = datetime.now().isoformat(sep=" ", timespec="seconds")
    st.session_state.chat.append({"role": "user", "content": user_msg})
    # Mant√©m apenas as √∫ltimas 30 intera√ß√µes na tela
    if len(st.session_state.chat) > 30:
        st.session_state.chat = st.session_state.chat[-30:]
    salvar_interacao(ts, st.session_state.session_id, prov, model_id, "user", user_msg)

    messages = build_minimal_messages(st.session_state.chat)

    with st.chat_message("assistant"):
        ph = st.empty()
        answer = ""
        try:
            if prov == "OpenRouter":
                gen = stream_openrouter(model_id, messages)
            elif prov == "Together":
                gen = stream_together(model_id, messages)
            elif prov == "Hugging Face":
                gen = stream_huggingface(model_id, messages)
            else:
                gen = stream_lmstudio(st.session_state.lms_base_url, model_id, messages)

            for delta in gen:
                answer += delta
                ph.markdown(answer + "‚ñå")
        except Exception as e:
            answer = f"[Erro ao chamar o modelo: {e}]"
            ph.markdown(answer)

    st.session_state.chat.append({"role": "assistant", "content": answer})
    # Mant√©m apenas as √∫ltimas 30 intera√ß√µes na tela
    if len(st.session_state.chat) > 30:
        st.session_state.chat = st.session_state.chat[-30:]
    ts2 = datetime.now().isoformat(sep=" ", timespec="seconds")
    salvar_interacao(ts2, st.session_state.session_id, prov, model_id, "assistant", answer)
    st.rerun()



