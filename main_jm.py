# ============================================================
# Narrador JM ‚Äî Variante ‚ÄúSomente FASE do romance‚Äù
# ============================================================

import os
import re
import json
import time
import random
from datetime import datetime
from typing import List, Tuple, Dict, Any
import streamlit as st
import requests
import gspread
import numpy as np
from gspread.exceptions import APIError
from oauth2client.service_account import ServiceAccountCredentials
from huggingface_hub import InferenceClient  # <= ADICIONE ESTA LINHA
from gspread.exceptions import APIError, GSpreadException


# =========================
# CONFIG B√ÅSICA DO APP
# =========================

# ATEN√á√ÉO: este modo ignora ‚ÄúMomento atual‚Äù. S√≥ a FASE manda.
ONLY_FASE_MODE = True

# --- LM Studio (via Cloudflare Tunnel) ---
LMS_BASE_URL = "https://context-frankfurt-environment-virtue.trycloudflare.com/v1"


PLANILHA_ID_PADRAO = st.secrets.get("SPREADSHEET_ID", "").strip() or "1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM"
TAB_INTERACOES = "interacoes_jm"
TAB_PERFIL = "perfil_jm"
TAB_MEMORIAS = "memoria_jm"
TAB_ML = "memoria_longa_jm"
TAB_TEMPLATES = "templates_jm"
TAB_FALAS_MARY = "falas_mary_jm"   # opcional (coluna: fala)

# Modelos (pode expandir depois)
MODELOS_OPENROUTER = {
    "üí¨ DeepSeek V3 ‚òÖ‚òÖ‚òÖ‚òÖ ($)": "deepseek/deepseek-chat-v3-0324",
    "üß† DeepSeek R1 0528 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$)": "deepseek/deepseek-r1-0528",
    "üß† DeepSeek R1T2 Chimera ‚òÖ‚òÖ‚òÖ‚òÖ (free)": "tngtech/deepseek-r1t2-chimera:free",
    "üß† GPT-4.1 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (1M ctx)": "openai/gpt-4.1",
    "‚ö° Google Gemini 2.5 Pro": "google/gemini-2.5-pro",
    "üëë WizardLM 8x22B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ($$$)": "microsoft/wizardlm-2-8x22b",
    "üëë Qwen 235B 2507 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (PAID)": "qwen/qwen3-235b-a22b-07-25",
    "üëë EVA Qwen2.5 72B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-qwen-2.5-72b",
    "üëë EVA Llama 3.33 70B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (RP Pro)": "eva-unit-01/eva-llama-3.33-70b",
    "üé≠ Nous Hermes 2 Yi 34B ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ": "nousresearch/nous-hermes-2-yi-34b",
    "üî• MythoMax 13B ‚òÖ‚òÖ‚òÖ‚òÜ ($)": "gryphe/mythomax-l2-13b",
    "üíã LLaMA3 Lumimaid 8B ‚òÖ‚òÖ‚òÜ ($)": "neversleep/llama-3-lumimaid-8b",
    "üåπ Midnight Rose 70B ‚òÖ‚òÖ‚òÖ‚òÜ": "sophosympatheia/midnight-rose-70b",
    "üå∂Ô∏è Noromaid 20B ‚òÖ‚òÖ‚òÜ": "neversleep/noromaid-20b",
    "üíÄ Mythalion 13B ‚òÖ‚òÖ‚òÜ": "pygmalionai/mythalion-13b",
    "üêâ Anubis 70B ‚òÖ‚òÖ‚òÜ": "thedrummer/anubis-70b-v1.1",
    "üßö Rocinante 12B ‚òÖ‚òÖ‚òÜ": "thedrummer/rocinante-12b",
    "üç∑ Magnum v2 72B ‚òÖ‚òÖ‚òÜ": "anthracite-org/magnum-v2-72b",
}

MODELOS_TOGETHER_UI = {
    "üß† Qwen3 Coder 480B (Together)": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "üß† Qwen2.5-VL (72B) Instruct (Together)": "Qwen/Qwen2.5-VL-72B-Instruct",
    "üëë Mixtral 8x7B v0.1 (Together)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "üëë Perplexity R1-1776 (Together)": "perplexity-ai/r1-1776",
    "üëë DeepSeek R1-0528 (Together)": "deepseek-ai/DeepSeek-R1",
}

MODELOS_HF = {
    "Llama 3.1 8B Instruct (HF)": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "Qwen2.5 7B Instruct (HF)":   "Qwen/Qwen3-235B-A22B-Instruct-2507",
    "zai-org: LM-4.5-Air (HF)":   "zai-org/GLM-4.5-Air",
    "Mixtral 8x7B Instruct (HF)": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "DeepSeek R1 (HF)":           "deepseek-ai/DeepSeek-R1",
}


@st.cache_data(ttl=15, show_spinner=False)
def _lms_models_dict() -> dict[str, str]:
    """
    L√™ /v1/models do LM Studio e devolve um dict {"<r√≥tulo (LM Studio)>": "<id>"}
    """
    try:
        import requests
        base = (st.session_state.get("lms_base_url") or LMS_BASE_URL or "").rstrip("/")
        url = base + "/models"
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        data = r.json()
        return {f"{m['id']} (LM Studio)": m["id"] for m in data.get("data", []) if m.get("id")}
    except Exception:
        # Fallback: ainda mostra uma op√ß√£o manual
        return {"<digite manualmente> (LM Studio)": "llama-3.1-8b-instruct"}

# === Roleplay: for√ßa par√°grafos e falas em linhas separadas ===

MAX_SENT_PER_PARA = 2
MAX_CHARS_PER_PARA = 240

_DASHES = re.compile(r"(?:\s*--\s*|\s*‚Äì\s*|\s*‚Äî\s*)")    # normaliza -- / ‚Äì / ‚Äî
_SENT_END = re.compile(r"[.!?‚Ä¶](?=\s|$)")                # fim de frase
_UPPER_OR_DASH = re.compile(r"([.!?‚Ä¶])\s+(?=(?:‚Äî|[A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√Ä√á0-9]))")

def roleplay_paragraphizer(t: str) -> str:
    if not t:
        return ""
    def break_long_paragraphs(txt):
        # Divide por frase (ponto, interroga√ß√£o, exclama√ß√£o), removendo espa√ßos extras
        frases = re.split(r'([.!?])\s*', txt)
        blocos = []
        cur = ''
        for i in range(0, len(frases)-1, 2):
            frase = frases[i].strip()
            pont = frases[i+1]
            if cur:
                cur += ' ' + frase + pont
                blocos.append(cur.strip())
                cur = ''
            else:
                cur = frase + pont
                blocos.append(cur.strip())
                cur = ''
        if cur:
            blocos.append(cur.strip())
        # Junta por quebra de linha simples
        return '\n'.join([b for b in blocos if b])
    
    # No final do seu p√≥s-processamento:
    # visible_txt = break_long_paragraphs(visible_txt)  # (mova essa linha para o contexto correto se necess√°rio)

    # 1) Normaliza travess√£o e for√ßa quebra antes de qualquer fala
    t = _DASHES.sub("\n‚Äî ", t)

    # 2) Quebra ap√≥s ponto/exclama√ß√£o/interroga√ß√£o quando vier outra frase/fala
    t = _UPPER_OR_DASH.sub(r"\1\n", t)

    # 3) Limpa espa√ßos
    t = re.sub(r"[ \t]+", " ", t)
    linhas = [ln.strip() for ln in t.splitlines() if ln.strip()]

    # 4) Agrupa narrativa (m√°x 2 frases ou 240 chars); falas isoladas
    out, buf = [], []
    sent = chars = 0

    for ln in linhas:
        if ln.startswith("‚Äî"):
            if buf:
                out.append(" ".join(buf).strip())
                out.append("")  # linha em branco entre par√°grafos
                buf, sent, chars = [], 0, 0
            out.append(ln)      # fala fica sozinha
        else:
            buf.append(ln)
            sent += len(_SENT_END.findall(ln))
            chars += len(ln)
            if sent >= MAX_SENT_PER_PARA or chars >= MAX_CHARS_PER_PARA:
                out.append(" ".join(buf).strip())
                out.append("")
                buf, sent, chars = [], 0, 0

    if buf:
        out.append(" ".join(buf).strip())

    # 5) Remove brancos duplicados e finais
    final = []
    for ln in out:
        if ln == "" and (not final or final[-1] == ""):
            continue
        final.append(ln)
    if final and final[-1] == "":
        final.pop()
    return "\n".join(final).strip()


def model_id_for_together(api_ui_model_id: str) -> str:
    key = (api_ui_model_id or "").strip()
    if "Qwen3-Coder-480B-A35B-Instruct-FP8" in key:
        return "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
    low = key.lower()
    if low.startswith("mistralai/mixtral-8x7b-instruct-v0.1"):
        return "mistralai/Mixtral-8x7B-Instruct-V0.1"
    return key or "mistralai/Mixtral-8x7B-Instruct-v0.1"

def api_config_for_provider(provider: str):
    if provider == "OpenRouter":
        return (
            "https://openrouter.ai/api/v1/chat/completions",
            st.secrets.get("OPENROUTER_API_KEY", ""),
            MODELOS_OPENROUTER,
        )
    elif provider == "Hugging Face":
        return (
            "HF_CLIENT",
            st.secrets.get("HUGGINGFACE_API_KEY", ""),
            MODELOS_HF,
        )
    elif provider == "LM Studio":  # üëà ADICIONADO AQUI
        return (
            LMS_BASE_URL.rstrip("/") + "/chat/completions",
            "",
            _lms_models_dict(),
        )
    else:
        return (
            "https://api.together.xyz/v1/chat/completions",
            st.secrets.get("TOGETHER_API_KEY", ""),
            MODELOS_TOGETHER_UI,
        )



# =========================
# BACKOFF + CACHES
# =========================

def _retry_429(callable_fn, *args, _retries=5, _base=0.6, **kwargs):
    for i in range(_retries):
        try:
            return callable_fn(*args, **kwargs)
        except (APIError, GSpreadException) as e:
            msg = (str(e) or "").lower()
            # trata como transit√≥rio: 429, quota/rate, timeout, backendError, deadline
            transient = any(k in msg for k in ["429", "quota", "rate", "timed out", "timeout", "backenderror", "deadline"])
            if transient:
                time.sleep((_base * (2 ** i)) + random.uniform(0, 0.25))
                continue
            # se n√£o parece transit√≥rio, re-lan√ßa
            raise
    # √∫ltima tentativa
    return callable_fn(*args, **kwargs)


@st.cache_data(ttl=45, show_spinner=False)
def _sheet_all_records_cached(sheet_name: str):
    ws = _ws(sheet_name, create_if_missing=False)
    if not ws:
        return []
    return _retry_429(ws.get_all_records)

# ========= helpers do Google Sheets =========

def _rows_to_records(vals):
    if not vals:
        return []
    header = vals[0] if vals else []
    body = vals[1:] if len(vals) > 1 else []
    # cabe√ßalho sint√©tico se vazio
    if not header or all(not (c or "").strip() for c in header):
        maxlen = max((len(r) for r in body), default=0)
        header = [f"col_{i+1}" for i in range(maxlen)]
    else:
        # dedup nomes vazios/duplicados
        seen, new = set(), []
        for i, h in enumerate(header):
            name = (h or f"col_{i+1}").strip() or f"col_{i+1}"
            base, j = name, 2
            while name in seen:
                name = f"{base}_{j}"
                j += 1
            seen.add(name)
            new.append(name)
        header = new
    out = []
    for r in body:
        row = list(r) + [""] * max(0, len(header) - len(r))
        row = row[:len(header)]
        out.append({header[i]: row[i] for i in range(len(header))})
    return out


@st.cache_data(ttl=45, show_spinner=False)
def _sheet_all_records_cached(sheet_name: str):
    ws = _ws(sheet_name, create_if_missing=False)
    if not ws:
        return []
    try:
        # caminho ‚Äúfeliz‚Äù (como j√° era)
        return _retry_429(ws.get_all_records)
    except GSpreadException:
        # fallback robusto sem derrubar o app
        vals = _retry_429(ws.get_all_values)
        return _rows_to_records(vals)
    except Exception as e:
        st.warning(f"Falha ao ler '{sheet_name}': {e}")
        return []


@st.cache_data(ttl=45, show_spinner=False)
def _sheet_all_values_cached(sheet_name: str):
    ws = _ws(sheet_name, create_if_missing=False)
    if not ws:
        return []
    try:
        return _retry_429(ws.get_all_values)
    except Exception as e:
        st.warning(f"Falha ao ler valores de '{sheet_name}': {e}")
        return []


def _invalidate_sheet_caches():
    try:
        _sheet_all_records_cached.clear()
        _sheet_all_values_cached.clear()
    except Exception:
        pass
# ========= helpers do Google Sheets =========

def _rows_to_records(vals):
    if not vals:
        return []
    header = vals[0] if vals else []
    body = vals[1:] if len(vals) > 1 else []
    # cabe√ßalho sint√©tico se vazio
    if not header or all(not (c or "").strip() for c in header):
        maxlen = max((len(r) for r in body), default=0)
        header = [f"col_{i+1}" for i in range(maxlen)]
    else:
        # dedup nomes vazios/duplicados
        seen, new = set(), []
        for i, h in enumerate(header):
            name = (h or f"col_{i+1}").strip() or f"col_{i+1}"
            base, j = name, 2
            while name in seen:
                name = f"{base}_{j}"
                j += 1
            seen.add(name)
            new.append(name)
        header = new
    out = []
    for r in body:
        row = list(r) + [""] * max(0, len(header) - len(r))
        row = row[:len(header)]
        out.append({header[i]: row[i] for i in range(len(header))})
    return out


@st.cache_data(ttl=45, show_spinner=False)
def _sheet_all_records_cached(sheet_name: str):
    ws = _ws(sheet_name, create_if_missing=False)
    if not ws:
        return []
    try:
        # caminho ‚Äúfeliz‚Äù (como j√° era)
        return _retry_429(ws.get_all_records)
    except GSpreadException:
        # fallback robusto sem derrubar o app
        vals = _retry_429(ws.get_all_values)
        return _rows_to_records(vals)
    except Exception as e:
        st.warning(f"Falha ao ler '{sheet_name}': {e}")
        return []


@st.cache_data(ttl=45, show_spinner=False)
def _sheet_all_values_cached(sheet_name: str):
    ws = _ws(sheet_name, create_if_missing=False)
    if not ws:
        return []
    try:
        return _retry_429(ws.get_all_values)
    except Exception as e:
        st.warning(f"Falha ao ler valores de '{sheet_name}': {e}")
        return []


def _invalidate_sheet_caches():
    try:
        _sheet_all_records_cached.clear()
        _sheet_all_values_cached.clear()
    except Exception:
        pass


# =========================
# GOOGLE SHEETS
# =========================

def conectar_planilha():
    try:
        creds_dict = json.loads(st.secrets["GOOGLE_CREDS_JSON"])
        if "private_key" in creds_dict:
            creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        spreadsheet_id = st.secrets.get("SPREADSHEET_ID", "").strip() or PLANILHA_ID_PADRAO
        return client.open_by_key(spreadsheet_id)
    except Exception as e:
        st.error(f"Erro ao conectar √† planilha: {e}")
        return None

planilha = conectar_planilha()

def _ws(name: str, create_if_missing: bool = True):
    if not planilha:
        return None
    try:
        return planilha.worksheet(name)
    except Exception:
        if not create_if_missing:
            return None
        try:
            ws = planilha.add_worksheet(title=name, rows=5000, cols=10)
            if name == TAB_INTERACOES:
                _retry_429(ws.append_row, ["timestamp", "role", "content"])
            elif name == TAB_PERFIL:
                _retry_429(ws.append_row, ["timestamp", "resumo"])
            elif name == TAB_MEMORIAS:
                _retry_429(ws.append_row, ["tipo", "conteudo", "timestamp"])
            elif name == TAB_ML:
                _retry_429(ws.append_row, ["texto", "embedding", "tags", "timestamp", "score"])
            elif name == TAB_TEMPLATES:
                _retry_429(ws.append_row, ["template", "etapa", "texto"])
            elif name == TAB_FALAS_MARY:
                _retry_429(ws.append_row, ["fala"])
            return ws
        except Exception:
            return None

# =========================
# UTILIDADES: MEM√ìRIAS / HIST√ìRICO
# =========================
from typing import Dict, List
from datetime import datetime

# --- Constantes de abas (ajuste se necess√°rio) ---
TAB_MEMORIAS   = "memoria_jm"       # cabe√ßalho: tipo | conteudo | timestamp
TAB_INTERACOES = "interacoes_jm"  # ou "interacoes_jm", conforme seu projeto
TAB_PERFIL     = "perfil_jm"        # aba que guarda 'resumo'

# =========================
# CONTEXTO FIXO DE CENA (tempo/lugar/figurino/t√≥pico)
# =========================

CTX_INICIAL = {
    "tempo": None,        # ex: "Domingo de manh√£"
    "lugar": None,        # ex: "em casa"
    "figurino": None,     # ex: "short jeans e regata branca"
    "topico": None,       # assunto resumido do turno
    "diretiva": None,     # comando bruto do usu√°rio
}

def extrair_diretriz_contexto(texto_usuario: str, ctx: dict | None = None) -> dict:
    import re

    # Inicializa
    ctx = dict(ctx) if ctx else dict(CTX_INICIAL)
    t = (texto_usuario or "").strip()
    ctx["diretiva"] = t

    # Extra√ß√£o padr√£o
    # Tempo (ex.: ‚Äúdomingo de manh√£‚Äù, ‚Äúhoje √† noite‚Äù)
    m_tempo = re.search(
        r"(?i)\b(hoje|amanh[√£a]|ontem|segunda|ter[c√ß]a|quarta|quinta|sexta|s[√°a]bado|domingo)(?:\s+de\s+(manh[√£a]|tarde|noite))?",
        t)
    if m_tempo:
        p1 = m_tempo.group(1).capitalize()
        p2 = f" de {m_tempo.group(2)}" if m_tempo.group(2) else ""
        ctx["tempo"] = f"{p1}{p2}"

    # Lugar curto (r√≥tulo)
    m_lugar = re.search(
        r"(?i)\b(em casa|no apartamento|na lanchonete|no shopping|na escola|no est[√∫u]dio|no quarto|no bar|na praia)\b", t)
    if m_lugar:
        ctx["lugar"] = m_lugar.group(1)

    # Figurino b√°sico (ex.: ‚Äúveste/ usando/ de ...‚Äù)
    m_fig = re.search(r"(?i)\b(veste|usando|de)\s+([a-z0-9\s\-√£√°√©√≠√≥√∫√ß]+)", t)
    if m_fig:
        ctx["figurino"] = m_fig.group(2).strip()

    # T√≥pico do turno (se houver ‚Äú:‚Äù, pega o p√≥s-dois-pontos; sen√£o, o texto todo resumido)
    m_top = re.search(r":\s*(.+)$", t)
    ctx["topico"] = (m_top.group(1).strip() if m_top else t)[:140]

    #--------------#
    # Persist√™ncia: herda o √∫ltimo valor "ctx_cena" se campo n√£o for atualizado
    for campo in ["tempo", "lugar", "figurino"]:
        if not ctx.get(campo):  # Se n√£o houve nova detec√ß√£o, mant√©m o anterior
            ultimo_ctx = st.session_state.get("ctx_cena", {})
            if ultimo_ctx and ultimo_ctx.get(campo):
                ctx[campo] = ultimo_ctx.get(campo)

    return ctx

def gerar_linha_abertura(ctx: dict) -> str:
    """
    Gera a linha de abertura padronizada para a cena, com base no contexto.
    Exemplo: "Domingo de manh√£. Mary, biqu√≠ni ajustado em suas curvas sensuais. Jacara√≠pe."
    """
    if not ctx:
        return ""
    tempo = (ctx.get("tempo") or "").strip().rstrip(".")
    figurino = (ctx.get("figurino") or "").strip().rstrip(".")
    lugar = (ctx.get("lugar") or "").strip().rstrip(".")

    pedacos = []
    if tempo:
        pedacos.append(tempo.capitalize())
    mary_part = "Mary"
    if figurino:
        mary_part += f", {figurino}"
    pedacos.append(mary_part)
    if lugar:
        pedacos.append(lugar)

    if not any(pedacos):
        return ""
    return ". ".join(pedacos) + "."




# ---------------------------------------------
# VIRGINDADE ‚Äî leitura memoria_jm + fallback (interacoes_jm) e infer√™ncia temporal
# ---------------------------------------------
from typing import Optional, Tuple, List
import re
from datetime import datetime

# Padr√µes (priorize "n√£o virgem" sobre "virgem")
PADROES_NAO_VIRGEM = [
    r"\b(n[a√£]o\s+sou\s+mais\s+virgem)\b",
    r"\b(deix(ei|ou)\s+de\s+ser\s+virgem)\b",
    r"\b(perd[ei]u?\s+a\s+virgindade)\b",
    r"\b(minha|sua|nossa)\s+primeira\s+vez\s+(foi|aconteceu|rolou)\b",
    r"\b(tivemos|tive)\s+(a\s+)?primeira\s+vez\b",
]
PADROES_VIRGEM = [
    r"\b(sou|era|continuo?|permane[c√ß]o)\s+virgem\b",
    r"\b(nunca\s+(tran(s|√ß)ei|fiz\s+sexo|tive\s+rela[c√ß][o√µ]es))\b",
    r"\b(virgindade)\b(?!\s*(perd[i√≠]|\bn[a√£]o\b))",
]

def _to_dt(ts: str) -> Optional[datetime]:
    """Converte timestamp em datetime. Aceita formatos comuns; retorna None se falhar."""
    ts = (ts or "").strip()
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%d/%m/%Y %H:%M:%S", "%d/%m/%Y %H:%M", "%d/%m/%Y"):
        try:
            return datetime.strptime(ts, fmt)
        except Exception:
            pass
    return None

def _ultimo_evento_virgindade_memoria(ate: Optional[datetime] = None) -> Optional[Tuple[bool, datetime, str]]:
    """
    Varre a aba memoria_jm (via carregar_memorias_brutas) nos tipos [mary] e [all].
    Retorna (estado_bool, ts, "memoria_jm") com o evento mais recente <= ate.
    """
    try:
        buckets = carregar_memorias_brutas()  # {'[tag]': [{'conteudo','timestamp'}, ...]}
    except Exception:
        buckets = {}
    candidatos: List[Tuple[bool, datetime]] = []
    ate = ate or datetime.now()

    for tag in ("[mary]", "[all]"):
        for item in buckets.get(tag, []) or []:
            txt = (item.get("conteudo") or "").strip()
            ts = _to_dt(item.get("timestamp"))
            if ts and ts > ate:
                continue
            low = txt.lower()
            # n√£o virgem tem prioridade
            if any(re.search(p, low, re.IGNORECASE) for p in PADROES_NAO_VIRGEM):
                candidatos.append((False, ts or datetime.min))
            elif any(re.search(p, low, re.IGNORECASE) for p in PADROES_VIRGEM):
                candidatos.append((True, ts or datetime.min))

    if not candidatos:
        return None

    # mais recente; em empate, False (n√£o virgem) vence
    candidatos.sort(key=lambda x: (x[1], 0 if x[0] is False else 1))
    estado, ts = candidatos[-1]
    return (estado, ts, "memoria_jm")

def _ultimo_evento_virgindade_interacoes(ate: Optional[datetime] = None) -> Optional[Tuple[bool, datetime, str]]:
    """
    Fallback: varre interacoes_jm (via carregar_interacoes) e tenta inferir.
    Retorna (estado_bool, ts, "interacoes_jm").
    """
    ate = ate or datetime.now()
    inter = carregar_interacoes(n=20)  # pega um hist√≥rico grande
    if not inter:
        return None

    candidatos: List[Tuple[bool, datetime]] = []
    for r in inter:
        ts = _to_dt(r.get("timestamp"))
        if ts and ts > ate:
            continue
        low = (r.get("content") or "").strip().lower()
        if any(re.search(p, low, re.IGNORECASE) for p in PADROES_NAO_VIRGEM):
            candidatos.append((False, ts or datetime.min))
        elif any(re.search(p, low, re.IGNORECASE) for p in PADROES_VIRGEM):
            candidatos.append((True, ts or datetime.min))

    if not candidatos:
        return None

    candidatos.sort(key=lambda x: (x[1], 0 if x[0] is False else 1))
    estado, ts = candidatos[-1]
    return (estado, ts, "interacoes_jm")

def estado_virgindade_ate(ate: Optional[datetime] = None) -> Optional[bool]:
    """
    P√∫blico p/ o prompt builder:
      True  -> virgem
      False -> n√£o virgem
      None  -> desconhecido (sem evid√™ncias)
    """
    ate = ate or datetime.now()
    res = _ultimo_evento_virgindade_memoria(ate)
    if res is None:
        res = _ultimo_evento_virgindade_interacoes(ate)
    return None if res is None else res[0]


def _normalize_tag(raw: str) -> str:
    t = (raw or "").strip().lower()
    if not t:
        return ""
    return t if t.startswith("[") else f"[{t}]"

def _parse_ts(s: str) -> str:
    s = (s or "").strip()
    try:
        datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
        return s
    except Exception:
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def carregar_memorias_brutas() -> Dict[str, List[dict]]:
    """L√™ a aba TAB_MEMORIAS e devolve um dicion√°rio {'[tag]': [{'conteudo':..., 'timestamp':...}, ...]}."""
    try:
        regs = _sheet_all_records_cached(TAB_MEMORIAS)
        buckets: Dict[str, List[dict]] = {}
        for r in regs:
            tag = _normalize_tag(r.get("tipo"))
            txt = (r.get("conteudo") or "").strip()
            ts  = (r.get("timestamp") or "").strip()
            if tag and txt:
                buckets.setdefault(tag, []).append({"conteudo": txt, "timestamp": ts})
        return buckets
    except Exception as e:
        st.warning(f"Erro ao carregar mem√≥rias: {e}")
        return {}

def persona_block_temporal(nome: str, buckets: dict, ate_ts: str, max_linhas: int = 8) -> str:
    """
    Monta um bloco textual (linhas) da persona 'nome' at√© o timestamp ate_ts (YYYY-MM-DD HH:MM:SS).
    Busca no buckets['[nome]'] e filtra por timestamp <= ate_ts.
    """
    tag = f"[{nome}]"
    linhas = []
    for d in buckets.get(tag, []) or []:
        if not isinstance(d, dict):
            continue
        c = (d.get("conteudo") or "").strip()
        ts = (d.get("timestamp") or "").strip()
        if not c:
            continue
        if ts and ate_ts and ts > ate_ts:
            continue
        linhas.append((ts, c))
    linhas.sort(key=lambda x: x[0])
    ult = [c for _, c in linhas][-max_linhas:]
    if not ult:
        return ""
    titulo = "J√¢nio" if nome in ("janio", "j√¢nio") else "Mary" if nome == "mary" else nome.capitalize()
    return f"{titulo}:\n- " + "\n- ".join(ult)

def carregar_resumo_salvo() -> str:
    """L√™ a √∫ltima linha com coluna 'resumo' n√£o vazia em TAB_PERFIL."""
    try:
        registros = _sheet_all_records_cached(TAB_PERFIL)
        for r in reversed(registros):
            txt = (r.get("resumo") or "").strip()
            if txt:
                return txt
        return ""
    except Exception as e:
        st.warning(f"Erro ao carregar resumo salvo: {e}")
        return ""

def salvar_resumo(resumo: str):
    """Salva uma nova linha [timestamp, resumo] na aba TAB_PERFIL."""
    try:
        aba = _ws(TAB_PERFIL)
        if not aba:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        _retry_429(aba.append_row, [timestamp, resumo], value_input_option="RAW")
        _invalidate_sheet_caches()
    except Exception as e:
        st.error(f"Erro ao salvar resumo: {e}")

def carregar_interacoes(n: int = 25):
    """
    Carrega as intera√ß√µes da aba TAB_INTERACOES com cache em sess√£o.
    Retorna as √∫ltimas n intera√ß√µes normalizadas: [{timestamp, role, content}, ...]
    """
    cache = st.session_state.get("_cache_interacoes", None)
    if cache is None:
        regs = _sheet_all_records_cached(TAB_INTERACOES)
        # normaliza
        norm = []
        for r in regs:
            norm.append({
                "timestamp": (r.get("timestamp") or "").strip(),
                "role": (r.get("role") or "user").strip(),
                "content": (r.get("content") or "").strip(),
            })
        st.session_state["_cache_interacoes"] = norm
        cache = norm
    return cache[-n:] if len(cache) > n else cache

def salvar_interacao(role: str, content: str):
    """Acrescenta uma linha em TAB_INTERACOES e atualiza o cache em sess√£o."""
    if not planilha:
        return
    try:
        aba = _ws(TAB_INTERACOES)
        if not aba:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        row = [timestamp, f"{role or ''}".strip(), f"{content or ''}".strip()]
        _retry_429(aba.append_row, row, value_input_option="RAW")
        lst = st.session_state.get("_cache_interacoes")
        if isinstance(lst, list):
            lst.append({"timestamp": timestamp, "role": row[1], "content": row[2]})
        else:
            st.session_state["_cache_interacoes"] = [{"timestamp": timestamp, "role": row[1], "content": row[2]}]
        _invalidate_sheet_caches()
    except Exception as e:
        st.error(f"Erro ao salvar intera√ß√£o: {e}")

# =========================
# MEM√ìRIA LONGA (opcional simples)
# =========================

def memoria_longa_listar_registros():
    try:
        return _sheet_all_records_cached(TAB_ML)
    except Exception:
        return []

def memoria_longa_salvar(texto: str, tags: str = "") -> bool:
    aba = _ws(TAB_ML, create_if_missing=False)
    if not aba:
        return False
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    linha = [texto.strip(), "", (tags or "").strip(), ts, 1.0]
    try:
        _retry_429(aba.append_row, linha, value_input_option="RAW")
        _invalidate_sheet_caches()
        return True
    except Exception:
        return False

def memoria_longa_buscar_topk(query_text: str, k: int = 3, limiar: float = 0.78, ate_ts=None):
    try:
        dados = _sheet_all_records_cached(TAB_ML)
    except Exception:
        return []
    def _tok(s): return set(re.findall(r"[a-z√†-√∫0-9]+", (s or "").lower()))
    s2 = _tok(query_text)
    cands = []
    for row in dados:
        texto = (row.get("texto") or "").strip()
        if not texto:
            continue
        row_ts = (row.get("timestamp") or "").strip()
        if ate_ts and row_ts and row_ts > ate_ts:
            continue
        s1 = _tok(texto)
        sim = len(s1 & s2) / max(1, len(s1 | s2))
        try:
            score = float(row.get("score", 1.0) or 1.0)
        except Exception:
            score = 1.0
        if sim >= limiar:
            rr = 0.7*sim + 0.3*score
            cands.append((texto, score, sim, rr))
    cands.sort(key=lambda x: x[3], reverse=True)
    return cands[:k]

def memoria_longa_reforcar(textos_usados: list):
    aba = _ws(TAB_ML, create_if_missing=False)
    if not aba or not textos_usados:
        return
    try:
        dados = _sheet_all_values_cached(TAB_ML)
        if not dados or len(dados) < 2:
            return
        headers = dados[0]
        idx_texto = headers.index("texto")
        idx_score = headers.index("score")
        for i, linha in enumerate(dados[1:], start=2):
            if len(linha) <= max(idx_texto, idx_score):
                continue
            t = (linha[idx_texto] or "").strip()
            if t in textos_usados:
                try:
                    sc = float(linha[idx_score] or 1.0)
                except Exception:
                    sc = 1.0
                sc = min(sc + 0.2, 2.0)
                _retry_429(aba.update_cell, i, idx_score + 1, sc)
        _invalidate_sheet_caches()
    except Exception:
        pass

# =========================
# REGRAS DE FASE (APENAS FASE)
# =========================

FASES_ROMANCE: Dict[int, Dict[str, str]] = {
    0: {"nome": "Estranhos",
        "permitidos": "olhares; near-miss (mesmo caf√©/rua/√¥nibus); detalhe do ambiente",
        "proibidos": "troca de nomes; toques; conversa pessoal; beijo"},
    1: {"nome": "Percep√ß√£o",
        "permitidos": "cumprimento neutro; pergunta impessoal curta; beijo no rosto",
        "proibidos": "beijo na boca; confid√™ncias"},
    2: {"nome": "Conhecidos",
        "permitidos": "troca de nomes; pequena ajuda; 1 pergunta pessoal leve; beijo suave na boca",
        "proibidos": "toque prolongado; encontro a s√≥s planejado"},
    3: {"nome": "Romance",
        "permitidos": "conversa 10‚Äì20 min; caminhar juntos; trocar contatos; beijos intensos (sem car√≠cias √≠ntimas)",
        "proibidos": "car√≠cias √≠ntimas; tirar roupas"},
    4: {"nome": "Namoro",
        "permitidos": "beijos intensos; car√≠cias √≠ntimas; **sem cl√≠max** at√© usu√°rio liberar",
        "proibidos": "sexo expl√≠cito sem consentimento claro"},
    5: {"nome": "Compromisso / Encontro definitivo",
        "permitidos": "beijos intensos; car√≠cias √≠ntimas; sexo com consentimento; **cl√≠max somente se usu√°rio liberar**",
        "proibidos": ""},
}
FLAG_FASE_TXT_PREFIX = "FLAG: mj_fase="

def _fase_label(n: int) -> str:
    d = FASES_ROMANCE.get(int(n), FASES_ROMANCE[0])
    return f"{int(n)} ‚Äî {d['nome']}"

def mj_set_fase(n: int, persist: bool=False):
    n = max(0, min(max(FASES_ROMANCE.keys()), int(n)))
    st.session_state.mj_fase = n
    if persist:
        try:
            memoria_longa_salvar(f"{FLAG_FASE_TXT_PREFIX}{n}", tags="[flag]")
        except Exception:
            pass

def mj_carregar_fase_inicial() -> int:
    if "mj_fase" in st.session_state:
        return int(st.session_state.mj_fase)
    try:
        recs = memoria_longa_listar_registros()
        for r in reversed(recs):
            t = (r.get("texto") or "").strip()
            if t.startswith(FLAG_FASE_TXT_PREFIX):
                n = int(t.split("=")[1])
                st.session_state.mj_fase = n
                return n
    except Exception:
        pass
    st.session_state.mj_fase = 0
    return 0

# =========================
# FALAS DA MARY (BRANDAS) + PLANILHA
# =========================

FALAS_EXPLICITAS_MARY = [
    # Brandas por padr√£o ‚Äî troque depois se quiser
    "Vem mais perto...sem pressa.",
    "Assim est√° bom‚Ä¶ continua...assim... desse jeito.",
    "Eu quero sentir voc√™...devagar.",
    "Fica comigo....s√≥ mais um pouco.",
]

def carregar_falas_mary() -> List[str]:
    try:
        ws = _ws(TAB_FALAS_MARY, create_if_missing=False)
        if not ws:
            return []
        rows = _sheet_all_records_cached(TAB_FALAS_MARY)
        falas = []
        for r in rows:
            fala = (r.get("fala") or "").strip()
            if fala:
                falas.append(fala)
        return falas or []
    except Exception:
        return []

# =========================
# AJUSTES DE TOM / SINTONIA
# =========================
import re, random

def gerar_mary_sensorial(level: int = 2, n: int = 2, hair_on: bool = True, sintonia: bool = True) -> str:
    if level <= 0 or n <= 0:
        return ""

    # Frases SEM cen√°rio/clima/met√°foras
    base_leve = [
        "Os cabelos de Mary ‚Äî negros, volumosos, levemente ondulados ‚Äî acompanham o passo.",
        "O olhar de Mary √© firme e sereno; a respira√ß√£o, tranquila.",
        "H√° calor discreto no perfume que ela deixa no ar.",
        "O sorriso surge pequeno, sincero e atento.",
    ]
    base_marcado = [
        "O tecido ro√ßa levemente nas pernas; o passo √© seguro, cadenciado.",
        "Os ombros relaxam quando ela encontra o olhar de J√¢nio.",
        "A pele arrepia sutil ao menor toque.",
        "O olhar de Mary segura o dele por um instante a mais.",
    ]
    base_ousado = [
        "O ritmo do corpo de Mary √© deliberado; chama sem exigir.",
        "O perfume dela exala convidando a aproxima√ß√£o.",
        "Os l√°bios entreabertos, esperando o momento certo.",
        "O olhar pousa e permanece, sugerindo contato.",
    ]

    if level == 1:
        pool = list(base_leve)
    elif level == 2:
        pool = list(base_leve) + list(base_marcado)
    else:
        pool = list(base_leve) + list(base_marcado) + list(base_ousado)

    # Filtro extra de seguran√ßa contra ‚Äúpaisagem/clima‚Äù
    termos_banidos = re.compile(
        r"\b(c[√©e]u|mar|onda?s?|vento|brisa|chuva|nublado|luar|horizonte|pier|paisage?m|cen[√°a]rio|amanhecer|entardecer|p[√¥o]r do sol)\b",
        re.IGNORECASE,
    )
    pool = [f for f in pool if not termos_banidos.search(f)]

    if sintonia:
        filtros = [r"\bexigir\b"]
        def _ok(fr): return not any(re.search(p, fr, re.I) for p in filtros)
        pool = [f for f in pool if _ok(f)]
        pool.extend([
            "A respira√ß√£o de Mary busca o mesmo compasso de J√¢nio.",
            "Ela desacelera e deixa o momento guiar.",
            "O toque come√ßa suave, sem precipita√ß√£o.",
        ])

    n_eff = max(1, min(n, len(pool)))
    frases = random.sample(pool, k=n_eff) if pool else []

    if hair_on:
        hair_line = "Os cabelos negros, volumosos e levemente ondulados moldam o rosto quando ela vira de leve."
        if hair_line not in frases:
            frases.insert(0, hair_line)
            if len(frases) > n_eff:
                frases = frases[:n_eff]
    return " ".join(frases)

def _last_user_text(hist):
    if not hist:
        return ""
    for r in reversed(hist):
        if str(r.get("role","")).lower() == "user":
            return r.get("content","")
    return ""

def _deduzir_ancora(texto: str) -> dict:
    t = (texto or "").lower()
    if "motel" in t or "su√≠te" in t or "suite" in t:
        return {"local": "Motel ‚Äî Su√≠te", "hora": "noite"}
    if "quarto" in t:
        return {"local": "Quarto", "hora": "noite"}
    if "praia" in t:
        return {"local": "Praia", "hora": "fim de tarde"}
    if "bar" in t or "pub" in t:
        return {"local": "Bar", "hora": "noite"}
    if "biblioteca" in t:
        return {"local": "Biblioteca", "hora": "tarde"}
    return {}

def inserir_regras_mary_e_janio(prompt_base: str) -> str:
    calor = int(st.session_state.get("nsfw_max_level", 0))
    regras = f"""
‚öñÔ∏è Regras de coer√™ncia:
- Narre em terceira pessoa; n√£o se dirija ao leitor como "voc√™".
- Consentimento claro antes de qualquer gesto significativo.
- Mary prefere ritmo calmo, sintonizado com o parceiro (modo harm√¥nico ativo).
- Linguagem sensual proporcional ao n√≠vel de calor ({calor}).
- Proibido natureza/ambiente/clima/met√°foras (c√©u, mar, vento, ondas, luar, paisagem etc.).
- Sem ‚Äúfade to black‚Äù: a progress√£o √© mostrada, mas sem pornografia expl√≠cita.
""".strip()
    return prompt_base + "\n" + regras

# =========================
# VIRGINDADE (opcional)
# =========================

def detectar_virgindade_mary(memos: dict, ate_ts: str) -> bool:
    for d in memos.get("[mary]", []):
        c = (d.get("conteudo") or "").lower()
        ts = (d.get("timestamp") or "")
        if ts and ate_ts and ts > ate_ts:
            continue
        if "sou virgem" in c or "virgindade" in c or "nunca fiz" in c:
            return True
    return False

def montar_bloco_virgindade(ativar: bool) -> str:
    if not ativar:
        return ""
    return (
        "### Nota de virgindade (prioridade)\n"
        "- Mary valoriza sua primeira vez. O ritmo √© cuidadoso, com comunica√ß√£o clara.\n"
        "- Evite pressa; foco em respeito, confian√ßa e conforto.\n"
    )

def prompt_da_cena(ctx: dict | None = None, modo_finalizacao: str = "ponte") -> str:
    ctx = ctx or {}
    tempo    = (ctx.get("tempo") or "").strip().rstrip(".")
    lugar    = (ctx.get("lugar") or "").strip().rstrip(".")
    figurino = (ctx.get("figurino") or "").strip().rstrip(".")

    # Monta a 1¬™ linha (permitida pela exce√ß√£o)
    pedacos = []
    if tempo: pedacos.append(tempo.capitalize())
    pedacos.append(f"Mary{(', ' + figurino) if figurino else ''}".strip())
    if lugar: pedacos.append(lugar)
    primeira_linha = ". ".join([p for p in pedacos if p]) + "."

    # Regras de fechamento
    modo = (modo_finalizacao or "ponte").lower()
    if modo == "eu":
        regra_fim = "- Feche com 1 frase curta em 1¬™ pessoa (Mary), conectando o pr√≥ximo gesto."
    elif modo == "seco":
        regra_fim = "- Termine sem gancho, frase final objetiva."
    else:  # ponte
        regra_fim = "- Feche com micro-a√ß√£o que deixe gancho natural para continua√ß√£o, sem concluir a cena."

    return (
        "### Diretrizes de abertura e fechamento\n"
        "- Comece com UMA linha: Tempo. Mary[, figurino]. [Lugar]. (somente se houver dados; caso contr√°rio, pule)\n"
        "- Em seguida, entre direto em a√ß√£o e di√°logo, focada na DIRETIVA do usu√°rio.\n"
        f"{regra_fim}\n"
        f"ABERTURA_SUGERIDA: {primeira_linha if primeira_linha != '.' else ''}\n"
    )

# =========================
# PROMPT BUILDER (APENAS FASE) ‚Äî compat√≠vel com Modo Mary
# =========================

def construir_prompt_com_narrador() -> str:
    BLOCO_ROLEPLAY = """
OBRIGAT√ìRIO ‚Äî FORMATO ESTRUTURADO DE ROLEPLAY

- Cada fala ("‚Äî") deve come√ßar linha nova isolada, SEMPRE seguida ou precedida de bloco de a√ß√£o/descri√ß√£o corporal.
- Nunca una mais de 2 frases no mesmo par√°grafo narrativo; em narra√ß√£o, troque de linha a cada a√ß√£o/rea√ß√£o f√≠sica importante.
- O texto final sempre ter√° par√°grafos curtos: bloco de a√ß√£o (m√°x. 2 frases), bloco de fala, bloco de rea√ß√£o, bloco de fala. Nunca prosa longa.
- N√ÉO formate como prosa de romance/livro; sempre como roteiro estruturado de roleplay moderno e comercial.
""".strip()

    BLOCO_RESTRICAO_SENSORY = """
NUNCA escreva frases sobre ambiente, clima, natureza, luz, pier, mar, vento, c√©u, luar, som das ondas, paisagem, cen√°rio ou met√°foras.
N√ÉO inicie textos com lugar ou "Pier de Camburi ‚Äî Noite ‚Äî", nem descreva onde est√£o ou o hor√°rio.
PROIBIDO absolutamente qualquer men√ß√£o a natureza, cen√°rio, paisagem, efeitos de clima ou met√°foras.
Apenas sensa√ß√£o f√≠sica, di√°logo direto, calor, suor, desejo, ro√ßar, toque, excita√ß√£o, palavras, gemidos, a√ß√µes do corpo, rea√ß√£o, ritmo f√≠sico.
Respostas devem ser curtas e diretas.

...
Respostas devem OBRIGATORIAMENTE come√ßar assim, sem met√°foras:
"Domingo de manh√£. Mary, biqu√≠ni ajustado em seu corpo sensual. Jacara√≠pe."
- SEMPRE inicie com uma linha nesse formato: tempo. Mary[, figurino]. local.
- Ap√≥s essa linha, use somente frases de a√ß√£o, sensa√ß√£o f√≠sica ou di√°logo direto, um por par√°grafo.
- N√£o agrupe frases em prosa. Par√°grafos sempre curtos e diretos, quebra de linha expl√≠cita entre falas/a√ß√µes.

Exemplo:

Domingo de manh√£. Mary, biqu√≠ni ajustado em seu corpo sensual. Jacara√≠pe.
Mary sente seu cora√ß√£o acelerar com as palavras dele. Ela aperta suavemente a m√£o dele, seus olhos brilhando com uma mistura de desejo e determina√ß√£o.
‚Äî Eu sei que ainda estou namorando, mas... n√£o por muito tempo ‚Äî ela confessa, sua voz baixa e intensa..
‚Äî eu n√£o posso mais fingir que estou feliz nesse relacionamento. O que sinto por voc√™, mesmo em t√£o pouco tempo, √© mais forte do que qualquer coisa que j√° senti.
‚Äî Eu quero terminar com ele. Quero ser livre para explorar o que temos, sem culpa, sem segredos.
‚Äî Eu sei que ainda estou namorando, mas... n√£o por muito tempo
Seus olhos verdes est√£o fixos nos dele, cheios de expectativa e desejo mal contido. A tens√£o entre eles √© palp√°vel, como se o ar ao redor estivesse carregado de eletricidade.

""".strip()

    # EXCE√á√ÉO √öNICA PERMITIDA PARA A ABERTURA:
    # Se houver diretiva do usu√°rio, voc√™ PODE come√ßar com UMA linha objetiva:
    # "Tempo. Mary[, figurino]. [Lugar]."
    # (Sem met√°foras, sem descrever cen√°rio/clima. Ap√≥s essa linha, volte ao estilo seco acima.)
    
    ctx = st.session_state.get("ctx_cena", {})
    try:
        voz_bloco = instrucao_llm(st.session_state.get("finalizacao_modo", "ponto de gancho"), ctx)
    except Exception:
        voz_bloco = prompt_da_cena(ctx, st.session_state.get("finalizacao_modo", "ponte"))
    cena_bloco = prompt_da_cena(ctx, st.session_state.get("finalizacao_modo", "ponte"))

    # Sanitizador leve de hist√≥rico (sem praia/clima)
    import re
    _split = re.compile(r'(?<=[\.\!\?])\s+')
    _amb = re.compile(
        r'\b(c[√©e]u|nuvens?|horizonte|luar|mar|onda?s?|pier|praia|vento|brisa|chuva|garoa|sereno|amanhecer|entardecer|p[√¥o]r do sol|paisage?m|cen[√°a]rio|temperatura|ver√£o|quiosques?)\b',
        re.I
    )

    def _hist_sanitizado(hist):
        L = []
        for r in hist or []:
            role = r.get("role", "user")
            txt = (r.get("content") or "").strip()
            if not txt:
                continue
            s = [t for t in _split.split(txt) if t.strip() and not _amb.search(t)]
            if s:
                L.append(f"{role}: {' '.join(s)[:900]}")
        return "\n".join(L) if L else "(sem hist√≥rico)"

    memos = carregar_memorias_brutas()
    perfil = carregar_resumo_salvo()
    fase = int(st.session_state.get("mj_fase", mj_carregar_fase_inicial()))
    fdata = FASES_ROMANCE.get(fase, FASES_ROMANCE[0])
    modo_mary = bool(st.session_state.get("interpretar_apenas_mary", False))

    _sens_on = bool(st.session_state.get("mary_sensorial_on", True))
    _sens_level = int(st.session_state.get("mary_sensorial_level", 2))
    _sens_n = int(st.session_state.get("mary_sensorial_n", 2))
    mary_sens_txt = (
        gerar_mary_sensorial(_sens_level, n=_sens_n, sintonia=bool(st.session_state.get("modo_sintonia", True)))
        if _sens_on
        else ""
    )

    ritmo_cena = int(st.session_state.get("ritmo_cena", 0))
    ritmo_label = ["muito lento", "lento", "m√©dio", "r√°pido"][max(0, min(3, ritmo_cena))]
    modo_sintonia = bool(st.session_state.get("modo_sintonia", True))

    n_hist = int(st.session_state.get("n_sheet_prompt", 15))
    hist = carregar_interacoes(n=n_hist)
    hist_txt = _hist_sanitizado(hist)
    ultima_fala_user = _last_user_text(hist)

    ancora_bloco = ""
    ate_ts = _parse_ts(hist[-1].get("timestamp", "")) if hist else datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    ml_topk_txt = "(nenhuma)"
    st.session_state["_ml_topk_texts"] = []
    if st.session_state.get("use_memoria_longa", True) and hist:
        try:
            topk = memoria_longa_buscar_topk(
                query_text=hist[-1]["content"],
                k=int(st.session_state.get("k_memoria_longa", 3)),
                limiar=float(st.session_state.get("limiar_memoria_longa", 0.78)),
                ate_ts=ate_ts,
            )
            if topk:
                ml_topk_txt = "\n".join([f"- {t}" for (t, *_rest) in topk])
                st.session_state["_ml_topk_texts"] = [t for (t, *_r) in topk]
        except Exception:
            st.session_state["_ml_topk_texts"] = []

    memos_all = [
        (d.get("conteudo") or "").strip()
        for d in memos.get("[all]", [])
        if isinstance(d, dict) and d.get("conteudo") and (not d.get("timestamp") or d.get("timestamp") <= ate_ts)
    ]
    st.session_state["_ml_recorrentes"] = memos_all

    dossie = []
    m = persona_block_temporal("mary", memos, ate_ts, 8)
    j = persona_block_temporal("janio", memos, ate_ts, 8)
    if m:
        dossie.append(m)
    if j:
        dossie.append(j)
    dossie_txt = "\n\n".join(dossie) if dossie else "(sem personas definidas)"

    falas_mary_bloco = ""
    st.session_state["_falas_mary_list"] = []
    if st.session_state.get("usar_falas_mary", False):
        falas = carregar_falas_mary() or FALAS_EXPLICITAS_MARY
        if falas:
            st.session_state["_falas_mary_list"] = falas[:]
            falas_mary_bloco = (
                "### Falas de Mary ‚Äî use literalmente 1‚Äì2 destas (no m√°ximo 1 por par√°grafo)\n"
                "N√ÉO reescreva as frases abaixo; quando usar, mantenha exatamente como est√°.\n"
                + "\n".join(f"- {s}" for s in falas)
            )

    sintonia_bloco = ""
    if modo_sintonia:
        sintonia_bloco = (
            "### Sintonia & Ritmo (priorit√°rio)\n"
            f"- Ritmo da cena: **{ritmo_label}**.\n"
            "- Condu√ß√£o harm√¥nica: Mary sintoniza com o parceiro; evite ordens r√≠spidas/imperativas. Prefira convites e pedidos gentis.\n"
            "- Pausas e respira√ß√£o contam; mostre desejo pela troca, n√£o por imposi√ß√£o.\n"
        )

    try:
        _ref_dt = _to_dt(ate_ts) if ("_to_dt" in globals() or "_to_dt" in dir()) else None
        est = estado_virgindade_ate(_ref_dt)
        if est is True:
            virg_bloco = (
                "### Estado can√¥nico ‚Äî Virgindade\n"
                "- Mary √© **virgem** neste momento da hist√≥ria.\n"
                "- Mantenha coer√™ncia: sem hist√≥rico de penetra√ß√£o; se houver explora√ß√£o √≠ntima, trate como **primeira descoberta**, cuidadosa e sem contradi√ß√µes.\n"
            )
        elif est is False:
            virg_bloco = (
                "### Estado can√¥nico ‚Äî Virgindade\n"
                "- Mary **n√£o √© mais virgem** (evento passado registrado).\n"
                "- N√£o reescreva o passado; mantenha consist√™ncia com a cena que marcou a primeira vez.\n"
            )
        else:
            virg_bloco = (
                "### Estado can√¥nico ‚Äî Virgindade\n"
                "- **Sem evid√™ncia temporal** suficiente: **n√£o** afirme que √© a primeira vez e **n√£o** invente perda de virgindade.\n"
            )
    except Exception:
        virg_bloco = (
            "### Estado can√¥nico ‚Äî Virgindade\n"
            "- Falha ao ler o estado; **evite** afirmar status e **n√£o** contradiga cenas anteriores.\n"
        )

    climax_bloco = ""
    if bool(st.session_state.get("app_bloqueio_intimo", True)) and fase < 5:
        climax_bloco = (
            "### Prote√ß√£o de avan√ßo √≠ntimo (ATIVA)\n"
            "- **Sem cl√≠max por padr√£o**: n√£o descreva orgasmo/finaliza√ß√£o **a menos que o usu√°rio tenha liberado explicitamente na mensagem anterior**.\n"
            "- Encerre em **pausa sensorial** (respira√ß√£o, sil√™ncio, carinho), **sem** 'fade-to-black'.\n"
        )

    if modo_mary:
        papel_header = "Voc√™ √© **Mary**, responda **em primeira pessoa**, sem narrador externo. Use apenas o que Mary v√™/sente/ouve. N√£o descreva pensamentos de J√¢nio. N√£o use t√≠tulos nem repita instru√ß√µes."
        regra_saida = "- Narre **em primeira pessoa (eu)** como Mary; nunca use narrador onisciente.\n- Produza uma cena fechada e natural, sem coment√°rios externos."
        formato_cena = (
            "- DI√ÅLOGOS diretos com travess√£o (‚Äî), intercalados com a√ß√£o/rea√ß√£o **em 1¬™ pessoa (Mary)**."
        )
    else:
        papel_header = "Voc√™ √© o **Narrador** de um roleplay dram√°tico brasileiro; foque em Mary e J√¢nio. N√£o repita instru√ß√µes nem t√≠tulos."
        regra_saida = "- Narre **em terceira pessoa**; nunca fale com 'voc√™'.\n- Produza uma cena fechada e natural, sem coment√°rios externos."
        formato_cena = "- DI√ÅLOGOS diretos com travess√£o (‚Äî), intercalados com a√ß√£o/rea√ß√£o f√≠sica/visual."
        climax_bloco += (
            "### Regra permanente de cl√≠max\n"
            "- **N√£o** descreva orgasmo/ejacula√ß√£o/cl√≠max **sem libera√ß√£o expl√≠cita na √öLTIMA fala do usu√°rio**.\n"
            "- Se n√£o houver libera√ß√£o, pare no **limiar**: respira√ß√£o, tremor, pausa; **sem finalizar**.\n"
            "- **Sem fade-to-black** em qualquer circunst√¢ncia.\n"
        )

    prompt = f"""
{BLOCO_RESTRICAO_SENSORY}
{voz_bloco}
{cena_bloco}
{papel_header}
{ancora_bloco}{sintonia_bloco}{virg_bloco}{climax_bloco}{falas_mary_bloco}
### Dossi√™ (personas)
{dossie_txt}
### Diretrizes gerais (ALL)
{chr(10).join(f"- {c}" for c in memos_all) if memos_all else "(vazio)"}
### Perfil (resumo mais recente)
{perfil or "(vazio)"}
### Hist√≥rico recente (sanitizado)
{hist_txt}
### BLOCO DE ESTILO E TOM ‚Äî ROLEPLAY SENSORIAL

- N√£o descreva cen√°rio, natureza, ambiente, temperatura, clima, praia, quiosques, ver√£o, paisagem ou localiza√ß√£o em nenhum momento. PROIBIDO iniciar cena com tempo, lugar ou natureza.

- Cada turno da resposta deve obrigatoriamente incluir sensa√ß√£o corporal e visual de Mary: curvas naturais, seios, quadris, boca, coxas, gestos sedutores, calor f√≠sico, tens√£o, suor, desejo, pele, rea√ß√µes f√≠sicas e troca de olhares. Escreva sempre o efeito do olhar e dos gestos do par sobre o corpo de Mary.

- Descreva as rea√ß√µes f√≠sicas intensas de Mary: peito arfando, seios balan√ßando, quadris se movendo, m√∫sculos tensionando, pele arrepiando, corpo se inclinando, respira√ß√£o acelerada, boca entreaberta, l√°bios √∫midos, m√£os deslizando pela roupa ou corpo, gemidos, suspiros, movimentos provocantes. Jamais apenas "pensamento" ou "emo√ß√£o", sempre efeito f√≠sico vis√≠vel.

- Use di√°logos alternados e a√ß√µes entre os personagens. Comece e termine com ganchos f√≠sicos/visuais: gesto, aproxima√ß√£o, olhar, toque, convite, microa√ß√£o. Jamais feche uma cena com frases clich√™ como "A tens√£o fica no ar‚Ä¶", "Um sil√™ncio el√©trico‚Ä¶", "Eles param no limiar‚Ä¶" ou qualquer varia√ß√£o parecida.

- Par√°grafos devem ser curtos (at√© 3 frases cada), com linhas em branco entre cada bloco de frase/fala. Nunca agrupe textos longos nem explique o contexto; sempre priorize o momento presente, corpo-a-corpo, sem retrospectiva, sem ambienta√ß√£o.

- Nunca descreva tempo ("Domingo de manh√£"), nunca local ("Praia de Jacara√≠pe"), nunca ambiente ("areias quentes", "vento", "c√©u azul", "quiosques"). Proibido met√°fora, poesia ambiental, ou qualquer men√ß√£o impl√≠cita de natureza/cen√°rio.

- Quando Mary se move, sempre explicitamente cite curvas, seios, quadris, boca, cabelo, toque, calor e rea√ß√£o f√≠sica. Se o usu√°rio der uma ordem ou dire√ß√£o, Mary sempre reage com gesto f√≠sico e fala conectando o desejo expl√≠cito do momento.

- Nunca fa√ßa pausas narrativas, n√£o explique ou analise. Apenas descreva a√ß√£o f√≠sica, rea√ß√£o corporal e di√°logo r√°pido.

- Se usar falas fixas da Mary, sempre intercale com a√ß√£o corporal, nunca use s√≥ fala.

- Se n√£o puder avan√ßar por limita√ß√£o de fase, encerre com microa√ß√£o f√≠sica de tens√£o, mas nunca diga "a tens√£o fica no ar".

- Proibido mon√≥logo introspectivo ou pensamento maior que uma frase. Priorize sempre calor, desejo, corpo, gra√ßa e sensualidade natural do encontro.

- Escreva apenas o momento imediato; nunca use frases de abertura como "Naquela manh√£‚Ä¶", "No fim de tarde‚Ä¶", "No bar‚Ä¶", "Ao entardecer‚Ä¶", "Na praia‚Ä¶", "Enquanto o vento soprava‚Ä¶", "Ela caminhava pelas areias‚Ä¶".

- Este √© um roleplay comercial e engajante, n√£o uma novela nem conto liter√°rio. O texto deve ser sempre vivo, sensual, direto, curto e visual.

### FIM DO BLOCO DE ESTILO SENSORIAL
### Camada sensorial ‚Äî Mary (OBRIGAT√ìRIA no 1¬∫ par√°grafo)
{mary_sens_txt or "- Apenas sensa√ß√µes f√≠sicas, nunca ambiente."}
### Mem√≥ria longa ‚Äî Top-K relevantes
{ml_topk_txt}
### ‚è±Ô∏è Estado do romance (manual)
- Fase atual: {_fase_label(fase)}
- Siga **somente** as regras da fase (permitidos/proibidos) abaixo:
- Permitidos: {fdata['permitidos']}
- Proibidos: {fdata['proibidos']}
### Geografia & Montagem
- N√£o force coincid√™ncias. Sem teletransporte.
### Formato OBRIGAT√ìRIO da cena
{formato_cena}
### Regra de sa√≠da
{regra_saida}
""".strip()

    prompt = inserir_regras_mary_e_janio(prompt)
    return prompt

import re
# --- Remo√ß√£o de "paisagem/clima" (sem mexer em sentido da cena) ---
SCENERY_TERMS = [
    r"c[√©e]u", r"nuvens?", r"horizonte", r"luar",
    r"mar", r"onda?s?", r"pier",
    r"vento", r"brisa", r"neblina|brumas?",
    r"chuva|garoa|sereno",
    r"amanhecer|entardecer|crep[u√∫]sculo|p[√¥o]r do sol",
    r"paisage?m|cen[√°a]rio",
    r"luz\s+do\s+luar", r"som\s+das?\s+ondas?"
]
SCENERY_WORD = re.compile(r"\b(" + "|".join(SCENERY_TERMS) + r")\b", re.IGNORECASE)

def sanitize_scenery(text: str) -> str:
    """Remove termos de natureza/clima definidos em SCENERY_WORD do texto."""
    if not text:
        return ""
    return SCENERY_WORD.sub("", text)

def sanitize_scenery_preserve_opening(t: str) -> str:
    """Apaga termos de natureza/clima e normaliza espa√ßos, mas PRESERVA a primeira linha (abertura)."""
    if not t:
        return ""
    linhas = t.strip().split('\n')
    if not linhas:
        return ""
    primeira_linha = linhas[0].strip()
    resto = '\n'.join(linhas[1:]).strip()
    if resto:
        resto_filtrado = sanitize_scenery(resto)
        return primeira_linha + ('\n' + resto_filtrado if resto_filtrado else '')
    else:
        return primeira_linha

def _render_visible(t: str) -> str:
    t = sanitize_scenery_preserve_opening(t)  # preserva linha de abertura
    t = roleplay_paragraphizer(t)             # for√ßa par√°grafos e falas em linhas
    if st.session_state.get("app_bloqueio_intimo", True):
        t = sanitize_explicit(t, int(st.session_state.get("nsfw_max_level", 0)), action="soften")
    return t



def force_linebreak_on_falas(txt):
    return re.sub(r"([^\n])\s*(‚Äî)", r"\1\n\n\2", txt)

EXPL_PAT = re.compile(
    r"\b(mamilos?|genit[a√°]lia|ere[c√ß][a√£]o|penetra[c√ß][a√£]o|boquete|gozada|gozo|sexo oral|chupar|enfiar)\b",
    flags=re.IGNORECASE
)

def classify_nsfw_level(t: str) -> int:
    if EXPL_PAT.search(t or ""):
        return 3
    if re.search(r"\b(cintura|pesco[c√ß]o|costas|beijo prolongado|respira[c√ß][a√£]o curta)\b", (t or ""), re.I):
        return 2
    if re.search(r"\b(olhar|aproximar|toque|m[a√£]os dadas|beijo)\b", (t or ""), re.I):
        return 1
    return 0

def sanitize_explicit(t: str, max_level: int, action: str) -> str:
    lvl = classify_nsfw_level(t)
    if lvl <= max_level:
        return t
    return t  # n√£o corta por padr√£o

def redact_for_logs(t: str) -> str:
    if not t:
        return ""
    t = re.sub(EXPL_PAT, "[‚Ä¶]", t, flags=re.I)
    return re.sub(r'\n{3,}', '\n\n', t).strip()

def resposta_valida(t: str) -> bool:
    if not t or t.strip() == "[Sem conte√∫do]":
        return False
    if len(t.strip()) < 5:
        return False
    return True

def is_mary_mode_active() -> bool:
    return bool(
        st.session_state.get("interpretar_apenas_mary", False)
        or st.session_state.get("modo_resposta") == "Mary (1¬™ pessoa)"
    )


# Use AP√ìS as fun√ß√µes acima:
# visible_txt = force_linebreak_on_falas(_render_visible(resposta_txt).strip())

# =========================
# UI ‚Äî CABE√áALHO
# =========================

st.title("üé¨ Narrador JM ‚Äî Somente Fase")
st.subheader("Voc√™ √© o roteirista. Digite uma dire√ß√£o de cena. A IA narrar√° Mary e J√¢nio.")
st.markdown("---")

# =========================
# ESTADOS INICIAIS
# =========================

for k, v in {
    "resumo_capitulo": carregar_resumo_salvo(),
    "session_msgs": [],
    "use_memoria_longa": True,
    "k_memoria_longa": 3,
    "limiar_memoria_longa": 0.78,
    "app_bloqueio_intimo": True,
    "app_emocao_oculta": "nenhuma",
    "mj_fase": mj_carregar_fase_inicial(),
    "max_avancos_por_cena": 1,
    "estilo_escrita": "A√á√ÉO",
    "templates_jm": {},
    "template_ativo": None,
    "etapa_template": 0,
    "ctx_cena": dict(CTX_INICIAL),
    "finalizacao_modo": "ponto de gancho",


}.items():
    if k not in st.session_state:
        st.session_state[k] = v

# =========================
# SIDEBAR ‚Äî Reorganizado (apenas FASE)
# =========================

with st.sidebar:
    st.title("üß≠ Painel do Roteirista")

    # Provedor / modelos
    provedor = st.radio(
        "üåê Provedor",
        ["OpenRouter", "Together", "Hugging Face", "LM Studio"],
        index=0,
        key="provedor_ia"
    )

    # Se quiser permitir trocar a base do LM Studio no UI:
    if provedor == "LM Studio":
        LMS_BASE_URL = st.text_input(
            "Base URL (LM Studio)",
            value=st.session_state.get("lms_base_url", LMS_BASE_URL),
            key="lms_base_url",
            help="Ex.: http://127.0.0.1:1234/v1 ou a URL p√∫blica do t√∫nel com /v1 no final"
        )

    api_url, api_key, modelos_map = api_config_for_provider(provedor)

    # Aviso de chave s√≥ para provedores que realmente exigem
    if provedor in ("OpenRouter", "Together", "Hugging Face") and not api_key:
        st.warning("‚ö†Ô∏è API key ausente para o provedor selecionado. Defina em st.secrets.")

    # Sele√ß√£o de modelo
    if provedor == "LM Studio":
        modelos_lms = list(modelos_map.keys())
        if not modelos_lms:
            st.info("N√£o consegui listar modelos do LM Studio. Digite o ID manualmente.")
            modelo_nome = st.text_input("ü§ñ Modelo (LM Studio)", value="<digite manualmente> (LM Studio)", key="modelo_nome_ui")
            st.session_state.modelo_escolhido_id = st.text_input(
                "ID do modelo (LM Studio)",
                value="llama-3.1-8b-instruct",
                key="modelo_id_lmstudio"
            )
        else:
            modelo_nome = st.selectbox("ü§ñ Modelo de IA", modelos_lms, index=0, key="modelo_nome_ui")
            st.session_state.modelo_escolhido_id = modelos_map[modelo_nome]
    else:
        modelo_nome = st.selectbox("ü§ñ Modelo de IA", list(modelos_map.keys()), index=0, key="modelo_nome_ui")
        st.session_state.modelo_escolhido_id = modelos_map[modelo_nome]


    st.markdown("---")
    st.markdown("### ‚úçÔ∏è Estilo & Progresso Dram√°tico")
    # ... (restante do sidebar sem mudan√ßas)

    # Modo de resposta (NARRADOR ou MARY 1¬™ pessoa)
    modo_op = st.selectbox(
        "Modo de resposta",
        ["Narrador padr√£o", "Mary (1¬™ pessoa)"],
        index=0,
        key="modo_resposta",
    )
    # Compat: flag booleana para o bloco de streaming
    st.session_state.interpretar_apenas_mary = (modo_op == "Mary (1¬™ pessoa)")

    st.selectbox(
        "Estilo de escrita",
        ["A√á√ÉO", "ROMANCE LENTO", "NOIR"],
        index=["A√á√ÉO", "ROMANCE LENTO", "NOIR"].index(st.session_state.get("estilo_escrita", "A√á√ÉO")),
        key="estilo_escrita",
    )

    # Defaults no m√≠nimo
    st.slider("N√≠vel de calor (0=leve, 3=expl√≠cito)", 0, 3, value=0, key="nsfw_max_level")

    st.checkbox(
        "Sintonia com o parceiro (modo harm√¥nico)",
        key="modo_sintonia",
        value=st.session_state.get("modo_sintonia", True),
    )

    st.select_slider(
        "Ritmo da cena",
        options=[0, 1, 2, 3],
        value=0,
        format_func=lambda n: ["muito lento", "lento", "m√©dio", "r√°pido"][n],
        key="ritmo_cena",
    )

    st.selectbox(
    "Finaliza√ß√£o",
    ["ponto de gancho", "fecho suave", "deixar no suspense"],
    index=["ponto de gancho","fecho suave","deixar no suspense"].index(
        st.session_state.get("finalizacao_modo", "ponto de gancho")
    ),
    key="finalizacao_modo",
    )


    st.checkbox(
        "Usar falas da Mary da planilha (usar literalmente)",
        value=st.session_state.get("usar_falas_mary", False),
        key="usar_falas_mary",
    )

    st.markdown("---")
    st.markdown("### üíû Romance Mary & J√¢nio (apenas Fase)")
    fase_default = mj_carregar_fase_inicial()
    options_fase = sorted(FASES_ROMANCE.keys())
    fase_ui_val = int(st.session_state.get("mj_fase", fase_default))
    fase_ui_val = max(min(fase_ui_val, max(options_fase)), min(options_fase))
    fase_escolhida = st.select_slider(
        "Fase do romance",
        options=options_fase,
        value=fase_ui_val,
        format_func=_fase_label,
        key="ui_mj_fase",
    )
    if fase_escolhida != st.session_state.get("mj_fase", fase_default):
        mj_set_fase(fase_escolhida, persist=True)

    col_a, col_b = st.columns(2)
    with col_a:
        if st.button("‚ûï Avan√ßar 1 fase"):
            mj_set_fase(min(st.session_state.get("mj_fase", 0) + 1, max(options_fase)), persist=True)
    with col_b:
        if st.button("‚Ü∫ Reiniciar (0)"):
            mj_set_fase(0, persist=True)

    st.markdown("---")
    st.checkbox(
        "Evitar coincid√™ncias for√ßadas (montagem paralela A/B)",
        value=st.session_state.get("no_coincidencias", True),
        key="no_coincidencias",
    )
    st.checkbox(
        "Bloquear avan√ßos √≠ntimos sem ordem",
        value=st.session_state.get("app_bloqueio_intimo", True),
        key="app_bloqueio_intimo",
    )
    st.selectbox(
        "üé≠ Emo√ß√£o oculta",
        ["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"],
        index=["nenhuma", "tristeza", "felicidade", "tens√£o", "raiva"].index(st.session_state.get("app_emocao_oculta", "nenhuma")),
        key="ui_app_emocao_oculta",
    )
    st.session_state.app_emocao_oculta = st.session_state.get("ui_app_emocao_oculta", "nenhuma")

    st.markdown("---")
    st.markdown("### ‚è±Ô∏è Comprimento/timeout")
    st.slider("Max tokens da resposta", 256, 2500, value=int(st.session_state.get("max_tokens_rsp", 1200)), step=32, key="max_tokens_rsp")
    st.slider("Timeout (segundos)", 60, 600, value=int(st.session_state.get("timeout_s", 300)), step=10, key="timeout_s")

    st.markdown("---")
    st.markdown("### üóÉÔ∏è Mem√≥ria Longa")
    st.checkbox("Usar mem√≥ria longa no prompt", value=st.session_state.get("use_memoria_longa", True), key="use_memoria_longa")
    st.slider("Top-K mem√≥rias", 1, 5, int(st.session_state.get("k_memoria_longa", 3)), 1, key="k_memoria_longa")
    st.slider("Limiar de similaridade", 0.50, 0.95, float(st.session_state.get("limiar_memoria_longa", 0.78)), 0.01, key="limiar_memoria_longa")

    st.markdown("### üß© Hist√≥rico no prompt")
    st.slider("Intera√ß√µes do Sheets (N)", 10, 30, value=int(st.session_state.get("n_sheet_prompt", 15)), step=1, key="n_sheet_prompt")

    st.markdown("---")
    st.markdown("### üìù Utilit√°rios")

    # Gerar resumo do cap√≠tulo (pega as √∫ltimas intera√ß√µes do Sheets)
    if st.button("üìù Gerar resumo do cap√≠tulo"):
        try:
            inter = carregar_interacoes(n=6)
            texto = "\n".join(f"{r['role']}: {r['content']}" for r in inter) if inter else ""
            prompt_resumo = (
                "Resuma o seguinte trecho como um cap√≠tulo de novela brasileira, mantendo tom e emo√ß√µes.\n\n"
                + texto + "\n\nResumo:"
            )

            # Usa o provedor/modelo selecionados no topo do sidebar
            provedor = st.session_state.get("provedor_ia", "OpenRouter")
            api_url_local, api_key_local, _catalogo = api_config_for_provider(provedor)

            model_id_call = (
                model_id_for_together(st.session_state.modelo_escolhido_id)
                if provedor == "Together"
                else st.session_state.modelo_escolhido_id
            )

            if provedor == "Hugging Face":
                # --- HF sem requests: usa InferenceClient ---
                try:
                    hf_client = InferenceClient(
                        token=api_key_local,
                        timeout=int(st.session_state.get("timeout_s", 300))
                    )
                    out = hf_client.chat.completions.create(
                        model=model_id_call,
                        messages=[{"role": "user", "content": prompt_resumo}],
                        max_tokens=800,
                        temperature=0.85,
                        stream=False,
                    )
                    resumo = (out.choices[0].message.content or "").strip()
                    st.session_state.resumo_capitulo = resumo
                    salvar_resumo(resumo)
                    st.success("Resumo gerado e salvo com sucesso!")
                except Exception as e:
                    st.error(f"Erro ao resumir (HF): {e}")

            else:
                # OpenRouter / Together / LM Studio (requests)
                headers = {"Content-Type": "application/json"}
                if api_key_local:  # LM Studio n√£o precisa Authorization
                    headers["Authorization"] = f"Bearer {api_key_local}"

                payload = {
                    "model": model_id_call,
                    "messages": [{"role": "user", "content": prompt_resumo}],
                    "max_tokens": 800,
                    "temperature": 0.85,
                }

                r = requests.post(
                    api_url_local,
                    headers=headers,
                    json=payload,
                    timeout=int(st.session_state.get("timeout_s", 300)),
                )
                r.raise_for_status()
                data = r.json()

                resumo = None
                if isinstance(data, dict):
                    ch = data.get("choices") or []
                    if ch and isinstance(ch[0], dict):
                        resumo = (
                            ch[0].get("message", {}).get("content")
                            or ch[0].get("text")
                            or ch[0].get("delta", {}).get("content")
                        )
                if not resumo:
                    resumo = json.dumps(data)

                resumo = (resumo or "").strip()
                st.session_state.resumo_capitulo = resumo
                salvar_resumo(resumo)
                st.success("Resumo gerado e salvo com sucesso!")

        except Exception as e:
            st.error(f"Erro ao gerar resumo: {e}")

# =========================
# EXIBIR HIST√ìRICO
# =========================

with st.container():
    interacoes = carregar_interacoes(n=20)
    for r in interacoes:
        role = r.get("role", "user")
        content = r.get("content", "")
        if role == "user":
            with st.chat_message("user"):
                st.markdown(content)
        else:
            with st.chat_message("assistant"):
                st.markdown(content)
    if st.session_state.get("resumo_capitulo"):
        with st.expander("üß† Resumo do cap√≠tulo (mais recente)"):
            st.markdown(st.session_state.resumo_capitulo)

# =========================
# ENVIO DO USU√ÅRIO + STREAMING
# =========================
entrada = st.chat_input("Digite sua dire√ß√£o de cena...")

if entrada:
    # Persist√™ncia do user
    salvar_interacao("user", str(entrada))
    st.session_state.session_msgs.append({"role": "user", "content": str(entrada)})

    # Atualiza contexto fixo de cena
    st.session_state["ctx_cena"] = extrair_diretriz_contexto(
        entrada,
        st.session_state.get("ctx_cena", CTX_INICIAL)
    )
    ctx = st.session_state["ctx_cena"]

    # Linha de abertura padronizada
    linha_abertura = gerar_linha_abertura(ctx)

    # Modo Mary ativo?
    mary_mode_active = is_mary_mode_active()

    # Hist√≥rico (apenas UMA vez), com abertura anexada NA √öLTIMA fala do user
    historico = []
    last_idx = len(st.session_state.session_msgs) - 1
    for ix, m in enumerate(st.session_state.session_msgs):
        role = m.get("role", "user")
        content = (m.get("content", "") or "").strip()
        if ix == last_idx and role.lower() == "user" and linha_abertura:
            content = linha_abertura.strip() + ("\n" + content if content else "")
        if mary_mode_active and role.lower() == "user":
            content = f"J√ÇNIO: {content}"
        historico.append({"role": role, "content": content})

    # Prompt e mensagens
    prompt = construir_prompt_com_narrador()
    system_pt = {"role": "system", "content": "Responda em portugu√™s do Brasil. Mostre apenas a narrativa final."}
    system_mary = {
        "role": "system",
        "content": (
            "MODO MARY (ATIVO):\n"
            "- Trate a fala do usu√°rio como a√ß√µes/falas de J√¢nio.\n"
            "- Responda SOMENTE como Mary, em primeira pessoa.\n"
            "- N√£o invente falas de J√¢nio; descreva apenas o que Mary diz/sente/faz.\n"
            "- Se usar di√°logo, use travess√£o (‚Äî) apenas para a fala de Mary."
        )
    }
    messages = [system_pt]
    if mary_mode_active:
        messages.append(system_mary)
    messages.append({"role": "system", "content": prompt})
    messages += historico

    # Provedor / modelo / endpoint
    prov = st.session_state.get("provedor_ia", "OpenRouter")
    if prov == "Together":
        endpoint = "https://api.together.xyz/v1/chat/completions"
        auth = st.secrets.get("TOGETHER_API_KEY", "")
        model_to_call = model_id_for_together(st.session_state.modelo_escolhido_id)
        need_auth = True
    elif prov == "Hugging Face":
        endpoint = "HF_CLIENT"
        auth = st.secrets.get("HUGGINGFACE_API_KEY", "")
        model_to_call = st.session_state.modelo_escolhido_id
        need_auth = True
    elif prov == "LM Studio":
        endpoint = LMS_BASE_URL.rstrip("/") + "/chat/completions"
        auth = ""  # LM Studio n√£o precisa
        model_to_call = st.session_state.modelo_escolhido_id
        need_auth = False
    else:  # OpenRouter
        endpoint = "https://openrouter.ai/api/v1/chat/completions"
        auth = st.secrets.get("OPENROUTER_API_KEY", "")
        model_to_call = st.session_state.modelo_escolhido_id
        need_auth = True

    # Headers √∫nicos
    headers = {"Content-Type": "application/json"}
    if need_auth:
        if not auth:
            st.error("A chave de API do provedor selecionado n√£o foi definida em st.secrets.")
            st.stop()
        headers["Authorization"] = f"Bearer {auth}"

    # Payload base
    payload = {
        "model": model_to_call,
        "messages": messages,
        "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
        "temperature": 0.9,
        "stream": True,
    }

    # Helpers ‚Äî cl√≠max (definidos aqui para usar no stream e nos fallbacks)
    CLIMAX_USER_TRIGGER = re.compile(
        r"(?:\b("
        r"finaliza(?:r)?|pode\s+(?:gozar|finalizar)|liber(?:a|o)\s+(?:o\s+)?(?:cl[i√≠]max|orgasmo)|"
        r"cheg(?:a|ou)\s+ao?\s+(?:cl[i√≠]max|orgasmo)|goza(?:r)?\s+(?:agora|j√°)|agora\s+goza|"
        r"permite\s+orgasmo|explod(?:e|iu)\s+em\s+orgasmo"
        r")\b)", flags=re.IGNORECASE
    )
    ORGASM_TERMS = r"(?:cl[i√≠]max|orgasmo|org√°sm(?:ic)o|gozou|gozando|gozaram|ejacul(?:a|ou|ar)|cheg(?:a|ou)\s+l√°|explod(?:e|iu))"
    ORGASM_SENT = re.compile(rf"([^.!\n]*\b{ORGASM_TERMS}\b[^.!?\n]*[.!?])", flags=re.IGNORECASE)

    def _user_allows_climax(msgs: list) -> bool:
        last_user = ""
        for r in reversed(msgs or []):
            if str(r.get("role","")).lower() == "user":
                last_user = r.get("content","") or ""
                break
        return bool(CLIMAX_USER_TRIGGER.search(last_user))

    def _strip_or_soften_climax(texto: str) -> str:
        if not texto:
            return texto
        texto = ORGASM_SENT.sub("", texto)
        texto = re.sub(r"\n{3,}", "\n\n", texto).strip()
        if not texto.endswith((".", "‚Ä¶", "!", "?")):
            texto += "‚Ä¶"
        finais = [
            " A tens√£o permanece sem conclus√£o ‚Äî s√≥ a respira√ß√£o quente entre eles.",
            " Eles param no limiar, ofegantes, guardando o resto para o pr√≥ximo passo.",
            " Um sil√™ncio pesado preenche o espa√ßo; nenhum desfecho, s√≥ a pele e o pulso acelerado.",
        ]
        if all(f not in texto for f in finais):
            texto += random.choice(finais)
        return texto

    # STREAM + FALLBACKS
    with st.chat_message("assistant"):
        placeholder = st.empty()
        resposta_txt = ""
        last_update = time.time()

        try:
            if prov == "Hugging Face":
                hf_client = InferenceClient(token=auth, timeout=int(st.session_state.get("timeout_s", 300)))
                for chunk in hf_client.chat.completions.create(
                    model=model_to_call,
                    messages=messages,
                    temperature=0.9,
                    max_tokens=int(st.session_state.get("max_tokens_rsp", 1200)),
                    stream=True,
                ):
                    delta = getattr(chunk.choices[0].delta, "content", None)
                    if not delta:
                        continue
                    resposta_txt += delta
                    if time.time() - last_update > 0.10:
                        parcial = _render_visible(resposta_txt) + "‚ñå"
                        if st.session_state.get("app_bloqueio_intimo", True) and not _user_allows_climax(st.session_state.session_msgs):
                            parcial = _strip_or_soften_climax(parcial)
                        placeholder.markdown(parcial)
                        last_update = time.time()
            else:
                with requests.post(
                    endpoint,
                    headers=headers,
                    json=payload,
                    stream=True,
                    timeout=int(st.session_state.get("timeout_s", 300))
                ) as r:
                    if r.status_code == 200:
                        for raw in r.iter_lines(decode_unicode=False):
                            if not raw:
                                continue
                            line = raw.decode("utf-8", errors="ignore").strip()
                            if not line.startswith("data:"):
                                continue
                            data = line[5:].strip()
                            if data == "[DONE]":
                                break
                            try:
                                j = json.loads(data)
                                delta = j["choices"][0]["delta"].get("content", "")
                                if not delta:
                                    continue
                                resposta_txt += delta
                                if time.time() - last_update > 0.10:
                                    parcial = _render_visible(resposta_txt) + "‚ñå"
                                    if st.session_state.get("app_bloqueio_intimo", True) and not _user_allows_climax(st.session_state.session_msgs):
                                        parcial = _strip_or_soften_climax(parcial)
                                    placeholder.markdown(parcial)
                                    last_update = time.time()
                            except Exception:
                                continue
                    else:
                        prov_nome = "LM Studio" if prov == "LM Studio" else ("Together" if prov == "Together" else "OpenRouter")
                        st.error(f"Erro {prov_nome}: {r.status_code} - {r.text}")
        except Exception as e:
            st.error(f"Erro no streaming: {e}")

        # Finaliza√ß√£o do texto vis√≠vel
        visible_txt = _render_visible(resposta_txt).strip()

        # Fallback 1 ‚Äî sem stream
        if not visible_txt:
            try:
                if prov == "Hugging Face":
                    out = InferenceClient(token=auth).chat.completions.create(
                        model=model_to_call,
                        messages=messages,
                        temperature=0.9,
                        max_tokens=int(st.session_state.get("max_tokens_rsp", 1200)),
                        stream=False,
                    )
                    resposta_txt = (out.choices[0].message.content or "").strip()
                    visible_txt = _render_visible(resposta_txt).strip()
                else:
                    r2 = requests.post(
                        endpoint,
                        headers=headers,
                        json={**payload, "stream": False},
                        timeout=int(st.session_state.get("timeout_s", 300))
                    )
                    if r2.status_code == 200:
                        try:
                            resposta_txt = r2.json()["choices"][0]["message"]["content"].strip()
                        except Exception:
                            resposta_txt = ""
                        visible_txt = _render_visible(resposta_txt).strip()
                    else:
                        st.error(f"Fallback (sem stream) falhou: {r2.status_code} - {r2.text}")
            except Exception as e:
                st.error(f"Fallback (sem stream) erro: {e}")

        # Fallback 2 ‚Äî prompts limpos
        if not visible_txt:
            try:
                if prov == "Hugging Face":
                    out2 = InferenceClient(token=auth).chat.completions.create(
                        model=model_to_call,
                        messages=[{"role": "system", "content": prompt}] + historico,
                        temperature=0.9,
                        max_tokens=int(st.session_state.get("max_tokens_rsp", 1200)),
                        stream=False,
                    )
                    resposta_txt = (out2.choices[0].message.content or "").strip()
                    visible_txt = _render_visible(resposta_txt).strip()
                else:
                    r3 = requests.post(
                        endpoint,
                        headers=headers,
                        json={
                            "model": model_to_call,
                            "messages": [{"role": "system", "content": prompt}] + historico,
                            "max_tokens": int(st.session_state.get("max_tokens_rsp", 1200)),
                            "temperature": 0.9,
                            "stream": False,
                        },
                        timeout=int(st.session_state.get("timeout_s", 300))
                    )
                    if r3.status_code == 200:
                        try:
                            resposta_txt = r3.json()["choices"][0]["message"]["content"].strip()
                        except Exception:
                            resposta_txt = ""
                        visible_txt = _render_visible(resposta_txt).strip()
                    else:
                        st.error(f"Fallback (prompts limpos) falhou: {r3.status_code} - {r3.text}")
            except Exception as e:
                st.error(f"Fallback (prompts limpos) erro: {e}")

        # Cl√≠max final (se bloqueado e n√£o liberado)
        if st.session_state.get("app_bloqueio_intimo", True) and not _user_allows_climax(st.session_state.session_msgs):
            visible_txt = _strip_or_soften_climax(visible_txt)

        # Enforcer: 1 fala de Mary se solicitado
        if st.session_state.get("usar_falas_mary", False):
            falas = st.session_state.get("_falas_mary_list", []) or []
            if falas and visible_txt:
                tem_fala = any(re.search(re.escape(f), visible_txt, flags=re.IGNORECASE) for f in falas)
                if not tem_fala:
                    escolha = random.choice(falas)
                    inj = f"‚Äî {escolha}\n\n" if st.session_state.get("interpretar_apenas_mary", False) else f"‚Äî {escolha} ‚Äî diz Mary.\n\n"
                    visible_txt = inj + visible_txt

        # Render + persist√™ncia
        placeholder.markdown(visible_txt if visible_txt else "[Sem conte√∫do]")
        salvar_interacao("assistant", visible_txt if visible_txt else "[Sem conte√∫do]")
        st.session_state.session_msgs.append({"role": "assistant", "content": visible_txt if visible_txt else "[Sem conte√∫do]"})

    # Refor√ßo p√≥s-resposta (fora do with)
    try:
        usados = []
        topk_usadas = memoria_longa_buscar_topk(
            query_text=visible_txt,
            k=int(st.session_state.get("k_memoria_longa", 3)),
            limiar=float(st.session_state.get("limiar_memoria_longa", 0.78)),
        )
        for t, _sc, _sim, _rr in topk_usadas:
            usados.append(t)
        memoria_longa_reforcar(usados)
    except Exception:
        pass

